{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data\\\\review_classification.csv', encoding = \"utf-8\", usecols = ['stars','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>went for dinner tonight. Amazing my husband ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>This was an amazing dinning experience! ORDER ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                               text\n",
       "0      5  went for dinner tonight. Amazing my husband ha...\n",
       "1      5  This was an amazing dinning experience! ORDER ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 586927 entries, 0 to 586926\n",
      "Data columns (total 2 columns):\n",
      "stars    586927 non-null int64\n",
      "text     586927 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 9.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set stars that higher than 4 to be positive, else negative\n",
    "df['target'] = df['stars'] > 4\n",
    "target = df['target'].values\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.47496877805928167, 0.49937304485540107, (586927,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.mean(), target.std(), target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature variables (Text Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"went for dinner tonight. Amazing my husband had lobster bisque and the T bone both were delish.I had the French onion soup and the pan seared duck. Cooked to perfection and I'm still raving about the flavor. If you are ever in Vegas this is a must try.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = df['text'].values\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('O'), (586927,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.dtype, documents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yle\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_train, documents_test, y_train, y_test = train_test_split(documents, target, test_size = 0.2, random_state = 66) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469541, 117386, 469541, 117386)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_train), len(documents_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get NLP represetation of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_features = 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469541, 350)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model with training data\n",
    "x_train = vectorizer.fit_transform(documents_train).toarray()\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vocab of tfidf\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117386, 350)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = vectorizer.fit_transform(documents_test).toarray()\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar review search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_random_number = 66\n",
    "search_query = documents_test[get_random_number]\n",
    "search_queries = [search_query]  # need to be put into a list-like format\n",
    "print(search_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_search_queries = vectorizer.transform(search_queries).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = cosine_similarity(vector_search_queries, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick top n similar reviews\n",
    "n = 5\n",
    "returned_reviews = [documents_train[i] for i in np.argsort(similarity_scores[0])[::-1][:n]] # argsort returns the indices that would sort an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query:\n",
      "My boyfriend and I came here for the first time and I  think we might've found our new sushi spot! Fresh sushi, a lot of selections and great service. Wish I had gotten our servers name but he was an Asian dude with glasses and he was the best!! Super nice and attentive at all times. Thank you!\n",
      "\n",
      "Most 5 similar reviews:\n",
      "#0:\n",
      "Fresh sushi, good service, great deals! I think I found my new sushi spot in mountains edge!\n",
      "#1:\n",
      "Nice spot with a good sushi. I ordered sushi for four people to go and paid around 45 dollars. I got huge pack of sushi and it was enough for big party. Thank you\n",
      "#2:\n",
      "The best sushi I think I've ever had especially on this side of town. Everything we ordered tasted super clean and fresh with minimal waiting time. We did AYCE and it was the first time I think I've ever stuffed my face with sushi and didn't feel like garbage after. The menu has a wide and unique variety of items that you typically can't find in sushi joints with all you can eat. I have definitely found my new regular spot.\n",
      "#3:\n",
      "One of the most affordable sushi spot near UNLV. You can have some sushi here and go next door for  billiards!\n",
      "#4:\n",
      "So soooo good! My new fav sushi spot! All the employees are all so nice and welcoming too!\n"
     ]
    }
   ],
   "source": [
    "# print the output\n",
    "print('Search query:')\n",
    "print(search_query)\n",
    "\n",
    "print('\\nMost %s similar reviews:' % n)\n",
    "for i, review in enumerate(returned_reviews):\n",
    "    print('#%s:' % i)\n",
    "    print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying positive/negative review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive-Bayes Classifier (Baseline Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb = GaussianNB()\n",
    "model_nb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7499685863428327"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "model_nb.score(x_train, y_train) # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59709846148603751"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "model_nb.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search to find best predictale classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "param_grid = [{'penalty':['l1'], 'C':[0.1, 1, 10]},\n",
    "              {'penalty':['l2'], 'C':[0.1, 1, 10]}]\n",
    "\n",
    "scores = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for accuracy\n",
      "\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "\n",
      "{'C': 1, 'penalty': 'l2'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "0.520 (+/-0.000) for {'C': 0.1, 'penalty': 'l1'}\n",
      "0.696 (+/-0.073) for {'C': 1, 'penalty': 'l1'}\n",
      "0.690 (+/-0.123) for {'C': 10, 'penalty': 'l1'}\n",
      "0.702 (+/-0.082) for {'C': 0.1, 'penalty': 'l2'}\n",
      "0.732 (+/-0.108) for {'C': 1, 'penalty': 'l2'}\n",
      "0.710 (+/-0.077) for {'C': 10, 'penalty': 'l2'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.63      0.81      0.71     61680\n",
      "       True       0.69      0.47      0.56     55706\n",
      "\n",
      "avg / total       0.66      0.65      0.64    117386\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for score in scores:\n",
    "    \n",
    "    print(\"# Tuning hyper-parameters for %s\" % score + \"\\n\\n\")\n",
    "    \n",
    "    clf = GridSearchCV(LogisticRegression(),\n",
    "                       param_grid,\n",
    "                       cv=5,\n",
    "                       scoring=score)\n",
    "    clf.fit(x_train[:500,:], y_train[:500])\n",
    "    print(\"Best parameters set found on development set:\\n\\n\")\n",
    "    print(clf.best_params_)\n",
    "    print(\"\\nGrid scores on development set:\\n\\n\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    \n",
    "    print(\"\\nDetailed classification report:\\n\")\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print(\"\\n\")\n",
    "    y_true, y_pred = y_test, clf.predict(x_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lrc = LogisticRegression(C =1 ,penalty = 'l2')\n",
    "model_lrc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7952106418821786"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "model_lrc.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6345134854241562"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "model_lrc.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazing',\n",
       " 'best',\n",
       " 'thank',\n",
       " 'awesome',\n",
       " 'delicious',\n",
       " 'highly',\n",
       " 'perfect',\n",
       " 'fantastic',\n",
       " 'favorite',\n",
       " 'excellent',\n",
       " 'great',\n",
       " 'wonderful',\n",
       " 'love',\n",
       " 'perfectly',\n",
       " 'loved',\n",
       " 'definitely',\n",
       " 'happy',\n",
       " 'vegas',\n",
       " 'absolutely',\n",
       " 'fresh']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top n key features(words) that make the positive prediction\n",
    "n = 20\n",
    "[words[i] for i in np.argsort(model_lrc.coef_[0])[::-1][:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['worst',\n",
       " 'horrible',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'slow',\n",
       " 'dry',\n",
       " 'decent',\n",
       " 'wasn',\n",
       " 'reason',\n",
       " 'wouldn',\n",
       " 'overall',\n",
       " 'bad',\n",
       " 'maybe',\n",
       " 'cold',\n",
       " 'money',\n",
       " 'pretty',\n",
       " 'didn',\n",
       " 'used',\n",
       " 'stars',\n",
       " 'asked']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top n key features(words) that make the negative prediction\n",
    "[words[i] for i in np.argsort(model_lrc.coef_[0])[:n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "param_grid = [{'n_estimators':[5, 10,15,20], 'min_samples_leaf':[1, 3, 5, 7]},\n",
    "              {'n_estimators':[5, 10,15,20], 'min_samples_leaf':[1, 3, 5, 7]}]\n",
    "\n",
    "scores = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for accuracy\n",
      "\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "\n",
      "{'min_samples_leaf': 1, 'n_estimators': 15}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "0.662 (+/-0.157) for {'min_samples_leaf': 1, 'n_estimators': 5}\n",
      "0.710 (+/-0.066) for {'min_samples_leaf': 1, 'n_estimators': 10}\n",
      "0.728 (+/-0.075) for {'min_samples_leaf': 1, 'n_estimators': 15}\n",
      "0.696 (+/-0.059) for {'min_samples_leaf': 1, 'n_estimators': 20}\n",
      "0.658 (+/-0.062) for {'min_samples_leaf': 3, 'n_estimators': 5}\n",
      "0.690 (+/-0.055) for {'min_samples_leaf': 3, 'n_estimators': 10}\n",
      "0.704 (+/-0.123) for {'min_samples_leaf': 3, 'n_estimators': 15}\n",
      "0.702 (+/-0.060) for {'min_samples_leaf': 3, 'n_estimators': 20}\n",
      "0.646 (+/-0.063) for {'min_samples_leaf': 5, 'n_estimators': 5}\n",
      "0.660 (+/-0.104) for {'min_samples_leaf': 5, 'n_estimators': 10}\n",
      "0.674 (+/-0.078) for {'min_samples_leaf': 5, 'n_estimators': 15}\n",
      "0.684 (+/-0.082) for {'min_samples_leaf': 5, 'n_estimators': 20}\n",
      "0.688 (+/-0.079) for {'min_samples_leaf': 7, 'n_estimators': 5}\n",
      "0.692 (+/-0.075) for {'min_samples_leaf': 7, 'n_estimators': 10}\n",
      "0.690 (+/-0.068) for {'min_samples_leaf': 7, 'n_estimators': 15}\n",
      "0.690 (+/-0.077) for {'min_samples_leaf': 7, 'n_estimators': 20}\n",
      "0.678 (+/-0.079) for {'min_samples_leaf': 1, 'n_estimators': 5}\n",
      "0.698 (+/-0.059) for {'min_samples_leaf': 1, 'n_estimators': 10}\n",
      "0.692 (+/-0.061) for {'min_samples_leaf': 1, 'n_estimators': 15}\n",
      "0.706 (+/-0.089) for {'min_samples_leaf': 1, 'n_estimators': 20}\n",
      "0.650 (+/-0.018) for {'min_samples_leaf': 3, 'n_estimators': 5}\n",
      "0.688 (+/-0.085) for {'min_samples_leaf': 3, 'n_estimators': 10}\n",
      "0.706 (+/-0.095) for {'min_samples_leaf': 3, 'n_estimators': 15}\n",
      "0.706 (+/-0.097) for {'min_samples_leaf': 3, 'n_estimators': 20}\n",
      "0.656 (+/-0.077) for {'min_samples_leaf': 5, 'n_estimators': 5}\n",
      "0.678 (+/-0.112) for {'min_samples_leaf': 5, 'n_estimators': 10}\n",
      "0.706 (+/-0.097) for {'min_samples_leaf': 5, 'n_estimators': 15}\n",
      "0.710 (+/-0.104) for {'min_samples_leaf': 5, 'n_estimators': 20}\n",
      "0.684 (+/-0.105) for {'min_samples_leaf': 7, 'n_estimators': 5}\n",
      "0.682 (+/-0.085) for {'min_samples_leaf': 7, 'n_estimators': 10}\n",
      "0.690 (+/-0.087) for {'min_samples_leaf': 7, 'n_estimators': 15}\n",
      "0.678 (+/-0.116) for {'min_samples_leaf': 7, 'n_estimators': 20}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.64      0.62      0.63     61680\n",
      "       True       0.60      0.62      0.61     55706\n",
      "\n",
      "avg / total       0.62      0.62      0.62    117386\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for score in scores:\n",
    "    \n",
    "    print(\"# Tuning hyper-parameters for %s\" % score + \"\\n\\n\")\n",
    "    \n",
    "    clf = GridSearchCV(RandomForestClassifier(),\n",
    "                       param_grid,\n",
    "                       cv=5,\n",
    "                       scoring=score)\n",
    "    clf.fit(x_train[:500,:], y_train[:500])\n",
    "    print(\"Best parameters set found on development set:\\n\\n\")\n",
    "    print(clf.best_params_)\n",
    "    print(\"\\nGrid scores on development set:\\n\\n\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    \n",
    "    print(\"\\nDetailed classification report:\\n\")\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print(\"\\n\")\n",
    "    y_true, y_pred = y_test, clf.predict(x_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rfc = RandomForestClassifier(max_depth = None,\n",
    "                                   n_estimators = 15,\n",
    "                                   min_samples_leaf = 1)\n",
    "model_rfc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99445202868333116"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "model_rfc.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63728212904434944"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "model_rfc.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazing',\n",
       " 'best',\n",
       " 'great',\n",
       " 'delicious',\n",
       " 'love',\n",
       " 'ok',\n",
       " 'good',\n",
       " 'didn',\n",
       " 'awesome',\n",
       " 'vegas',\n",
       " 'food',\n",
       " 'definitely',\n",
       " 'place',\n",
       " 'favorite',\n",
       " 'excellent',\n",
       " 'worst',\n",
       " 'service',\n",
       " 'bad',\n",
       " 'perfect',\n",
       " 'like']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top n most important features(words)\n",
    "n = 20\n",
    "[words[i] for i in np.argsort(model_rfc.feature_importances_)[::-1][:n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cross Validation to check three methods performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.74870353,  0.74915875,  0.75028752,  0.74974443,  0.7508732 ])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "# naive bayes\n",
    "cv_scores = cross_val_score(model_nb,\n",
    "                            x_train,\n",
    "                            y_train,\n",
    "                            cv = 5,\n",
    "                            scoring=\"accuracy\")\n",
    "execution_time = time.time() - start\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navie Bayes - Execution Time:  18.43092370033264  Accuracy:  0.749753484893\n"
     ]
    }
   ],
   "source": [
    "print(\"Navie Bayes - Execution Time: \",execution_time,\" Accuracy: \",np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.79371519,  0.79588533,  0.79547003,  0.79477787,  0.79414959])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "# logistic regression\n",
    "cv_scores = cross_val_score(model_lrc,\n",
    "                            x_train,\n",
    "                            y_train,\n",
    "                            cv = 5,\n",
    "                            scoring=\"accuracy\")\n",
    "execution_time = time.time() - start\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Execution Time:  21.563192129135132  Accuracy:  0.794799604474\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression - Execution Time: \",execution_time,\" Accuracy: \",np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.77153414,  0.77388508,  0.77318226,  0.7724262 ,  0.77092473])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "# random forest\n",
    "cv_scores = cross_val_score(model_rfc,\n",
    "                            x_train,\n",
    "                            y_train,\n",
    "                            cv = 5,\n",
    "                            scoring=\"accuracy\")\n",
    "execution_time = time.time() - start\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Execution Time:  273.11503529548645  Accuracy:  0.772390485296\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest - Execution Time: \",execution_time,\" Accuracy: \",np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In conclusion, Logistic Regression performs better than Random Forest. It is more 10 times faster tan random forest, but achieved 2 more percentage accuracy on the test set. However, random forest is overfitting. The accuracy of traning set is 99%. If more data are fed into random forest model. Theproblem can be remitted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
