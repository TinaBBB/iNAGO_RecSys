{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Downloading https://files.pythonhosted.org/packages/80/93/d384479da0ead712bdaf697a8399c13a9a89bd856ada5a27d462fb45e47b/geopy-1.20.0-py2.py3-none-any.whl (100kB)\n",
      "Collecting geographiclib<2,>=1.49 (from geopy)\n",
      "  Downloading https://files.pythonhosted.org/packages/8b/62/26ec95a98ba64299163199e95ad1b0e34ad3f4e176e221c40245f211e425/geographiclib-1.50-py3-none-any.whl\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-1.50 geopy-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.sparse import csr_matrix, load_npz, save_npz\n",
    "from tqdm import tqdm\n",
    "import statistics as stats\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import yaml\n",
    "import scipy.sparse as sparse\n",
    "import yaml\n",
    "from tkinter import *\n",
    "import tkinter\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy import distance\n",
    "from geopy import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewJson = \"..\\\\data\\\\Export_CleanedReview.json\"\n",
    "reviewJsonWithClosedRes = \"..\\\\data\\\\Export_CleanedReviewWithClosedRes.json\"\n",
    "reviewJsonToronto = \"..\\\\data\\\\Export_TorontoData.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Select top frenquent user and top frequenty restaurants that had at least 1 review >= 4 stars (Kickking out users that gave all  reviews <=3 and restaurants that never got start >= 4 stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yelp_df(path = 'data/', filename = 'Export_CleanedReview.json', sampling=False, top_user_num=6100, top_item_num=4000):\n",
    "    \"\"\"\n",
    "    Get the pandas dataframe\n",
    "    Sampling only the top users/items by density \n",
    "    Implicit representation applies\n",
    "    \"\"\"\n",
    "    with open(filename,'r') as f:\n",
    "        data = f.readlines()\n",
    "        data = list(map(json.loads, data))\n",
    "    \n",
    "    data = data[0]\n",
    "    #Get all the data from the dggeata file\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df.rename(columns={'stars': 'review_stars', 'text': 'review_text', 'cool': 'review_cool',\n",
    "                       'funny': 'review_funny', 'useful': 'review_useful'},\n",
    "              inplace=True)\n",
    "\n",
    "    df['business_num_id'] = df.business_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.business_id.nunique()))\n",
    "    df['business_num_id'] = df['business_num_id'].astype('int')\n",
    "\n",
    "    df['user_num_id'] = df.user_id.astype('category').\\\n",
    "    cat.rename_categories(range(0, df.user_id.nunique()))\n",
    "    df['user_num_id'] = df['user_num_id'].astype('int')\n",
    "\n",
    "    df['timestamp'] = df['date'].apply(date_to_timestamp)\n",
    "\n",
    "    if sampling:\n",
    "        df = filter_yelp_df(df, top_user_num=top_user_num, top_item_num=top_item_num)\n",
    "        # Refresh num id\n",
    "        df['business_num_id'] = df.business_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.business_id.nunique()))\n",
    "        df['business_num_id'] = df['business_num_id'].astype('int')\n",
    "        \n",
    "        df['user_num_id'] = df.user_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.user_id.nunique()))\n",
    "        df['user_num_id'] = df['user_num_id'].astype('int')\n",
    "#     drop_list = ['date','review_id','review_funny','review_cool','review_useful']\n",
    "#     df = df.drop(drop_list, axis=1)\n",
    "\n",
    "    df = df.reset_index(drop = True)\n",
    "\n",
    "    return df \n",
    "\n",
    "def filter_yelp_df(df, top_user_num=6100, top_item_num=4000):\n",
    "    #Getting the reviews where starts are above 3\n",
    "    df_implicit = df[df['review_stars']>3]\n",
    "    frequent_user_id = df_implicit['user_num_id'].value_counts().head(top_user_num).index.values\n",
    "    frequent_item_id = df_implicit['business_num_id'].value_counts().head(top_item_num).index.values\n",
    "    return df.loc[(df['user_num_id'].isin(frequent_user_id)) & (df['business_num_id'].isin(frequent_item_id))]\n",
    "\n",
    "def date_to_timestamp(date):\n",
    "    dt = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    return time.mktime(dt.timetuple())\n",
    "\n",
    "def df_to_sparse(df, row_name='userId', col_name='movieId', value_name='rating',\n",
    "                 shape=None):\n",
    "    rows = df[row_name]\n",
    "    cols = df[col_name]\n",
    "    if value_name is not None:\n",
    "        values = df[value_name]\n",
    "    else:\n",
    "        values = [1]*len(rows)\n",
    "\n",
    "    return csr_matrix((values, (rows, cols)), shape=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rating-UI and timestamp-UI matrix from original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_timestamp_matrix(df, sampling=False, top_user_num=6100, top_item_num=4000):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #make the df implicit with top frenquent users and \n",
    "    #no need to sample anymore if df was sampled before \n",
    "    if sampling:\n",
    "        df = filter_yelp_df(df, top_user_num=top_user_num, top_item_num=top_item_num)\n",
    "\n",
    "    rating_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "                                 col_name='business_num_id',\n",
    "                                 value_name='review_stars',\n",
    "                                 shape=None)\n",
    "    \n",
    "    #Have same dimension and data entries with rating_matrix, except that the review stars are - user avg\n",
    "#     ratingWuserAvg_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "#                                  col_name='business_num_id',\n",
    "#                                  value_name='reviewStars_userAvg',\n",
    "#                                  shape=None)\n",
    "    \n",
    "    timestamp_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "                                    col_name='business_num_id',\n",
    "                                    value_name='timestamp',\n",
    "                                    shape=None)\n",
    "    \n",
    "    \n",
    "    IC_matrix = get_I_C(df)\n",
    "#     ratingWuserAvg_matrix\n",
    "    return rating_matrix, timestamp_matrix, IC_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_I_C(df):\n",
    "    lst = df.categories.values.tolist()\n",
    "    cat = []\n",
    "    for i in range(len(lst)):\n",
    "        cat.extend(lst[i].split(', '))\n",
    "    unique_cat = set(cat)\n",
    "    #     set categories id\n",
    "    df_cat = pd.DataFrame(list(unique_cat),columns=[\"Categories\"])\n",
    "    df_cat['cat_id'] = df_cat.Categories.astype('category').cat.rename_categories(range(0, df_cat.Categories.nunique()))\n",
    "    dict_cat = df_cat.set_index('Categories')['cat_id'].to_dict()\n",
    "    \n",
    "    df_I_C = pd.DataFrame(columns=['business_num_id', 'cat_id'])\n",
    "    \n",
    "    for i in range((df['business_num_id'].unique().shape)[0]):\n",
    "        df_temp = df[df['business_num_id'] == i].iloc[:1]\n",
    "        temp_lst = df_temp['categories'].to_list()[0].split(\",\")\n",
    "        for j in range(len(temp_lst)):\n",
    "            df_I_C = df_I_C.append({'business_num_id' : i  , 'cat_id' : dict_cat[temp_lst[j].strip()]} , ignore_index=True)\n",
    "    \n",
    "    IC_Matrix = df_to_sparse(df_I_C, row_name='business_num_id',\n",
    "                                 col_name='cat_id',\n",
    "                                 value_name=None,\n",
    "                                 shape=None)    \n",
    "    return IC_Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time ordered split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_ordered_split(rating_matrix, ratingWuserAvg_matrix, timestamp_matrix, ratio=[0.5, 0.2, 0.3],\n",
    "                       implicit=True, remove_empty=False, threshold=3,\n",
    "                       sampling=False, sampling_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Split the data to train,valid,test by time\n",
    "    ratio:  train:valid:test\n",
    "    threshold: for implicit representation\n",
    "    \"\"\"\n",
    "    if implicit:\n",
    "        temp_rating_matrix = sparse.csr_matrix(rating_matrix.shape)\n",
    "        temp_rating_matrix[(rating_matrix > threshold).nonzero()] = 1\n",
    "        rating_matrix = temp_rating_matrix\n",
    "        timestamp_matrix = timestamp_matrix.multiply(rating_matrix)\n",
    "        #ratingWuserAvg_matrix = ratingWuserAvg_matrix.multiply(rating_matrix)\n",
    "\n",
    "    nonzero_index = None\n",
    "\n",
    "    #Default false, not removing empty columns and rows\n",
    "    #Should not have this case, since users should have at least 1 record of 4,5 \n",
    "    #And restuarant should have at least 1 record of 4,5 \n",
    "    if remove_empty:\n",
    "        # Remove empty columns. record original item index\n",
    "        nonzero_index = np.unique(rating_matrix.nonzero()[1])\n",
    "        rating_matrix = rating_matrix[:, nonzero_index]\n",
    "        timestamp_matrix = timestamp_matrix[:, nonzero_index]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[:, nonzero_index]\n",
    "\n",
    "        # Remove empty rows. record original user index\n",
    "        nonzero_rows = np.unique(rating_matrix.nonzero()[0])\n",
    "        rating_matrix = rating_matrix[nonzero_rows]\n",
    "        timestamp_matrix = timestamp_matrix[nonzero_rows]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[nonzero_rows]\n",
    "\n",
    "    user_num, item_num = rating_matrix.shape\n",
    "\n",
    "    rtrain = []\n",
    "    rtrain_userAvg = []\n",
    "    rtime = []\n",
    "    rvalid = []\n",
    "    rvalid_userAvg = []\n",
    "    rtest = []\n",
    "    rtest_userAvg = []\n",
    "    # Get the index list corresponding to item for train,valid,test\n",
    "    item_idx_train = []\n",
    "    item_idx_valid = []\n",
    "    item_idx_test = []\n",
    "    \n",
    "    for i in tqdm(range(user_num)):\n",
    "        #Get the non_zero indexs, restuarants where the user visited/liked if implicit \n",
    "        item_indexes = rating_matrix[i].nonzero()[1]\n",
    "        \n",
    "        #Get the data for the user\n",
    "        data = rating_matrix[i].data\n",
    "        \n",
    "        #Get time stamp value \n",
    "        timestamp = timestamp_matrix[i].data\n",
    "        \n",
    "        #Get review stars with user avg data \n",
    "        if implicit == False:\n",
    "            dataWuserAvg = ratingWuserAvg_matrix[i].data\n",
    "        \n",
    "        #Non zero reviews for this user\n",
    "        num_nonzeros = len(item_indexes)\n",
    "        \n",
    "        #If the user has at least one review\n",
    "        if num_nonzeros >= 1:\n",
    "            #Get number of test and valid data \n",
    "            #train is 30%\n",
    "            num_test = int(num_nonzeros * ratio[2])\n",
    "            #validate is 50%\n",
    "            num_valid = int(num_nonzeros * (ratio[1] + ratio[2]))\n",
    "\n",
    "            valid_offset = num_nonzeros - num_valid\n",
    "            test_offset = num_nonzeros - num_test\n",
    "\n",
    "            #Sort the timestamp for each review for the user\n",
    "            argsort = np.argsort(timestamp)\n",
    "            \n",
    "            #Sort the reviews for the user according to the time stamp \n",
    "            data = data[argsort]\n",
    "            \n",
    "            #Sort the review with user avg accoridng to the time stamp\n",
    "            if implicit == False:\n",
    "                dataWuserAvg = dataWuserAvg[argsort]\n",
    "            \n",
    "            #Non-zero review index sort according to time\n",
    "            item_indexes = item_indexes[argsort]\n",
    "            \n",
    "            #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "            rtrain.append([data[:valid_offset], np.full(valid_offset, i), item_indexes[:valid_offset]])\n",
    "            \n",
    "            if implicit == False:\n",
    "                #Changing valid set to binary\n",
    "                count=valid_offset\n",
    "                for eachData in data[valid_offset:test_offset]:\n",
    "                    #if rating-avgRating > 0 then like\n",
    "                    if eachData >= 4:\n",
    "                        data[count] = 1\n",
    "                    else:\n",
    "                        data[count] = 0\n",
    "                    count += 1\n",
    "                \n",
    "            #50%-70%\n",
    "            rvalid.append([data[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                           item_indexes[valid_offset:test_offset]])\n",
    "            #remaining 30%\n",
    "            rtest.append([data[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "            \n",
    "            if implicit == False:\n",
    "                #Now for the rating matrix that considers user average rating\n",
    "                #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "                rtrain_userAvg.append([dataWuserAvg[:valid_offset], np.full(valid_offset, i), item_indexes[:valid_offset]])\n",
    "                #50%-70%\n",
    "\n",
    "                #Changing valid set to binary\n",
    "                count=valid_offset\n",
    "                for eachData in dataWuserAvg[valid_offset:test_offset]:\n",
    "                    #if rating-avgRating > 0 then like\n",
    "                    if eachData > 0:\n",
    "                        dataWuserAvg[count] = 1\n",
    "                    else:\n",
    "                        dataWuserAvg[count] = 0\n",
    "                    count += 1\n",
    "\n",
    "                rvalid_userAvg.append([dataWuserAvg[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                               item_indexes[valid_offset:test_offset]])\n",
    "\n",
    "                #Change test set to binary even we don't use it\n",
    "                countTest = test_offset\n",
    "                for eachData in dataWuserAvg[test_offset:]:\n",
    "                    #if rating-avgRating > 0 then like\n",
    "                    if eachData > 0:\n",
    "                        dataWuserAvg[count] = 1\n",
    "                    else:\n",
    "                        dataWuserAvg[count] = 0\n",
    "                    count += 1\n",
    "\n",
    "\n",
    "                #remaining 30%\n",
    "                rtest_userAvg.append([dataWuserAvg[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "                \n",
    "            item_idx_train.append(item_indexes[:valid_offset])\n",
    "            \n",
    "#             item_idx_valid.append(item_indexes[valid_offset:test_offset])\n",
    "#             item_idx_test.append(item_indexes[test_offset:])\n",
    "        else:\n",
    "            item_idx_train.append([])\n",
    "#             item_idx_valid.append([])\n",
    "#             item_idx_test.append([])\n",
    "    \n",
    "    rtrain = np.array(rtrain)\n",
    "    rvalid = np.array(rvalid)\n",
    "    rtest = np.array(rtest)\n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = np.array(rtrain_userAvg)\n",
    "        rvalid_userAvg = np.array(rvalid_userAvg)\n",
    "        rtest_userAvg = np.array(rtest_userAvg)\n",
    "\n",
    "    #take non-zeros values, row index, and column (non-zero) index and store into sparse matrix\n",
    "    rtrain = sparse.csr_matrix((np.hstack(rtrain[:, 0]), (np.hstack(rtrain[:, 1]), np.hstack(rtrain[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rvalid = sparse.csr_matrix((np.hstack(rvalid[:, 0]), (np.hstack(rvalid[:, 1]), np.hstack(rvalid[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rtest = sparse.csr_matrix((np.hstack(rtest[:, 0]), (np.hstack(rtest[:, 1]), np.hstack(rtest[:, 2]))),\n",
    "                              shape=rating_matrix.shape, dtype=np.float32)\n",
    "    \n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = sparse.csr_matrix((np.hstack(rtrain_userAvg[:, 0]), (np.hstack(rtrain_userAvg[:, 1]), np.hstack(rtrain_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rvalid_userAvg = sparse.csr_matrix((np.hstack(rvalid_userAvg[:, 0]), (np.hstack(rvalid_userAvg[:, 1]), np.hstack(rvalid_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rtest_userAvg = sparse.csr_matrix((np.hstack(rtest_userAvg[:, 0]), (np.hstack(rtest_userAvg[:, 1]), np.hstack(rtest_userAvg[:, 2]))),\n",
    "                                  shape=rating_matrix.shape, dtype=np.float32)\n",
    "\n",
    "    return rtrain, rvalid, rtest,rtrain_userAvg, rvalid_userAvg, rtest_userAvg, nonzero_index, timestamp_matrix, item_idx_train, item_idx_valid, item_idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_ordered_splitModified(rating_matrix, ratingWuserAvg_matrix, timestamp_matrix, ratio=[0.5, 0.2, 0.3],\n",
    "                       implicit=True, remove_empty=False, threshold=3,\n",
    "                       sampling=False, sampling_ratio=0.1, trainSampling=1):\n",
    "    \"\"\"\n",
    "    Split the data to train,valid,test by time\n",
    "    ratio:  train:valid:test\n",
    "    threshold: for implicit representation\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if implicit:\n",
    "        temp_rating_matrix = sparse.csr_matrix(rating_matrix.shape)\n",
    "        temp_rating_matrix[(rating_matrix > threshold).nonzero()] = 1\n",
    "        rating_matrix = temp_rating_matrix\n",
    "        timestamp_matrix = timestamp_matrix.multiply(rating_matrix)\n",
    "        #ratingWuserAvg_matrix = ratingWuserAvg_matrix.multiply(rating_matrix)\n",
    "\n",
    "    nonzero_index = None\n",
    "\n",
    "    #Default false, not removing empty columns and rows\n",
    "    #Should not have this case, since users should have at least 1 record of 4,5 \n",
    "    #And restuarant should have at least 1 record of 4,5 \n",
    "    if remove_empty:\n",
    "        # Remove empty columns. record original item index\n",
    "        nonzero_index = np.unique(rating_matrix.nonzero()[1])\n",
    "        rating_matrix = rating_matrix[:, nonzero_index]\n",
    "        timestamp_matrix = timestamp_matrix[:, nonzero_index]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[:, nonzero_index]\n",
    "\n",
    "        # Remove empty rows. record original user index\n",
    "        nonzero_rows = np.unique(rating_matrix.nonzero()[0])\n",
    "        rating_matrix = rating_matrix[nonzero_rows]\n",
    "        timestamp_matrix = timestamp_matrix[nonzero_rows]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[nonzero_rows]\n",
    "\n",
    "    user_num, item_num = rating_matrix.shape\n",
    "\n",
    "    rtrain = []\n",
    "    rtrain_userAvg = []\n",
    "    rtime = []\n",
    "    rvalid = []\n",
    "    rvalid_userAvg = []\n",
    "    rtest = []\n",
    "    rtest_userAvg = []\n",
    "    # Get the index list corresponding to item for train,valid,test\n",
    "    item_idx_train = []\n",
    "    item_idx_valid = []\n",
    "    item_idx_test = []\n",
    "    \n",
    "    for i in tqdm(range(user_num)):\n",
    "        #Get the non_zero indexs, restuarants where the user visited/liked if implicit \n",
    "        item_indexes = rating_matrix[i].nonzero()[1]        \n",
    "        #Get the data for the user\n",
    "        data = rating_matrix[i].data      \n",
    "        #Get time stamp value \n",
    "        timestamp = timestamp_matrix[i].data \n",
    "        #Get review stars with user avg data \n",
    "        if implicit == False:\n",
    "            dataWuserAvg = ratingWuserAvg_matrix[i].data\n",
    "\n",
    "            \n",
    "        #Non zero reviews for this user\n",
    "        num_nonzeros = len(item_indexes)\n",
    "        \n",
    "        #If the user has at least one review\n",
    "        if num_nonzeros >= 1:\n",
    "            num_test = int(num_nonzeros * ratio[2])\n",
    "            num_valid = int(num_nonzeros * (ratio[1] + ratio[2]))\n",
    "            valid_offset = num_nonzeros - num_valid\n",
    "            \n",
    "            # Adding this for sampling for training set\n",
    "            valid_offsetSample = int(valid_offset*trainSampling)\n",
    "            test_offset = num_nonzeros - num_test\n",
    "            \n",
    "            #Sort the timestamp for each review for the user\n",
    "            argsort = np.argsort(timestamp)\n",
    "            \n",
    "            #Sort the reviews for the user according to the time stamp \n",
    "            data = data[argsort]\n",
    "            \n",
    "            #Sort the review with user avg accoridng to the time stamp\n",
    "            if implicit == False:\n",
    "                dataWuserAvg = dataWuserAvg[argsort]\n",
    "            \n",
    "            #Non-zero review index sort according to time\n",
    "            item_indexes = item_indexes[argsort]\n",
    "            \n",
    "            #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "            #if take from old to new\n",
    "            #rtrain.append([data[:valid_offsetSample], np.full(valid_offsetSample, i), item_indexes[:valid_offsetSample]])\n",
    "            #if take from new to old\n",
    "            rtrain.append([data[valid_offset-valid_offsetSample:valid_offset], np.full(valid_offsetSample, i), item_indexes[valid_offset-valid_offsetSample:valid_offset]])\n",
    "            rvalid.append([data[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                           item_indexes[valid_offset:test_offset]])\n",
    "            rtest.append([data[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "            \n",
    "            if implicit == False:\n",
    "                #Now for the rating matrix that considers user average rating\n",
    "                #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "                #from old to new\n",
    "                #rtrain_userAvg.append([dataWuserAvg[:valid_offsetSample], np.full(valid_offsetSample, i), item_indexes[:valid_offsetSample]])\n",
    "                #take nearest\n",
    "                rtrain_userAvg.append([dataWuserAvg[valid_offset-valid_offsetSample:valid_offset], np.full(valid_offsetSample, i), item_indexes[valid_offset-valid_offsetSample:valid_offset]])                \n",
    "                    \n",
    "                rvalid_userAvg.append([dataWuserAvg[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                               item_indexes[valid_offset:test_offset]])\n",
    "                \n",
    "                rtest_userAvg.append([dataWuserAvg[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "                \n",
    "            item_idx_train.append(item_indexes[:valid_offsetSample])\n",
    "            \n",
    "        else:\n",
    "            item_idx_train.append([])\n",
    "    \n",
    "    rtrain = np.array(rtrain)\n",
    "    rvalid = np.array(rvalid)\n",
    "    rtest = np.array(rtest)\n",
    "   \n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = np.array(rtrain_userAvg)\n",
    "        rvalid_userAvg = np.array(rvalid_userAvg)\n",
    "        rtest_userAvg = np.array(rtest_userAvg)\n",
    "\n",
    "    #take non-zeros values, row index, and column (non-zero) index and store into sparse matrix\n",
    "    rtrain = sparse.csr_matrix((np.hstack(rtrain[:, 0]), (np.hstack(rtrain[:, 1]), np.hstack(rtrain[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rvalid = sparse.csr_matrix((np.hstack(rvalid[:, 0]), (np.hstack(rvalid[:, 1]), np.hstack(rvalid[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rtest = sparse.csr_matrix((np.hstack(rtest[:, 0]), (np.hstack(rtest[:, 1]), np.hstack(rtest[:, 2]))),\n",
    "                              shape=rating_matrix.shape, dtype=np.float32)\n",
    "    \n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = sparse.csr_matrix((np.hstack(rtrain_userAvg[:, 0]), (np.hstack(rtrain_userAvg[:, 1]), np.hstack(rtrain_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rvalid_userAvg = sparse.csr_matrix((np.hstack(rvalid_userAvg[:, 0]), (np.hstack(rvalid_userAvg[:, 1]), np.hstack(rvalid_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rtest_userAvg = sparse.csr_matrix((np.hstack(rtest_userAvg[:, 0]), (np.hstack(rtest_userAvg[:, 1]), np.hstack(rtest_userAvg[:, 2]))),\n",
    "                                  shape=rating_matrix.shape, dtype=np.float32)\n",
    "\n",
    "    return rtrain, rvalid, rtest,rtrain_userAvg, rvalid_userAvg, rtest_userAvg, nonzero_index, timestamp_matrix, item_idx_train, item_idx_valid, item_idx_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get df for training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Item idex matrix stores the reivews starts\n",
    "#This function returns a list of index for the reviews included in training set \n",
    "def get_corpus_idx_list(df, item_idx_matrix):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    df: total dataframe\n",
    "    item_idx_matrix: train index list got from time_split \n",
    "    Output: row index in original dataframe for training data by time split\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    #For all the users: 5791\n",
    "    for i in tqdm(range(len(item_idx_matrix))):\n",
    "        \n",
    "        #find row index where user_num_id is i\n",
    "        a = df.index[df['user_num_id'] == i].tolist()\n",
    "        \n",
    "        #loop through the busienss id that the user i reviewed for in offvalid set \n",
    "        for item_idx in  item_idx_matrix[i]:\n",
    "            \n",
    "            #get the row index for reviews for business that the user liked in the train set\n",
    "            b = df.index[df['business_num_id'] == item_idx].tolist()\n",
    "            \n",
    "            #Find the index for which this user liked, one user only rate a business once\n",
    "            idx_to_add = list(set(a).intersection(b))\n",
    "            \n",
    "            if idx_to_add not in lst:\n",
    "                lst.extend(idx_to_add)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess using Term Frequency - CounterVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shenti10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shenti10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Stemming and Lemmatisation\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Get corpus and CountVector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "new_words = ['not_the']\n",
    "stop_words = stop_words.union(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#Should 'because' added?\n",
    "def preprocess(df, reset_list = [',','.','?',';','however','but']):\n",
    "    corpus = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        text = df['review_text'][i]\n",
    "        change_flg = 0\n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        ##Convert to list from string, loop through the review text\n",
    "        text = text.split()\n",
    "        \n",
    "        #any sentence that encounters a not, the folloing words will become not phrase until hit the sentence end\n",
    "        for j in range(len(text)):\n",
    "            #Make the not_ hack\n",
    "            if text[j] == 'not':\n",
    "                change_flg = 1\n",
    "#                 print 'changes is made after ', i\n",
    "                continue\n",
    "            #if was 1 was round and not hit a 'not' in this round\n",
    "            if change_flg == 1 and any(reset in text[j] for reset in reset_list):\n",
    "                text[j] = 'not_' + text[j]\n",
    "                change_flg = 0\n",
    "#                 print 'reset at ', i\n",
    "            if change_flg == 1:\n",
    "                text[j] = 'not_' + text[j]\n",
    "        \n",
    "        #Convert back to string\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "        #Remove punctuations\n",
    "#       text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        \n",
    "        #remove tags\n",
    "        text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "        \n",
    "        # remove special characters and digits\n",
    "        text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "        \n",
    "        ##Convert to list from string\n",
    "        text = text.split()\n",
    "        \n",
    "        ##Stemming\n",
    "        ps=PorterStemmer()\n",
    "        \n",
    "        #Lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        text = [lem.lemmatize(word) for word in text if not word in  \n",
    "                stop_words] \n",
    "        text = \" \".join(text)\n",
    "        corpus.append(text)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def train(matrix_train):\n",
    "    similarity = cosine_similarity(X=matrix_train, Y=None, dense_output=True)\n",
    "    return similarity\n",
    "\n",
    "def get_I_K(df, X, row_name = 'business_num_id', binary = True, shape = (121994,6000)):\n",
    "    \"\"\"\n",
    "    get the item-keyphrase matrix\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    cols = []\n",
    "    vals = []\n",
    "    #For each review history\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        #Get the array of frequencies for document/review i \n",
    "        arr = X[i].toarray() \n",
    "        nonzero_element = arr.nonzero()[1]  # Get nonzero element in each line, keyphrase that appears index \n",
    "        length_of_nonzero = len(nonzero_element) #number of important keyphrase that appears\n",
    "        \n",
    "        # df[row_name][i] is the item idex\n",
    "        #Get a list row index that indicates the document/review\n",
    "        rows.extend(np.array([df[row_name][i]]*length_of_nonzero)) ## Item index\n",
    "        #print(rows)\n",
    "        \n",
    "        #Get a list of column index indicating the key phrase that appears in i document/review\n",
    "        cols.extend(nonzero_element) ## Keyword Index\n",
    "        if binary:\n",
    "            #Create a bunch of 1s\n",
    "            vals.extend(np.array([1]*length_of_nonzero))\n",
    "        else:\n",
    "            #If not binary \n",
    "            vals.extend(arr[arr.nonzero()])    \n",
    "    return csr_matrix((vals, (rows, cols)), shape=shape)\n",
    "\n",
    "\n",
    "#Get a UI matrix if it's not item_similarity based or else IU\n",
    "def predict(matrix_train, k, similarity, item_similarity_en = False):\n",
    "    prediction_scores = []\n",
    "    \n",
    "    #inverse to IU matrix\n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    #for each user or item, depends UI or IU \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "        # Get user u's prediction scores for all items\n",
    "        #Get prediction/similarity score for each user 1*num or user or num of items\n",
    "        vector_u = similarity[user_index]\n",
    "\n",
    "        # Get closest K neighbors excluding user u self\n",
    "        #Decending accoding to similarity score, select top k\n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        \n",
    "        # Get neighbors similarity weights and ratings\n",
    "        similar_users_weights = similarity[user_index][similar_users]\n",
    "        \n",
    "        #similar_users_weights_sum = np.sum(similar_users_weights)\n",
    "        #print(similar_users_weights.shape)\n",
    "        #shape: num of res * k\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "              \n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "        #print(prediction_scores_u)\n",
    "        \n",
    "        \n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "        \n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    return res\n",
    "\n",
    "\n",
    "#Preidction score is UI or IU?\n",
    "def prediction(prediction_score, topK, matrix_Train):\n",
    "\n",
    "    prediction = []\n",
    "\n",
    "    #for each user\n",
    "    for user_index in tqdm(range(matrix_Train.shape[0])):\n",
    "        \n",
    "        #take the prediction scores for user 1 * num res\n",
    "        vector_u = prediction_score[user_index]\n",
    "        \n",
    "        #The restuarant the user rated\n",
    "        vector_train = matrix_Train[user_index]\n",
    "        \n",
    "        if len(vector_train.nonzero()[0]) > 0:\n",
    "            vector_predict = sub_routine(vector_u, vector_train, topK=topK)\n",
    "        else:\n",
    "            vector_predict = np.zeros(topK, dtype=np.float32)\n",
    "\n",
    "        prediction.append(vector_predict)\n",
    "\n",
    "    return np.vstack(prediction)\n",
    "\n",
    "#topK: the number of restuarants we are suggesting \n",
    "#if vector_train has number, then the user has visited\n",
    "def sub_routine(vector_u, vector_train, topK=500):\n",
    "\n",
    "    #index where non-zero\n",
    "    train_index = vector_train.nonzero()[1]\n",
    "    \n",
    "    vector_u = vector_u\n",
    "    \n",
    "    #get topk + num rated res prediction score descending, top index \n",
    "    candidate_index = np.argpartition(-vector_u, topK+len(train_index))[:topK+len(train_index)]\n",
    "    \n",
    "    #sort top prediction score index in range topK+len(train_index) into vector_u`\n",
    "    vector_u = candidate_index[vector_u[candidate_index].argsort()[::-1]]\n",
    "    \n",
    "    #deleted the rated res from the topk+train_index prediction score vector for user u \n",
    "    #Delete the user rated res index from the topk+numRated index\n",
    "    vector_u = np.delete(vector_u, np.isin(vector_u, train_index).nonzero()[0])\n",
    "\n",
    "    #so we only include the top K prediction score here\n",
    "    return vector_u[:topK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recallk(vector_true_dense, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "\n",
    "def precisionk(vector_predict, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_predict)\n",
    "\n",
    "\n",
    "def average_precisionk(vector_predict, hits, **unused):\n",
    "    precisions = np.cumsum(hits, dtype=np.float32)/range(1, len(vector_predict)+1)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "def r_precision(vector_true_dense, vector_predict, **unused):\n",
    "    vector_predict_short = vector_predict[:len(vector_true_dense)]\n",
    "    hits = len(np.isin(vector_predict_short, vector_true_dense).nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "\n",
    "def _dcg_support(size):\n",
    "    arr = np.arange(1, size+1)+1\n",
    "    return 1./np.log2(arr)\n",
    "\n",
    "\n",
    "def ndcg(vector_true_dense, vector_predict, hits):\n",
    "    idcg = np.sum(_dcg_support(len(vector_true_dense)))\n",
    "    dcg_base = _dcg_support(len(vector_predict))\n",
    "    dcg_base[np.logical_not(hits)] = 0\n",
    "    dcg = np.sum(dcg_base)\n",
    "    return dcg/idcg\n",
    "\n",
    "\n",
    "def click(hits, **unused):\n",
    "    first_hit = next((i for i, x in enumerate(hits) if x), None)\n",
    "    if first_hit is None:\n",
    "        return 5\n",
    "    else:\n",
    "        return first_hit/10\n",
    "\n",
    "\n",
    "def evaluate(matrix_Predict, matrix_Test, metric_names =['R-Precision', 'NDCG', 'Precision', 'Recall', 'MAP'], atK = [5, 10, 15, 20, 50], analytical=False):\n",
    "    \"\"\"\n",
    "    :param matrix_U: Latent representations of users, for LRecs it is RQ, for ALSs it is U\n",
    "    :param matrix_V: Latent representations of items, for LRecs it is Q, for ALSs it is V\n",
    "    :param matrix_Train: Rating matrix for training, features.\n",
    "    :param matrix_Test: Rating matrix for evaluation, true labels.\n",
    "    :param k: Top K retrieval\n",
    "    :param metric_names: Evaluation metrics\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global_metrics = {\n",
    "        #\"R-Precision\": r_precision,\n",
    "        #\"NDCG\": ndcg,\n",
    "        #\"Clicks\": click\n",
    "    }\n",
    "\n",
    "    local_metrics = {\n",
    "        #\"Precision\": precisionk,\n",
    "        #\"Recall\": recallk,\n",
    "        \"MAP\": average_precisionk\n",
    "    }\n",
    "\n",
    "    output = dict()\n",
    "\n",
    "    num_users = matrix_Predict.shape[0]\n",
    "\n",
    "    for k in atK:\n",
    "\n",
    "        local_metric_names = list(set(metric_names).intersection(local_metrics.keys()))\n",
    "        results = {name: [] for name in local_metric_names}\n",
    "        topK_Predict = matrix_Predict[:, :k]\n",
    "\n",
    "        for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "            vector_predict = topK_Predict[user_index]\n",
    "            if len(vector_predict.nonzero()[0]) > 0:\n",
    "                vector_true = matrix_Test[user_index]\n",
    "                vector_true_dense = vector_true.nonzero()[1]\n",
    "                hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "                if vector_true_dense.size > 0:\n",
    "                    for name in local_metric_names:\n",
    "                        results[name].append(local_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                                 vector_predict=vector_predict,\n",
    "                                                                 hits=hits))\n",
    "        results_summary = dict()\n",
    "        if analytical:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = round(results[name],4)\n",
    "        else:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = (round((np.average(results[name])),4),\n",
    "                                                              round((1.96*np.std(results[name])/np.sqrt(num_users)),4))\n",
    "        output.update(results_summary)\n",
    "\n",
    "    global_metric_names = list(set(metric_names).intersection(global_metrics.keys()))\n",
    "    results = {name: [] for name in global_metric_names}\n",
    "\n",
    "    topK_Predict = matrix_Predict[:]\n",
    "\n",
    "    for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "        vector_predict = topK_Predict[user_index]\n",
    "\n",
    "        if len(vector_predict.nonzero()[0]) > 0:\n",
    "            vector_true = matrix_Test[user_index]\n",
    "            vector_true_dense = vector_true.nonzero()[1]\n",
    "            hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "            # if user_index == 1:\n",
    "            #     import ipdb;\n",
    "            #     ipdb.set_trace()\n",
    "\n",
    "            if vector_true_dense.size > 0:\n",
    "                for name in global_metric_names:\n",
    "                    results[name].append(global_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                              vector_predict=vector_predict,\n",
    "                                                              hits=hits))\n",
    "    results_summary = dict()\n",
    "    if analytical:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = round(results[name],4)\n",
    "    else:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = (round(np.average(results[name]),4), round((1.96*np.std(results[name])/np.sqrt(num_users)),4))\n",
    "    output.update(results_summary)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add 2 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility functions\n",
    "#3906 restuarant, 3000 keyphrase, 5791 user \n",
    "def add_two_matrix(ratio, U_I_matrix,I_K_matrix, shape = (3906, 3000+5791)):\n",
    "    # ratio determine Keywords/User in the matrix\n",
    "    rows = []\n",
    "    cols = []\n",
    "    datas = []\n",
    "    I_U_matrix = U_I_matrix.transpose()\n",
    "    \n",
    "    #for each restuarant\n",
    "    for i in tqdm(range(I_K_matrix.shape[0])):\n",
    "        #key phrase that this item has, column(key phrase) index\n",
    "        nonzero1 = I_K_matrix[i].nonzero()\n",
    "        \n",
    "        #user that rated this item, column(user) index \n",
    "        nonzero2 = I_U_matrix[i].nonzero()\n",
    "        \n",
    "        #Trying to create a sparse matrix that stores \n",
    "        #index of restuarant for (K + U) times\n",
    "        row = [i]*(len(nonzero1[1])+len(nonzero2[1]))\n",
    "        \n",
    "        #column index for key phrase and users that are non-zero\n",
    "        col = nonzero1[1].tolist()+ nonzero2[1].tolist()\n",
    "        \n",
    "        \n",
    "        data = [ratio]*len(nonzero1[1])+[1-ratio]*len(nonzero2[1]) # Binary representation of I-K/U matrix\n",
    "        \n",
    "        rows.extend(row)\n",
    "        cols.extend(col)\n",
    "        datas.extend(data)\n",
    "    return csr_matrix( (datas,(rows,cols)), shape=shape )\n",
    "\n",
    "def transfer_to_implicit(rating_matrix, threshold = 0):\n",
    "    temp_rating_matrix = sparse.csr_matrix(rating_matrix.shape)\n",
    "    temp_rating_matrix[(rating_matrix > threshold).nonzero()] = 1\n",
    "    rating_matrix = temp_rating_matrix\n",
    "    return rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImplicitMatrix(sparseMatrix, threashold=0):\n",
    "    temp_matrix = sparse.csr_matrix(sparseMatrix.shape)\n",
    "    temp_matrix[(sparseMatrix > threashold).nonzero()] = 1\n",
    "    return temp_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictUU(matrix_train, k, chooseWeigthMethod, similarity1=None, similarity2=None, similarity3=None, similarity4=None, similarity5=None, item_similarity_en = False):\n",
    "    prediction_scores = []\n",
    "    #Convert from list to ndarray, add an axis\n",
    "    if isinstance(chooseWeigthMethod, list):\n",
    "        chooseWeigthMethod = np.array(chooseWeigthMethod)[:, np.newaxis]\n",
    "   \n",
    "    \"make sure that when passing in chooseWeightMethod, the weight must be aligned with similarity metrices, even if set to None\"\n",
    "    \"They should add to 1 as well\"\n",
    "    #inverse to IU matrix\n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    #for each user or item, depends UI or IU \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "    #for user_index in tqdm(range(10,20)):\n",
    "        \n",
    "        numberSimilarMatrix = 0\n",
    "        # Get user u's prediction scores for all items \n",
    "        #Get prediction/similarity score for each user 1*num or user or num of items\n",
    "        if similarity1 is not None:\n",
    "            vector_u1 = similarity1[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u1 = [0]*matrix_train.shape[0]\n",
    "            \n",
    "        if similarity2 is not None:\n",
    "            vector_u2 = similarity2[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u2 = [0]*len(vector_u1)\n",
    "            \n",
    "        if similarity3 is not None:\n",
    "            vector_u3 = similarity3[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u3 = [0]*len(vector_u1)\n",
    "            \n",
    "        if similarity4 is not None:\n",
    "            vector_u4 = similarity4[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u4 = [0]*len(vector_u1)\n",
    "        \n",
    "        if similarity5 is not None:\n",
    "            vector_u5 = similarity5[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u5 = [0]*len(vector_u1)\n",
    "        \n",
    "        #Temperary vector that stacks all 4 vectors together\n",
    "        tempVector = np.array([vector_u1,vector_u2,vector_u3,vector_u4, vector_u5])\n",
    "        \n",
    "        if chooseWeigthMethod is None:\n",
    "            #Get the similarity score from the first similarity matrix anyways \n",
    "            vector_u = vector_u1.copy()\n",
    "            \n",
    "        #If we are choosing the max, min, avg or similarity scores\n",
    "        if chooseWeigthMethod is not None:\n",
    "            if chooseWeigthMethod == 'max':\n",
    "                vector_u = tempVector.max(axis=0)\n",
    "            elif chooseWeigthMethod == 'min':\n",
    "                vector_u = tempVector.min(axis=0)\n",
    "            elif chooseWeigthMethod == 'average':\n",
    "                vector_u = tempVector.mean(axis=0)\n",
    "            elif isinstance(chooseWeigthMethod, np.ndarray):\n",
    "                #Validate that number of weights passed in equals number of matrices\n",
    "                #assert(len(chooseWeigthMethod) == numberSimilarMatrix)\n",
    "                #Get the new combined similarity vector \n",
    "                weighted_u = tempVector * chooseWeigthMethod\n",
    "                vector_u =np.sum(weighted_u, axis=0)\n",
    "                #print((vector_u == vector_u4).sum())\n",
    "                \n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        \n",
    "        # Get neighbors similarity weights and ratings\n",
    "        #similar_users_weights = similarity1[user_index][similar_users]\n",
    "        similar_users_weights = vector_u[similar_users]\n",
    "        \n",
    "        #similar_users_weights_sum = np.sum(similar_users_weights)\n",
    "        #print(similar_users_weights.shape)\n",
    "        #shape: num of res * k\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "              \n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "        #print(prediction_scores_u)\n",
    "        \n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "        \n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get original dataframe out of the review datastet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_yelp_df(path ='', filename=reviewJsonToronto, sampling= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get rating-UI matrix and timestepm-UI matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix, timestamp_matrix , I_C_matrix = get_rating_timestamp_matrix(df)\n",
    "\n",
    "# get ratingWuserAvg_matrix\n",
    "rating_array = rating_matrix.toarray()\n",
    "user_average_array = rating_array.sum(axis = 1)/np.count_nonzero(rating_array,axis = 1)\n",
    "init_UI = np.zeros(rating_array.shape)\n",
    "init_UI[rating_array.nonzero()] = 1\n",
    "for i in range(user_average_array.shape[0]):\n",
    "    init_UI[i] = init_UI[i] * (user_average_array[i]-0.001) \n",
    "user_average_array = init_UI\n",
    "ratingWuserAvg_array = rating_array - user_average_array\n",
    "ratingWuserAvg_matrix=sparse.csr_matrix(ratingWuserAvg_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split to get rtrain-UI matrix and valid and test.. item_index_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  del sys.path[0]\n",
      "100%|| 6100/6100 [00:01<00:00, 4332.50it/s]\n"
     ]
    }
   ],
   "source": [
    "rtrain_implicit, rvalid_implicit, rtest_implicit, rtrain_userAvg_implicit, rvalid_userAvg_implicit, rtest_userAvg_implicit, nonzero_index, rtime, item_idx_matrix_train_implicit,item_idx_matrix_valid_implicit, item_idx_matrix_test_implicit = time_ordered_splitModified(rating_matrix=rating_matrix, ratingWuserAvg_matrix=ratingWuserAvg_matrix, timestamp_matrix=timestamp_matrix,\n",
    "                                                                     ratio=[0.5,0.2,0.3],\n",
    "                                                                     implicit=True,\n",
    "                                                                     remove_empty=False, threshold=3,sampling=False, \n",
    "                                                                     sampling_ratio=0.1, trainSampling=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6100/6100 [00:01<00:00, 3568.11it/s]\n"
     ]
    }
   ],
   "source": [
    "rtrain, rvalid, rtest, rtrain_userAvg, rvalid_userAvg, rtest_userAvg, nonzero_index, rtime, item_idx_matrix_train,item_idx_matrix_valid, item_idx_matrix_test = time_ordered_splitModified(rating_matrix=rating_matrix, ratingWuserAvg_matrix=ratingWuserAvg_matrix, timestamp_matrix=timestamp_matrix,\n",
    "                                                                     ratio=[0.5,0.2,0.3],\n",
    "                                                                     implicit=False,\n",
    "                                                                     remove_empty=False, threshold=3,\n",
    "                                                                     sampling=False, sampling_ratio=0.1, \n",
    "                                                                     trainSampling=0.95)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get df shrink to df_train for rtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6100/6100 [01:34<00:00, 64.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(84805, 43)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the list of row index for the training set \n",
    "lst_train = get_corpus_idx_list(df, item_idx_matrix_train)\n",
    "\n",
    "# Get the training dataframe from the original dataframe\n",
    "df_train = df.loc[lst_train]\n",
    "\n",
    "#Resetting the index of the train data\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>Unnamed: 0_y</th>\n",
       "      <th>Updated</th>\n",
       "      <th>Year</th>\n",
       "      <th>alias</th>\n",
       "      <th>business_id</th>\n",
       "      <th>business_stars</th>\n",
       "      <th>categories</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>date</th>\n",
       "      <th>display_phone</th>\n",
       "      <th>distance</th>\n",
       "      <th>friend_count</th>\n",
       "      <th>ghost</th>\n",
       "      <th>image_url</th>\n",
       "      <th>img_dsc</th>\n",
       "      <th>img_url</th>\n",
       "      <th>is_closed</th>\n",
       "      <th>location</th>\n",
       "      <th>name</th>\n",
       "      <th>nr</th>\n",
       "      <th>phone</th>\n",
       "      <th>photo_count</th>\n",
       "      <th>price</th>\n",
       "      <th>review_count_x</th>\n",
       "      <th>review_count_y</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_language</th>\n",
       "      <th>review_stars</th>\n",
       "      <th>review_text</th>\n",
       "      <th>transactions</th>\n",
       "      <th>ufc</th>\n",
       "      <th>url</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_loc</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>business_num_id</th>\n",
       "      <th>user_num_id</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>628892</td>\n",
       "      <td>628892</td>\n",
       "      <td>8039</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>bobbette-and-belle-toronto</td>\n",
       "      <td>vcxvQyAggPqxcHwvJXvjGg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[{'alias': 'catering', 'title': 'Caterers'}, {...</td>\n",
       "      <td>{'latitude': 43.66201, 'longitude': -79.33476}</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>+1 416-466-8800</td>\n",
       "      <td>7046.832404</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>https://s3-media3.fl.yelpcdn.com/bphoto/7DRlzP...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>{'address1': '1121 Queen Street E', 'address2'...</td>\n",
       "      <td>Bobbette &amp; Belle</td>\n",
       "      <td>False</td>\n",
       "      <td>1.416467e+10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>$$</td>\n",
       "      <td>78</td>\n",
       "      <td>154</td>\n",
       "      <td>['1', '13', '2017']</td>\n",
       "      <td>9e3cmOBflFYd98XZVN8YXQ</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Food and drink: Loved the alfajores (mini lemo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://www.yelp.com/biz/bobbette-and-belle-to...</td>\n",
       "      <td>--BumyUHiO_7YsHurb9Hkw</td>\n",
       "      <td>Old Toronto, Toronto, Canada</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3725</td>\n",
       "      <td>0</td>\n",
       "      <td>1.484284e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>582871</td>\n",
       "      <td>582871</td>\n",
       "      <td>15015</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>pai-northern-thai-kitchen-toronto-5</td>\n",
       "      <td>r_BrIgzYcwo1NAuG9dLbpg</td>\n",
       "      <td>4.5</td>\n",
       "      <td>[{'alias': 'thai', 'title': 'Thai'}]</td>\n",
       "      <td>{'latitude': 43.647866, 'longitude': -79.3886415}</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>+1 416-901-4724</td>\n",
       "      <td>4202.070047</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>https://s3-media2.fl.yelpcdn.com/bphoto/KzTHwC...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>{'address1': '18 Duncan Street', 'address2': '...</td>\n",
       "      <td>Pai Northern Thai Kitchen</td>\n",
       "      <td>False</td>\n",
       "      <td>1.416901e+10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>$$</td>\n",
       "      <td>78</td>\n",
       "      <td>2144</td>\n",
       "      <td>['1', '13', '2017']</td>\n",
       "      <td>El28CtPV5fnMXyofHoGhoA</td>\n",
       "      <td>en</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Food and drink: One of my favourite places for...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://www.yelp.com/biz/pai-northern-thai-kit...</td>\n",
       "      <td>--BumyUHiO_7YsHurb9Hkw</td>\n",
       "      <td>Old Toronto, Toronto, Canada</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3482</td>\n",
       "      <td>0</td>\n",
       "      <td>1.484284e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>437873</td>\n",
       "      <td>437873</td>\n",
       "      <td>10150</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>college-falafel-toronto</td>\n",
       "      <td>xsl-d_opm3AU5H2Z-im33g</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[{'alias': 'mideastern', 'title': 'Middle East...</td>\n",
       "      <td>{'latitude': 43.65455, 'longitude': -79.42289}</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>+1 416-532-8698</td>\n",
       "      <td>4471.118815</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>https://s3-media1.fl.yelpcdn.com/bphoto/NjObd5...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>{'address1': '450 Ossington Ave', 'address2': ...</td>\n",
       "      <td>College Falafel</td>\n",
       "      <td>False</td>\n",
       "      <td>1.416533e+10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>$</td>\n",
       "      <td>78</td>\n",
       "      <td>86</td>\n",
       "      <td>['1', '23', '2017']</td>\n",
       "      <td>jq-96Xeg9ilhl_jxyN49PA</td>\n",
       "      <td>en</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Food: The falafel sandwich was great - fresh, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://www.yelp.com/biz/college-falafel-toron...</td>\n",
       "      <td>--BumyUHiO_7YsHurb9Hkw</td>\n",
       "      <td>Old Toronto, Toronto, Canada</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3861</td>\n",
       "      <td>0</td>\n",
       "      <td>1.485148e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>385263</td>\n",
       "      <td>385263</td>\n",
       "      <td>2748</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>bar-raval-toronto</td>\n",
       "      <td>41o1FUbCYKJv2djtnlkzlg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[{'alias': 'spanish', 'title': 'Spanish'}, {'a...</td>\n",
       "      <td>{'latitude': 43.6559438482769, 'longitude': -7...</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>+1 647-344-8001</td>\n",
       "      <td>3530.003000</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>https://s3-media2.fl.yelpcdn.com/bphoto/Bx6Tl9...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>{'address1': '505 College Street', 'address2':...</td>\n",
       "      <td>Bar Raval</td>\n",
       "      <td>False</td>\n",
       "      <td>1.647345e+10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>$$$</td>\n",
       "      <td>78</td>\n",
       "      <td>228</td>\n",
       "      <td>['1', '23', '2017']</td>\n",
       "      <td>htHfoKD30p9sgQoFwye9jQ</td>\n",
       "      <td>en</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Food and drink: Went for cocktails only; I had...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://www.yelp.com/biz/bar-raval-toronto?adj...</td>\n",
       "      <td>--BumyUHiO_7YsHurb9Hkw</td>\n",
       "      <td>Old Toronto, Toronto, Canada</td>\n",
       "      <td>0.0</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>1.485148e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>88984</td>\n",
       "      <td>88984</td>\n",
       "      <td>11232</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>bar-isabel-toronto</td>\n",
       "      <td>q5xrVJ4kivx_yEfJeOKNYQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[{'alias': 'spanish', 'title': 'Spanish'}, {'a...</td>\n",
       "      <td>{'latitude': 43.65463, 'longitude': -79.42075}</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>+1 416-532-2222</td>\n",
       "      <td>4230.791736</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>https://s3-media1.fl.yelpcdn.com/bphoto/Ne8Ykv...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>{'address1': '797 College Street', 'address2':...</td>\n",
       "      <td>Bar Isabel</td>\n",
       "      <td>False</td>\n",
       "      <td>1.416532e+10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>$$$</td>\n",
       "      <td>78</td>\n",
       "      <td>379</td>\n",
       "      <td>['1', '23', '2017']</td>\n",
       "      <td>jIgYSZAWS0TX4097BN1u8g</td>\n",
       "      <td>en</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Food and drink: As a vegetarian, the options a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://www.yelp.com/biz/bar-isabel-toronto?ad...</td>\n",
       "      <td>--BumyUHiO_7YsHurb9Hkw</td>\n",
       "      <td>Old Toronto, Toronto, Canada</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3408</td>\n",
       "      <td>0</td>\n",
       "      <td>1.485148e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day  Month  Unnamed: 0.1  Unnamed: 0_x  Unnamed: 0_y  Updated  Year  \\\n",
       "0   13      1        628892        628892          8039    False  2017   \n",
       "1   13      1        582871        582871         15015    False  2017   \n",
       "2   23      1        437873        437873         10150    False  2017   \n",
       "3   23      1        385263        385263          2748    False  2017   \n",
       "4   23      1         88984         88984         11232    False  2017   \n",
       "\n",
       "                                 alias             business_id  \\\n",
       "0           bobbette-and-belle-toronto  vcxvQyAggPqxcHwvJXvjGg   \n",
       "1  pai-northern-thai-kitchen-toronto-5  r_BrIgzYcwo1NAuG9dLbpg   \n",
       "2              college-falafel-toronto  xsl-d_opm3AU5H2Z-im33g   \n",
       "3                    bar-raval-toronto  41o1FUbCYKJv2djtnlkzlg   \n",
       "4                   bar-isabel-toronto  q5xrVJ4kivx_yEfJeOKNYQ   \n",
       "\n",
       "   business_stars                                         categories  \\\n",
       "0             4.0  [{'alias': 'catering', 'title': 'Caterers'}, {...   \n",
       "1             4.5               [{'alias': 'thai', 'title': 'Thai'}]   \n",
       "2             4.0  [{'alias': 'mideastern', 'title': 'Middle East...   \n",
       "3             4.0  [{'alias': 'spanish', 'title': 'Spanish'}, {'a...   \n",
       "4             4.0  [{'alias': 'spanish', 'title': 'Spanish'}, {'a...   \n",
       "\n",
       "                                         coordinates        date  \\\n",
       "0     {'latitude': 43.66201, 'longitude': -79.33476}  2017-01-13   \n",
       "1  {'latitude': 43.647866, 'longitude': -79.3886415}  2017-01-13   \n",
       "2     {'latitude': 43.65455, 'longitude': -79.42289}  2017-01-23   \n",
       "3  {'latitude': 43.6559438482769, 'longitude': -7...  2017-01-23   \n",
       "4     {'latitude': 43.65463, 'longitude': -79.42075}  2017-01-23   \n",
       "\n",
       "     display_phone     distance  friend_count  ghost  \\\n",
       "0  +1 416-466-8800  7046.832404             4  False   \n",
       "1  +1 416-901-4724  4202.070047             4  False   \n",
       "2  +1 416-532-8698  4471.118815             4  False   \n",
       "3  +1 647-344-8001  3530.003000             4  False   \n",
       "4  +1 416-532-2222  4230.791736             4  False   \n",
       "\n",
       "                                           image_url img_dsc img_url  \\\n",
       "0  https://s3-media3.fl.yelpcdn.com/bphoto/7DRlzP...      []      []   \n",
       "1  https://s3-media2.fl.yelpcdn.com/bphoto/KzTHwC...      []      []   \n",
       "2  https://s3-media1.fl.yelpcdn.com/bphoto/NjObd5...      []      []   \n",
       "3  https://s3-media2.fl.yelpcdn.com/bphoto/Bx6Tl9...      []      []   \n",
       "4  https://s3-media1.fl.yelpcdn.com/bphoto/Ne8Ykv...      []      []   \n",
       "\n",
       "   is_closed                                           location  \\\n",
       "0      False  {'address1': '1121 Queen Street E', 'address2'...   \n",
       "1      False  {'address1': '18 Duncan Street', 'address2': '...   \n",
       "2      False  {'address1': '450 Ossington Ave', 'address2': ...   \n",
       "3      False  {'address1': '505 College Street', 'address2':...   \n",
       "4      False  {'address1': '797 College Street', 'address2':...   \n",
       "\n",
       "                        name     nr         phone  photo_count price  \\\n",
       "0           Bobbette & Belle  False  1.416467e+10         13.0    $$   \n",
       "1  Pai Northern Thai Kitchen  False  1.416901e+10         13.0    $$   \n",
       "2            College Falafel  False  1.416533e+10         13.0     $   \n",
       "3                  Bar Raval  False  1.647345e+10         13.0   $$$   \n",
       "4                 Bar Isabel  False  1.416532e+10         13.0   $$$   \n",
       "\n",
       "   review_count_x  review_count_y          review_date  \\\n",
       "0              78             154  ['1', '13', '2017']   \n",
       "1              78            2144  ['1', '13', '2017']   \n",
       "2              78              86  ['1', '23', '2017']   \n",
       "3              78             228  ['1', '23', '2017']   \n",
       "4              78             379  ['1', '23', '2017']   \n",
       "\n",
       "                review_id review_language  review_stars  \\\n",
       "0  9e3cmOBflFYd98XZVN8YXQ              en           5.0   \n",
       "1  El28CtPV5fnMXyofHoGhoA              en           4.0   \n",
       "2  jq-96Xeg9ilhl_jxyN49PA              en           4.0   \n",
       "3  htHfoKD30p9sgQoFwye9jQ              en           3.0   \n",
       "4  jIgYSZAWS0TX4097BN1u8g              en           4.0   \n",
       "\n",
       "                                         review_text transactions        ufc  \\\n",
       "0  Food and drink: Loved the alfajores (mini lemo...           []  [0, 0, 0]   \n",
       "1  Food and drink: One of my favourite places for...           []  [0, 0, 0]   \n",
       "2  Food: The falafel sandwich was great - fresh, ...           []  [0, 0, 0]   \n",
       "3  Food and drink: Went for cocktails only; I had...           []  [0, 0, 0]   \n",
       "4  Food and drink: As a vegetarian, the options a...           []  [0, 0, 0]   \n",
       "\n",
       "                                                 url                 user_id  \\\n",
       "0  https://www.yelp.com/biz/bobbette-and-belle-to...  --BumyUHiO_7YsHurb9Hkw   \n",
       "1  https://www.yelp.com/biz/pai-northern-thai-kit...  --BumyUHiO_7YsHurb9Hkw   \n",
       "2  https://www.yelp.com/biz/college-falafel-toron...  --BumyUHiO_7YsHurb9Hkw   \n",
       "3  https://www.yelp.com/biz/bar-raval-toronto?adj...  --BumyUHiO_7YsHurb9Hkw   \n",
       "4  https://www.yelp.com/biz/bar-isabel-toronto?ad...  --BumyUHiO_7YsHurb9Hkw   \n",
       "\n",
       "                       user_loc  vote_count  business_num_id  user_num_id  \\\n",
       "0  Old Toronto, Toronto, Canada         0.0             3725            0   \n",
       "1  Old Toronto, Toronto, Canada         0.0             3482            0   \n",
       "2  Old Toronto, Toronto, Canada         0.0             3861            0   \n",
       "3  Old Toronto, Toronto, Canada         0.0              309            0   \n",
       "4  Old Toronto, Toronto, Canada         0.0             3408            0   \n",
       "\n",
       "      timestamp  \n",
       "0  1.484284e+09  \n",
       "1  1.484284e+09  \n",
       "2  1.485148e+09  \n",
       "3  1.485148e+09  \n",
       "4  1.485148e+09  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If using term frequency only to compute corpus and X(review vs. terms) CounterVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire corpus\n",
    "#corpus = preprocess(df_train)\n",
    "# X row: df_train row, column: key words frequency \n",
    "# When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n",
    "#cv=CountVectorizer(max_df=0.9,stop_words=stop_words, max_features=5000, ngram_range=(1,1))\n",
    "#X=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If using TD-IDF to compute corpus and X (business vs. terms) TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 84805/84805 [00:38<00:00, 2229.92it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = preprocess(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary to store business: review text\n",
    "dict_text = {}\n",
    "for i in range(len(corpus)):\n",
    "    if df_train['business_num_id'][i] not in dict_text:\n",
    "        dict_text[df_train['business_num_id'][i]] = corpus[i]\n",
    "    else:\n",
    "        temp = dict_text[df_train['business_num_id'][i]]\n",
    "        temp = temp + corpus[i]\n",
    "        dict_text[df_train['business_num_id'][i]] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list for the review text, where the row dimension = total business ids\n",
    "list_text = []\n",
    "for key in range(0,max(list(dict_text.keys()))+1) :\n",
    "    if key not in dict_text.keys():\n",
    "        list_text.extend([\"\"])\n",
    "    else:\n",
    "        list_text.extend([dict_text[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the X vector, where dimension is #business vs #terms\n",
    "vectorizer = TfidfVectorizer(max_df=0.9,stop_words=stop_words, max_features=5000, ngram_range=(1,1))\n",
    "X_cleaned = vectorizer.fit_transform(list_text).toarray()\n",
    "X_cleaned_sparse = csr_matrix(X_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-rating KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. With ratings that subtracts user average rating, cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6100/6100 [00:10<00:00, 602.84it/s]\n",
      "100%|| 6100/6100 [00:01<00:00, 3947.06it/s]\n"
     ]
    }
   ],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "similarity_1 = train(rtrain)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "UI_predictionScore_Explicit = predict(rtrain_userAvg, 100, similarity_1, item_similarity_en= False)\n",
    "UI_predict_Explicit = prediction(UI_predictionScore_Explicit, 50, rtrain_userAvg)\n",
    "#user_item_res1 = evaluate(user_item_predict1, rvalid_userAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2378, 2584,  103, 3482, 1893, 2563,  353, 1802, 3534,  835, 1367,\n",
       "       1996, 2872,  423, 1051, 3982,  373,  746,  906, 2725,  590, 2827,\n",
       "       3601,  325, 2258, 3623, 3410,  625, 2521, 2509, 2800, 1917, 3649,\n",
       "        591, 3110, 1909, 3949, 2947, 2702, 2650, 3625,  819, 2729, 3776,\n",
       "       2276, 1294, 2895, 1077, 2487, 1385], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UI_predict_Explicit[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implicit User-rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6100/6100 [00:09<00:00, 615.49it/s]\n",
      "100%|| 6100/6100 [00:01<00:00, 4092.59it/s]\n"
     ]
    }
   ],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "similarity_2 = train(rtrain_implicit)\n",
    "UI_predictionScore_Implicit = predict(rtrain_implicit, 100, similarity_2, item_similarity_en= False)\n",
    "UI_predict_Implicit = prediction(UI_predictionScore_Implicit, 50, rtrain_implicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2378, 3035, 1441, 3626,  746, 3482,  103, 2729, 2584,  591, 2563,\n",
       "       2841, 3649, 1051, 3384,  157, 1533, 1978, 1802, 1957,  290,  421,\n",
       "        373, 3513,  768, 3351, 1996, 3557, 1367, 2615, 3407,  819, 1086,\n",
       "       2365, 1277,  291, 2800, 2510,  665, 3233, 2500,  973,  562, 2773,\n",
       "       2505, 3415,  948, 1207, 2648, 2474], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UI_predict_Implicit[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implicit similarity, Explicit user-rating combination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6100/6100 [00:10<00:00, 601.34it/s]\n",
      "100%|| 6100/6100 [00:01<00:00, 3923.96it/s]\n"
     ]
    }
   ],
   "source": [
    "similarity_3 = train(rtrain_implicit)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "UI_predictionScore_IECombined = predict(rtrain, 100, similarity_3, item_similarity_en= False)\n",
    "UI_predict_IECombined = prediction(UI_predictionScore_IECombined, 50, rtrain)\n",
    "# user_item_res1 = evaluate(UI_predict_IECombined, rvalid)\n",
    "# user_item_res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. NO NEED Something random, to be filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6100/6100 [00:10<00:00, 566.64it/s]\n",
      "100%|| 6100/6100 [00:01<00:00, 4124.39it/s]\n"
     ]
    }
   ],
   "source": [
    "similarity_4 = train(rtrain)\n",
    "UI_predictionScore_random1 = predict(rtrain_implicit, 100, similarity_4, item_similarity_en= False)\n",
    "UI_predict_random1 = prediction(UI_predictionScore_random1, 50, rtrain_implicit)\n",
    "# user_item_res1 = evaluate(UI_predict_IECombined, rvalid)\n",
    "# user_item_res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. something random, to be filled 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "userVisitMatrix = getImplicitMatrix(rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity1 = train(rtrain)\n",
    "similarity2 = train(rtrain_implicit)\n",
    "similarity3 =train(userVisitMatrix)\n",
    "similarity4 = train(rtrain_userAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6100/6100 [00:22<00:00, 271.32it/s]\n",
      "100%|| 6100/6100 [00:01<00:00, 3731.56it/s]\n"
     ]
    }
   ],
   "source": [
    "UI_predictionScore_max = predictUU(rtrain, 100, 'max', similarity1, similarity2, similarity3, similarity4)\n",
    "UI_predict_max = prediction(UI_predictionScore_max, 50, rtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find restaurant recommendation within set location radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setting, don't need to modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df.copy()\n",
    "dff_popular = df.copy()\n",
    "dff_popular = dff_popular.sort_values(by=[\"review_count_y\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "popular_list = dff_popular[\"business_num_id\"].tolist()\n",
    "dundas_and_yonge = Point(\"43.6561,-79.3802\")\n",
    "bay_and_queens = Point(\"43.6518,-79.3802\")\n",
    "king_and_jarvis = Point(\"43.650577,-79.371887\")\n",
    "bloor_and_yonge = Point(\"43.670409,-79.386814\")\n",
    "yonge_and_eglinton = Point(\"43.7064,-79.3986\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = dundas_and_yonge\n",
    "userIndex = 0\n",
    "businessIndexRange = 5\n",
    "prediction_matrix = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geographical_dist(prediction_matrix,intersection,user_id, bus_indexRange):\n",
    "    list = []\n",
    "    length = 0\n",
    "    if isinstance(prediction_matrix, type(list)):\n",
    "        length = len(popular_list)\n",
    "    else:\n",
    "        length = prediction_matrix[user_id].shape[0]\n",
    "    for j in range(length):\n",
    "        if isinstance(prediction_matrix, type(list)):\n",
    "            coordinateDict = yaml.safe_load(dff[dff[\"business_num_id\"] == popular_list[j]].iloc[0].coordinates)\n",
    "        else:    \n",
    "            coordinateDict = yaml.safe_load(dff[dff[\"business_num_id\"] == prediction_matrix[user_id][j]].iloc[0].coordinates)\n",
    "        test_point = Point(coordinateDict['latitude'],coordinateDict['longitude'])\n",
    "\n",
    "        result = distance.distance(intersection,test_point).kilometers\n",
    "        if result<=5:\n",
    "            if isinstance(prediction_matrix, type(list)):\n",
    "                list.append(popular_list[j])\n",
    "            else:\n",
    "                list.append(prediction_matrix[user_id][j])\n",
    "        list = list[0:bus_indexRange]\n",
    "    return list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce the list of restaurants close to the set location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First list\n",
    "UI_predict_Explicit = geographical_dist(UI_predict_Explicit,intersection,userIndex,businessIndexRange)\n",
    "\n",
    "#Second list\n",
    "UI_predict_Implicit = geographical_dist(UI_predict_Implicit,intersection,userIndex,businessIndexRange)\n",
    "\n",
    "#Third list\n",
    "UI_predict_IECombined = geographical_dist(UI_predict_IECombined,intersection,userIndex,businessIndexRange)\n",
    "\n",
    "#Fourth lsit\n",
    "UI_predict_popular = geographical_dist(popular_list,intersection,userIndex,businessIndexRange)\n",
    "\n",
    "#Five list\n",
    "UI_predict_max = geographical_dist(UI_predict_max,intersection,userIndex,businessIndexRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2378, 2584, 103, 3482, 1893]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UI_predict_Explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2378, 3035, 1441, 3626, 746]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UI_predict_Implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3482, 2378, 1780, 2921, 1480]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UI_predict_popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2378, 3482, 3649, 3725, 1441]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UI_predict_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df[\"business_num_id\"] == UI_prediction[userIndex][busIndex]].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match restaurant information according to business_num_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the user number that we are trying to recommend for\n",
    "#Enter the # of businesses that we want to recommend for them \n",
    "#Pass in the UI_Prediction matrix for users \n",
    "#userIndex parameter not used \n",
    "def constructResDictionary(userIndex, busIndexRange, UI_prediction):\n",
    "    #Construct the dictionary for the recommended restaurants to display \n",
    "    dictionaryToConstruct = {}\n",
    "    \n",
    "    #Loop through the number of businesses \n",
    "    for busIndex in range(busIndexRange):\n",
    "        #Get the business information for the recommended business \n",
    "        businessSeries = df[df[\"business_num_id\"] == UI_prediction[busIndex]].iloc[0]\n",
    "        #Get the business name \n",
    "        busName = businessSeries['name']\n",
    "        \n",
    "        #get the list of strings to generate the address information \n",
    "        address_generator = (str(w) for w in yaml.safe_load(businessSeries.location)['display_address'])\n",
    "        busLocation = ', '.join(address_generator)\n",
    "        bus_Price = businessSeries.price\n",
    "        busStars = businessSeries.business_stars\n",
    "        busReviewCount = businessSeries.review_count_y \n",
    "        category_generator = (str(s) for s in [cateDict['title'] for cateDict in yaml.safe_load(businessSeries.categories)])\n",
    "        #busCategories = [cateDict['title'] for cateDict in yaml.safe_load(businessSeries.categories)]\n",
    "        busCategories = ', '.join(category_generator)\n",
    "        #Now add the restaurant to the dictionary\n",
    "        dictionaryToConstruct[busName] = {'Address': busLocation,\\\n",
    "                                'Price': bus_Price,\\\n",
    "                                 'Star': busStars, \\\n",
    "                                 'Review Count': busReviewCount, \\\n",
    "                                 'Category': busCategories}\n",
    "    return dictionaryToConstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to recommend for user 0 for now \n",
    "\n",
    "\n",
    "dict1_ExplicitRecommend = {}\n",
    "dict2_ImplicitRecommend = {}\n",
    "dict3_EICombineRecommend = {}\n",
    "dict4_PopularityRecommend = {}\n",
    "dict5_maxRecommend = {}\n",
    "#User predict [user index] [item index]\n",
    "#Add a for loop for the top recommended items\n",
    "\n",
    "dict1_ExplicitRecommend = constructResDictionary(userIndex, businessIndexRange, UI_predict_Explicit)\n",
    "dict2_ImplicitRecommend = constructResDictionary(userIndex, businessIndexRange, UI_predict_Implicit)\n",
    "dict3_EICombineRecommend = constructResDictionary(userIndex, businessIndexRange, UI_predict_IECombined)\n",
    "dict4_PopularityRecommend = constructResDictionary(userIndex, businessIndexRange, UI_predict_popular)\n",
    "dict5_maxRecommend = constructResDictionary(userIndex, businessIndexRange, UI_predict_max)\n",
    "\n",
    "#Say we are recommending 5 restaurants for now\n",
    "# for busIndex in range(5):\n",
    "#     businessSeries = df[df[\"business_num_id\"] == UI_predict_Explicit[0][busIndex]].iloc[0]\n",
    "#     busName = businessSeries['name']\n",
    "#     address_generator = (str(w) for w in yaml.safe_load(businessSeries.location)['display_address'])\n",
    "#     busLocation = ', '.join(address_generator)\n",
    "#     bus_Price = businessSeries.price\n",
    "#     busStars = businessSeries.business_stars\n",
    "#     busReviewCount = businessSeries.review_count_y \n",
    "#     category_generator = (str(s) for s in [cateDict['title'] for cateDict in yaml.safe_load(businessSeries.categories)])\n",
    "#     #busCategories = [cateDict['title'] for cateDict in yaml.safe_load(businessSeries.categories)]\n",
    "#     busCategories = ', '.join(category_generator)\n",
    "#     #Now add the restaurant to the dictionary\n",
    "#     dict1stAlgo[busName] = {'Address': busLocation,\\\n",
    "#                             'Price': bus_Price,\\\n",
    "#                              'Star': busStars, \\\n",
    "#                              'Review Count': busReviewCount, \\\n",
    "#                              'Category': busCategories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiListbox(Frame):\n",
    "    \n",
    "    def __init__(self, master, lists):\n",
    "        Frame.__init__(self, master)\n",
    "        self.lists = []\n",
    "        \n",
    "        #Loop through the lists, l is the list label and widthW is the width \n",
    "        for l,widthW in lists:\n",
    "            \n",
    "            frame = Frame(self); frame.pack(side=LEFT, expand=YES, fill=BOTH)\n",
    "            \n",
    "            Label(frame, text=l, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "            \n",
    "            lb = Listbox(frame, width=widthW, borderwidth=0, selectborderwidth=0,\n",
    "                 relief=FLAT, exportselection=FALSE)\n",
    "            lb.pack(expand=YES, fill=BOTH)\n",
    "            \n",
    "            self.lists.append(lb)\n",
    "            \n",
    "            lb.bind('<B1-Motion>', lambda e, s=self: s._select(e.y))\n",
    "            lb.bind('<Button-1>', lambda e, s=self: s._select(e.y))\n",
    "            \n",
    "            lb.bind('<Leave>', lambda e: 'break')\n",
    "            \n",
    "            lb.bind('<B2-Motion>', lambda e, s=self: s._b2motion(e.x, e.y))\n",
    "            lb.bind('<Button-2>', lambda e, s=self: s._button2(e.x, e.y))\n",
    "            \n",
    "        #pakcing the frame\n",
    "        frame = Frame(self); frame.pack(side=LEFT, fill=Y)\n",
    "        \n",
    "        #packing the label\n",
    "        Label(frame, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "        \n",
    "        #Setting a scrollbar \n",
    "#         sb = Scrollbar(frame, orient=VERTICAL, command=self._scroll)\n",
    "        \n",
    "#         sb.pack(expand=YES, fill=Y)\n",
    "        \n",
    "#         self.lists[0]['yscrollcommand']=sb.set\n",
    "\n",
    "    def _select(self, y):\n",
    "        row = self.lists[0].nearest(y)\n",
    "        self.selection_clear(0, END)\n",
    "        self.selection_set(row)\n",
    "        return 'break'\n",
    "\n",
    "    def _button2(self, x, y):\n",
    "        for l in self.lists: l.scan_mark(x, y)\n",
    "        return 'break'\n",
    "\n",
    "    def _b2motion(self, x, y):\n",
    "        for l in self.lists: l.scan_dragto(x, y)\n",
    "        return 'break'\n",
    "\n",
    "    def _scroll(self, *args):\n",
    "        for l in self.lists:\n",
    "            apply(l.yview, args)\n",
    "\n",
    "    def curselection(self):\n",
    "        return self.lists[0].curselection()\n",
    "\n",
    "    def delete(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.delete(first, last)\n",
    "\n",
    "#     def get(self, first, last=None):\n",
    "#         result = []\n",
    "#         for l in self.lists:\n",
    "#             result.append(l.get(first,last))\n",
    "#         if last: return apply(map, [None] + result)\n",
    "#         return result\n",
    "\n",
    "    def index(self, index):\n",
    "        self.lists[0].index(index)\n",
    "\n",
    "    def insert(self, index, *elements):\n",
    "        #Loop through the elements \n",
    "        for element in elements:\n",
    "            i = 0\n",
    "            for l in self.lists:\n",
    "                l.insert(index, element[i])\n",
    "                i = i + 1\n",
    "\n",
    "    def size(self):\n",
    "        return self.lists[0].size()\n",
    "\n",
    "    def see(self, index):\n",
    "        for l in self.lists:\n",
    "            l.see(index)\n",
    "\n",
    "    def selection_anchor(self, index):\n",
    "        for l in self.lists:\n",
    "            l.selection_anchor(index)\n",
    "\n",
    "    def selection_clear(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.selection_clear(first, last)\n",
    "\n",
    "    def selection_includes(self, index):\n",
    "        return self.lists[0].selection_includes(index)\n",
    "\n",
    "    def selection_set(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.selection_set(first, last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #Initiate a frame or master? \n",
    "    tk = Tk()\n",
    "    Label(tk, text='List of recommended restaurants').pack()\n",
    "    \n",
    "    #Creating the multi-listbox object, pass in a tuple of tuple (lists), this will be the list objects\n",
    "    mlb = MultiListbox_entries(tk, (('Explicit User-Ratings', 20), ('Implicit User-Ratings', 20), \\\n",
    "                            ('Explicit Implicit Combined', 20), ('Popularity List', 20), ('Taking Max', 20)))\n",
    "    \n",
    "    #loop through the length of recommend list, # = 5 \n",
    "    for index in range(len(dict1_ExplicitRecommend)):\n",
    "        #First restaurant information to display \n",
    "        #Following the restaurants' name \n",
    "        restList1 = list(dict1_ExplicitRecommend.keys())[index]\n",
    "        restList2 = list(dict2_ImplicitRecommend.keys())[index]\n",
    "        restList3 = list(dict3_EICombineRecommend.keys())[index]\n",
    "        restList4 = list(dict4_PopularityRecommend.keys())[index]\n",
    "        restList5 = list(dict5_maxRecommend.keys())[index]\n",
    "        \n",
    "        mlb.insert(END, (' ', ' ', ' ', ' ', ' '))\n",
    "        \n",
    "        #Inserting the restaurant names\n",
    "        mlb.insert(END, ('%d: %s' % (index + 1, restList1),'%d: %s' % (index + 1, restList2),\n",
    "                         '%d: %s' % (index + 1, restList3),'%d: %s' % (index + 1, restList4),\n",
    "                         '%d: %s' % (index + 1, restList5)))\n",
    "        \n",
    "        #Looping through each attribute keys - resinfo\n",
    "        for resinfo in dict1_ExplicitRecommend.get(restList1).keys():\n",
    "            restList1Info = resinfo + ':' + str(dict1_ExplicitRecommend.get(restList1).get(resinfo,''))\n",
    "            restList2Info = resinfo + ':' + str(dict2_ImplicitRecommend.get(restList2).get(resinfo,''))\n",
    "            restList3Info = resinfo + ':' + str(dict3_EICombineRecommend.get(restList3).get(resinfo,''))\n",
    "            restList4Info = resinfo + ':' + str(dict4_PopularityRecommend.get(restList4).get(resinfo,''))\n",
    "            restList5Info = resinfo + ':' + str(dict5_maxRecommend.get(restList5).get(resinfo,''))\n",
    "            \n",
    "            mlb.insert(END, (restList1Info, restList2Info, restList3Info, restList4Info, restList5Info))\n",
    "        \n",
    "        mlb.insert(END, ('----------------', '----------------', '----------------', '----------------', '----------------'))\n",
    "           \n",
    "    mlb.pack(expand=YES,fill=BOTH)\n",
    "    tk.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['Like', 'Dislike']\n",
    "\n",
    "def fetch(entries):\n",
    "    for entry in entries:\n",
    "        #index 0 is the field \n",
    "        #index 1 is the entry data \n",
    "        field = entry[0]\n",
    "        text  = entry[1].get()\n",
    "        print('%s: \"%s\"' % (field, text)) \n",
    "\n",
    "def makeform(root, fields):\n",
    "    entries = []\n",
    "    #For each field, like \n",
    "    for field in fields:\n",
    "        row = Frame(root)\n",
    "        lab = Label(row, width=15, text=field, anchor='w')\n",
    "        ent = Entry(row)\n",
    "        row.pack(side=TOP, fill=X, padx=5, pady=5)\n",
    "        lab.pack(side=LEFT)\n",
    "        ent.pack(side=RIGHT, expand=YES, fill=X)\n",
    "        entries.append((field, ent))\n",
    "    return entries\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #Initialize the root \n",
    "    root = Tk()\n",
    "    \n",
    "    #Initialize the entries\n",
    "    ents = makeform(root, fields)\n",
    "    \n",
    "    #the buttons \n",
    "    root.bind('<Button-1>', (lambda event, e=ents: fetch(e)))   \n",
    "    b1 = Button(root, text='Show', command=(lambda e=ents: fetch(e)))\n",
    "    b1.pack(side=LEFT, padx=5, pady=5)\n",
    "    b2 = Button(root, text='Quit', command=root.quit)\n",
    "    b2.pack(side=LEFT, padx=5, pady=5)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilist box with entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiListbox_entries(Frame):\n",
    "    \n",
    "    def __init__(self, master, lists):\n",
    "        Frame.__init__(self, master)\n",
    "        self.lists = []\n",
    "        self.entries = []\n",
    "        self.fields = 'Like', 'Dislike'\n",
    "        #Loop through the lists, l is the list label and widthW is the width \n",
    "        for l,widthW in lists:\n",
    "            \n",
    "            frame = Frame(self); frame.pack(side=LEFT, expand=YES, fill=BOTH)\n",
    "            \n",
    "            Label(frame, text=l, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "            \n",
    "            lb = Listbox(frame, width=widthW, borderwidth=0, selectborderwidth=0,\n",
    "                 relief=FLAT, exportselection=FALSE)\n",
    "            lb.pack(expand=YES, fill=BOTH)\n",
    "            \n",
    "            self.lists.append(lb)\n",
    "            \n",
    "            lb.bind('<B1-Motion>', lambda e, s=self: s._select(e.y))\n",
    "            lb.bind('<Button-1>', lambda e, s=self: s._select(e.y))\n",
    "            \n",
    "            lb.bind('<Leave>', lambda e: 'break')\n",
    "            \n",
    "            lb.bind('<B2-Motion>', lambda e, s=self: s._b2motion(e.x, e.y))\n",
    "            lb.bind('<Button-2>', lambda e, s=self: s._button2(e.x, e.y))\n",
    "             \n",
    "             #loop through the fields\n",
    "            for field in fields:\n",
    "                #row = Frame(self)\n",
    "                lab = Label(frame, width=15, text=field, anchor='w')\n",
    "                ent = Entry(frame)\n",
    "                #row.pack(side=BOTTOM, fill=Y, padx=5, pady=5) #=BOTTOM\n",
    "                lab.pack()\n",
    "                #ent.pack(side=RIGHT, expand=YES, fill=Y)\n",
    "                ent.pack(side=TOP, fill=X)\n",
    "                self.entries.append((field, ent))\n",
    "\n",
    "        #Set the buttons \n",
    "        #master.bind('<Button-1>', (lambda event, e=self.entries: fetch(e)))   \n",
    "        b1 = Button(master, text='Submit', command=(lambda e=self.entries: fetch(e)))\n",
    "        b1.pack(side=BOTTOM, padx=5, pady=5)\n",
    "        #b2 = Button(root, text='Quit', command=root.quit)\n",
    "        #b2.pack(side=LEFT, padx=5, pady=5)\n",
    "            \n",
    "        #pakcing the frame\n",
    "        frame = Frame(self); frame.pack(side=LEFT, fill=Y)\n",
    "        \n",
    "        #packing the label\n",
    "        Label(frame, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "        #Setting a scrollbar \n",
    "#         sb = Scrollbar(frame, orient=VERTICAL, command=self._scroll)\n",
    "#         sb.pack(expand=YES, fill=Y)\n",
    "#         self.lists[0]['yscrollcommand']=sb.set\n",
    "\n",
    "    def _select(self, y):\n",
    "        row = self.lists[0].nearest(y)\n",
    "        self.selection_clear(0, END)\n",
    "        self.selection_set(row)\n",
    "        return 'break'\n",
    "\n",
    "    def _button2(self, x, y):\n",
    "        for l in self.lists: l.scan_mark(x, y)\n",
    "        return 'break'\n",
    "\n",
    "    def _b2motion(self, x, y):\n",
    "        for l in self.lists: l.scan_dragto(x, y)\n",
    "        return 'break'\n",
    "\n",
    "    def _scroll(self, *args):\n",
    "        for l in self.lists:\n",
    "            apply(l.yview, args)\n",
    "\n",
    "    def curselection(self):\n",
    "        return self.lists[0].curselection()\n",
    "\n",
    "    def delete(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.delete(first, last)\n",
    "\n",
    "#     def get(self, first, last=None):\n",
    "#         result = []\n",
    "#         for l in self.lists:\n",
    "#             result.append(l.get(first,last))\n",
    "#         if last: return apply(map, [None] + result)\n",
    "#         return result\n",
    "\n",
    "    def index(self, index):\n",
    "        self.lists[0].index(index)\n",
    "\n",
    "    def insert(self, index, *elements):\n",
    "        #Loop through the elements \n",
    "        for element in elements:\n",
    "            i = 0\n",
    "            for l in self.lists:\n",
    "                l.insert(index, element[i])\n",
    "                i = i + 1\n",
    "\n",
    "    def size(self):\n",
    "        return self.lists[0].size()\n",
    "\n",
    "    def see(self, index):\n",
    "        for l in self.lists:\n",
    "            l.see(index)\n",
    "\n",
    "    def selection_anchor(self, index):\n",
    "        for l in self.lists:\n",
    "            l.selection_anchor(index)\n",
    "\n",
    "    def selection_clear(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.selection_clear(first, last)\n",
    "\n",
    "    def selection_includes(self, index):\n",
    "        return self.lists[0].selection_includes(index)\n",
    "\n",
    "    def selection_set(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.selection_set(first, last)\n",
    "            \n",
    "    def fetch(entries):\n",
    "        for entry in entries:\n",
    "            #index 0 is the field \n",
    "            #index 1 is the entry data \n",
    "            field = entry[0]\n",
    "            text  = entry[1].get()\n",
    "            print('%s: \"%s\"' % (field, text)) \n",
    "    \n",
    "    #Make the entry form \n",
    "    def makeform(root, fields):\n",
    "        #For each field, like \n",
    "        for field in fields:\n",
    "            row = Frame(master)\n",
    "            lab = Label(row, width=15, text=field, anchor='w')\n",
    "            ent = Entry(row)\n",
    "            row.pack(side=TOP, fill=X, padx=5, pady=5)\n",
    "            lab.pack(side=LEFT)\n",
    "            ent.pack(side=RIGHT, expand=YES, fill=X)\n",
    "            self.entries.append((field, ent))\n",
    "            \n",
    "        self.entries =  entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "similarity1 = train(rtrain_implicit)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "user_item_prediction_score1 = predict(rtrain_implicit, 90, similarity1, item_similarity_en= False)\n",
    "user_item_predict1 = prediction(user_item_prediction_score1, 50, rtrain_implicit)\n",
    "user_item_res1 = evaluate(user_item_predict1, rvalid_implicit)\n",
    "user_item_res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rvalid_implicit.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine implicit and explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "similarity1 = train(rtrain_implicit)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "user_item_prediction_score1 = predict(rtrain, 90, similarity1, item_similarity_en= False)\n",
    "user_item_predict1 = prediction(user_item_prediction_score1, 50, rtrain)\n",
    "user_item_res1 = evaluate(user_item_predict1, rvalid)\n",
    "user_item_res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. With ratings that subtracts user average rating, pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #UU similarity, using cosine similarity\n",
    "# similarity2 = train(rtrain_userAvg)\n",
    "# #get a user-item matrix  UI prediction\n",
    "# #Predict using UI matrix with ratings in it \n",
    "# user_item_prediction_score2 = predict(rtrain_userAvg, 100, similarity2, item_similarity_en= False)\n",
    "# user_item_predict2 = prediction(user_item_prediction_score2, 50, rtrain_userAvg)\n",
    "# user_item_res2 = evaluate(user_item_predict2, rvalid_userAvg)\n",
    "# user_item_res2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. With raw ratings, cosinesimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UU similarity\n",
    "similarity3 = train(rtrain)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "user_item_prediction_score3 = predict(rtrain, 90, similarity3, item_similarity_en= False)\n",
    "user_item_predict3 = prediction(user_item_prediction_score3, 50, rtrain)\n",
    "#Check user item prediction score\n",
    "user_item_res3 = evaluate(user_item_predict3, rvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Base KNN using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IK_MATRIX = X_cleaned_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_I_similarity = train(IK_MATRIX)\n",
    "item_based_prediction_score4 = predict(rtrain, 10, I_I_similarity, item_similarity_en= True)\n",
    "#for each restuarant top50 users \n",
    "item_based_predict4 = prediction(item_based_prediction_score4, 50, rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_based_res_TFIDF = evaluate(item_based_predict4, rvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item_based_res_TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item based KNN with IC Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_C_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_I_C = train(I_C_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_item_prediction_score_I_C = predict(e\n",
    "                                         , 90, similarity_I_C, item_similarity_en= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each restuarant top50 users \n",
    "item_based_predict_I_C = prediction(user_item_prediction_score_I_C, 50, rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_based_res_I_C = evaluate(item_based_predict_I_C, rvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item_based_res_I_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User based KNN with explicit UC Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_C_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain_implicit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_C_matrix = rtrain_implicit*I_C_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_UC = train(U_C_matrix)\n",
    "user_item_prediction_score_UC = predict(rtrain, 90, similarity_UC, item_similarity_en= False)\n",
    "user_item_predict_UC = prediction(user_item_prediction_score_UC, 90,rtrain)\n",
    "user_item_res_UC = evaluate(user_item_predict_UC, rvalid)\n",
    "user_item_res_UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_item_res_UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User based KNN with implicit UC Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_C_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImplicitMatrix(sparseMatrix, threashold=0):\n",
    "    temp_matrix = sparse.csr_matrix(sparseMatrix.shape)\n",
    "    temp_matrix[(sparseMatrix > threashold).nonzero()] = 1\n",
    "    return temp_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_C_matrix_implicit = getImplicitMatrix(U_C_matrix,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_C_matrix_implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_UC_implicit = train(U_C_matrix_implicit)\n",
    "user_item_prediction_score_UC_implicit = predict(rtrain, 90, similarity_UC_implicit, item_similarity_en= False)\n",
    "user_item_predict_UC_implicit = prediction(user_item_prediction_score_UC_implicit, 90,rtrain)\n",
    "user_item_res_UC_implicit = evaluate(user_item_predict_UC_implicit, rvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res_UC_implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try combining different similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a UI matrix if it's not item_similarity based or else IU\n",
    "def predictUU(matrix_train, k, similarity1, similarity2=None, similarity3=None, similarity4=None, weight1=None, weight2=None, weight3=None, weight4=None, chooseWeigthMethod = None, item_similarity_en = False):\n",
    "    prediction_scores = []\n",
    "    \n",
    "    #inverse to IU matrix\n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    #for each user or item, depends UI or IU \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "    #for user_index in tqdm(range(1)):\n",
    "        \n",
    "        numberSimilarMatrix = 0\n",
    "        # Get user u's prediction scores for all items \n",
    "        #Get prediction/similarity score for each user 1*num or user or num of items\n",
    "        vector_u1 = similarity1[user_index]\n",
    "        numberSimilarMatrix += 1\n",
    "        \n",
    "        if similarity2 is not None:\n",
    "            vector_u2 = similarity2[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u2 = [0]*len(vector_u1)\n",
    "            \n",
    "        if similarity3 is not None:\n",
    "            vector_u3 = similarity3[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u3 = [0]*len(vector_u1)\n",
    "            \n",
    "        if similarity4 is not None:\n",
    "            vector_u4 = similarity4[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u4 = [0]*len(vector_u1)\n",
    "        \n",
    "        vector_u = vector_u1.copy()\n",
    "            \n",
    "        #If we are choosing the max, min, or avg or similarity scores\n",
    "        if chooseWeigthMethod is not None:\n",
    "            #loop through the user index \n",
    "            for item_index in tqdm(range(matrix_train.shape[0])):\n",
    "\n",
    "                if chooseWeigthMethod == 'max':\n",
    "                    vector_u[item_index] = max(vector_u1[item_index], vector_u2[item_index], vector_u3[item_index],vector_u4[item_index])\n",
    "                elif chooseWeigthMethod == 'min':\n",
    "                    vector_u[item_index] = min(vector_u1[item_index], vector_u2[item_index], vector_u3[item_index],vector_u4[item_index])\n",
    "                elif chooseWeigthMethod == 'average':\n",
    "                    vector_u[item_index] = (vector_u1[item_index]+vector_u2[item_index]+vector_u3[item_index]+vector_u4[item_index])/(numberSimilarMatrix)\n",
    "        \n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        \n",
    "        # Get neighbors similarity weights and ratings\n",
    "        #similar_users_weights = similarity1[user_index][similar_users]\n",
    "        similar_users_weights = vector_u[similar_users]\n",
    "        \n",
    "        #similar_users_weights_sum = np.sum(similar_users_weights)\n",
    "        #print(similar_users_weights.shape)\n",
    "        #shape: num of res * k\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "              \n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "        #print(prediction_scores_u)\n",
    "        \n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "        \n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    return res\n",
    "    #return vector_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "similarity1 = train(rtrain)\n",
    "#similarity2 = train(rtrain_userAvg)\n",
    "similarity2 = None\n",
    "#similarity3 =train(userVisitMatrix)\n",
    "similarity3 = None\n",
    "similarity4 = train(rtrain_implicit)\n",
    "#similarity4 = None\n",
    "weight1 = None\n",
    "weight2=None\n",
    "weight3=None\n",
    "#vectorU = predictUU(rtrain_userAvg, 90, similarity1, similarity2, similarity3, weight1, weight2, weight3, item_similarity_en= False)\n",
    "user_item_prediction_score1 = predictUU(rtrain, 90, similarity1, similarity2, similarity3, similarity4, weight1, weight2, weight3, chooseWeigthMethod = 'max', item_similarity_en= False)\n",
    "user_item_predict1 = prediction(user_item_prediction_score1, 50, rtrain)\n",
    "user_item_res1 = evaluate(user_item_predict1, rvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1   #1,4, max rtrain,rvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1   #1,2,3,4 max rtrain,rvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1  #1,2,3,4, average  rtrain, rvalid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1  #1,4 average rtrain_implicit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1  #1,4 average, rtrain_userAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1  #1,4 average rtrain, rvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_item_res1    #1,4 average with userVisitMatrix, rvalid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1 #average 1,4 similarity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1 #average 1,2,4 similarity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_item_res1    #average 1-2 similarity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1   #average 3 1-3similarity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1  #average 4, 1-4similarity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1   #min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1 #max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1  #rtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1 #rtrain_userAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_prediction_score1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[3764   36  896 5156 5737  618 2345   11 1706 1554  726  627 5821 3348\n",
    "  243 1571 1247 5209  203 4856 3754  544 5417 3849 2411 1763  274 5211\n",
    "   91 2558 4208 4063 4712 3520  457 2900  387 2455 1178 3024 1342 6014\n",
    " 2823 5848 4691 5099 5109 5545 3987 6069 5794 5572   77 3180 3266 5039\n",
    " 5904 2325 3628  493 5735   27 4797 4813 3128 5337 4845 3907 3544 2128\n",
    "  320  134 1985 3470 5944 3188 2438 4850 2630 3912 1245 1674 5986 2360\n",
    " 5094 3670 5902 1491 3997 3856]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[3026 4208 5069 5507 1571 3529  544 3090 2538 4797 3527 1558 6079 5736\n",
    "   13 5209 5363 1866 3640 5684 1062 5213 5757 5417 5986 1466 5902 3109\n",
    " 4096  464  493  629 3912 3814 1268 3552 2438 5109 1207 4504 5634 4870\n",
    " 5229  320 4354  785 3167 2416 3128 4540 5457 5420 1241 1674 4850 5140\n",
    "   15  457  710 1245  942 5147 5164 4314 4346 3648 3180 3103 3189 1357\n",
    " 4917 3987 5571 2906  210 2489  901 2293 5737  879 2558  564 4310 2119\n",
    " 5521 5059 2376 3856 1967 4094]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number in rtrain_userAvg:\n",
    "    print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain_userAvg[0][0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain_userAvg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImplicitMatrix(sparseMatrix, threashold=0):\n",
    "    temp_matrix = sparse.csr_matrix(sparseMatrix.shape)\n",
    "    temp_matrix[(sparseMatrix > threashold).nonzero()] = 1\n",
    "    return temp_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "userVisitMatrix = getImplicitMatrix(rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UU similarity\n",
    "similarity3 = train(userVisitMatrix)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "user_item_prediction_score3 = predict(userVisitMatrix, 90, similarity3, item_similarity_en= False)\n",
    "user_item_predict3 = prediction(user_item_prediction_score3, 50, userVisitMatrix)\n",
    "#Check user item prediction score\n",
    "user_item_res3 = evaluate(user_item_predict3, rvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_index in tqdm(range(2)):\n",
    "    print(user_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userVisitMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(5,8,9,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.mean([1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([5,10,2], [10,5,9], [2,5,13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
