{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\hexiaoni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hexiaoni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hexiaoni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hexiaoni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hexiaoni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "#Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#Data Packages\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "#Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Counter\n",
    "from collections import Counter\n",
    "\n",
    "#Operation\n",
    "import operator\n",
    "\n",
    "#Natural Language Processing Packages\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "## Download Resources\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.data import find\n",
    "\n",
    "## Machine Learning\n",
    "import sklearn\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Polarity Score for Restaurant Specific Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = {\n",
    "#    'quick': ?.?\n",
    "}\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "#Update new words with polarity score\n",
    "sid.lexicon.update(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contraction Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Viewing Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', 50) #default 50, full text -1\n",
    "#pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datafile Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewJsonToronto = \"..\\\\data\\\\Export_TorontoData.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yelp_df(path = 'data/', filename = 'Export_CleanedReview.json', sampling=False, top_user_num=6100, top_item_num=4000):\n",
    "    \"\"\"\n",
    "    Get the pandas dataframe\n",
    "    Sampling only the top users/items by density \n",
    "    Implicit representation applies\n",
    "    \"\"\"\n",
    "    with open(filename,'r') as f:\n",
    "        data = f.readlines()\n",
    "        data = list(map(json.loads, data))\n",
    "    \n",
    "    data = data[0]\n",
    "    #Get all the data from the data\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df.rename(columns={'stars': 'review_stars', 'text': 'review_text', 'cool': 'review_cool',\n",
    "                       'funny': 'review_funny', 'useful': 'review_useful'}, inplace=True)\n",
    "\n",
    "    df['business_num_id'] = df.business_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.business_id.nunique()))\n",
    "    df['business_num_id'] = df['business_num_id'].astype('int')\n",
    "\n",
    "    df['user_num_id'] = df.user_id.astype('category').\\\n",
    "    cat.rename_categories(range(0, df.user_id.nunique()))\n",
    "    df['user_num_id'] = df['user_num_id'].astype('int')\n",
    "\n",
    "    df['timestamp'] = df['date'].apply(date_to_timestamp)\n",
    "\n",
    "    if sampling:\n",
    "        df = filter_yelp_df(df, top_user_num=top_user_num, top_item_num=top_item_num)\n",
    "        # Refresh num id\n",
    "        df['business_num_id'] = df.business_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.business_id.nunique()))\n",
    "        df['business_num_id'] = df['business_num_id'].astype('int')\n",
    "        \n",
    "        df['user_num_id'] = df.user_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.user_id.nunique()))\n",
    "        df['user_num_id'] = df['user_num_id'].astype('int')\n",
    "        # drop_list = ['date','review_id','review_funny','review_cool','review_useful']\n",
    "        # df = df.drop(drop_list, axis=1)\n",
    "    df = df.reset_index(drop = True)\n",
    "    return df \n",
    "\n",
    "def filter_yelp_df(df, top_user_num=6100, top_item_num=4000):\n",
    "    #Getting the reviews where starts are above 3\n",
    "    df_implicit = df[df['review_stars']>3]\n",
    "    frequent_user_id = df_implicit['user_num_id'].value_counts().head(top_user_num).index.values\n",
    "    frequent_item_id = df_implicit['business_num_id'].value_counts().head(top_item_num).index.values\n",
    "    return df.loc[(df['user_num_id'].isin(frequent_user_id)) & (df['business_num_id'].isin(frequent_item_id))]\n",
    "\n",
    "def date_to_timestamp(date):\n",
    "    dt = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    return time.mktime(dt.timetuple())\n",
    "\n",
    "def df_to_sparse(df, row_name='userId', col_name='movieId', value_name='rating',\n",
    "                 shape=None):\n",
    "    rows = df[row_name]\n",
    "    cols = df[col_name]\n",
    "    if value_name is not None:\n",
    "        values = df[value_name]\n",
    "    else:\n",
    "        values = [1]*len(rows)\n",
    "    return csr_matrix((values, (rows, cols)), shape=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !Action items: needs to update reviewJsonToronto File\n",
    "### Need to run, but can be done later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Day', 'Month', 'Unnamed: 0.1', 'Unnamed: 0_x', 'Unnamed: 0_y',\n",
       "       'Updated', 'Year', 'alias', 'business_id', 'business_stars',\n",
       "       'categories', 'coordinates', 'date', 'display_phone', 'distance',\n",
       "       'friend_count', 'ghost', 'image_url', 'img_dsc', 'img_url', 'is_closed',\n",
       "       'location', 'name', 'nr', 'phone', 'photo_count', 'price',\n",
       "       'review_count_x', 'review_count_y', 'review_date', 'review_id',\n",
       "       'review_language', 'review_stars', 'review_text', 'transactions', 'ufc',\n",
       "       'url', 'user_id', 'user_loc', 'vote_count', 'business_num_id',\n",
       "       'user_num_id', 'timestamp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_yelp_df(path ='', filename=reviewJsonToronto, sampling= True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Liuyishou Hotpot Downtown Toronto', 'Congee Queen', 'Chatime',\n",
       "       'GB Hand-pulled Noodles', 'KINTON RAMEN', 'Freshii', 'Tsujiri',\n",
       "       'Holiday Inn Express Toronto Downtown', 'Top Gun Burgers',\n",
       "       '98 Aroma', 'Pai Northern Thai Kitchen', '1915 Lan Zhou Ramen'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#('Pai Northern Thai Kitchen', 868), ('KINTON RAMEN', 832), ('Congee Queen', 596),  ('Tsujiri', 429), \n",
    "#('GB Hand-pulled Noodles', 229),  ('Chatime', 112), ('Freshii', 63)\n",
    "#('Liuyishou Hotpot Downtown Toronto', 10),  ('Holiday Inn Express Toronto Downtown', 3), ('Top Gun Burgers', 14)\n",
    "#('98 Aroma',21), ('1915 Lan Zhou Ramen', 52)\n",
    "df_Subset = df.loc[df['name'].isin(['1915 Lan Zhou Ramen','98 Aroma','Pai Northern Thai Kitchen', \\\n",
    "                                                'KINTON RAMEN', 'Congee Queen', 'Tsujiri','GB Hand-pulled Noodles', \\\n",
    "                                                'Chatime', 'Freshii', 'Liuyishou Hotpot Downtown Toronto', \\\n",
    "                                                'Holiday Inn Express Toronto Downtown', 'Top Gun Burgers'])]\n",
    "df_Subset['name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>Unnamed: 0_y</th>\n",
       "      <th>Updated</th>\n",
       "      <th>Year</th>\n",
       "      <th>alias</th>\n",
       "      <th>business_id</th>\n",
       "      <th>business_stars</th>\n",
       "      <th>...</th>\n",
       "      <th>review_text</th>\n",
       "      <th>transactions</th>\n",
       "      <th>ufc</th>\n",
       "      <th>url</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_loc</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>business_num_id</th>\n",
       "      <th>user_num_id</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>44214</td>\n",
       "      <td>44214</td>\n",
       "      <td>5885</td>\n",
       "      <td>False</td>\n",
       "      <td>2018</td>\n",
       "      <td>liuyishou-hotpot-downtown-toronto-toronto-2</td>\n",
       "      <td>r4uLhd8wBRea8H8sprBLUg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>A few problems with this place 1. Way too spic...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://www.yelp.com/biz/liuyishou-hotpot-down...</td>\n",
       "      <td>siCkQhJcmmUzOq6qUTCUhA</td>\n",
       "      <td>Montreal, Canada</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3464</td>\n",
       "      <td>5380</td>\n",
       "      <td>1.539749e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day  Month  Unnamed: 0.1  Unnamed: 0_x  Unnamed: 0_y  Updated  Year  \\\n",
       "0   17     10         44214         44214          5885    False  2018   \n",
       "\n",
       "                                         alias             business_id  \\\n",
       "0  liuyishou-hotpot-downtown-toronto-toronto-2  r4uLhd8wBRea8H8sprBLUg   \n",
       "\n",
       "   business_stars  ...                                        review_text  \\\n",
       "0             4.0  ...  A few problems with this place 1. Way too spic...   \n",
       "\n",
       "  transactions        ufc                                                url  \\\n",
       "0           []  [0, 0, 0]  https://www.yelp.com/biz/liuyishou-hotpot-down...   \n",
       "\n",
       "                  user_id          user_loc  vote_count business_num_id  \\\n",
       "0  siCkQhJcmmUzOq6qUTCUhA  Montreal, Canada         0.0            3464   \n",
       "\n",
       "  user_num_id     timestamp  \n",
       "0        5380  1.539749e+09  \n",
       "\n",
       "[1 rows x 43 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Subset.index = np.arange(0, len(df_Subset))\n",
    "df_Subset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>business_id</th>\n",
       "      <th>business_num_id</th>\n",
       "      <th>business_stars</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_stars</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liuyishou Hotpot Downtown Toronto</td>\n",
       "      <td>{'address1': '254 Spadina Avenue', 'address2':...</td>\n",
       "      <td>{'latitude': 43.65169, 'longitude': -79.39796}</td>\n",
       "      <td>r4uLhd8wBRea8H8sprBLUg</td>\n",
       "      <td>3464</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Y29fhhUpddT1DXDh-SajyA</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A few problems with this place 1. Way too spic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Liuyishou Hotpot Downtown Toronto</td>\n",
       "      <td>{'address1': '254 Spadina Avenue', 'address2':...</td>\n",
       "      <td>{'latitude': 43.65169, 'longitude': -79.39796}</td>\n",
       "      <td>r4uLhd8wBRea8H8sprBLUg</td>\n",
       "      <td>3464</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-DrDWlxrhUr0RCusOTl47Q</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The line up for this place on a Saturday eveni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                name  \\\n",
       "0  Liuyishou Hotpot Downtown Toronto   \n",
       "1  Liuyishou Hotpot Downtown Toronto   \n",
       "\n",
       "                                            location  \\\n",
       "0  {'address1': '254 Spadina Avenue', 'address2':...   \n",
       "1  {'address1': '254 Spadina Avenue', 'address2':...   \n",
       "\n",
       "                                      coordinates             business_id  \\\n",
       "0  {'latitude': 43.65169, 'longitude': -79.39796}  r4uLhd8wBRea8H8sprBLUg   \n",
       "1  {'latitude': 43.65169, 'longitude': -79.39796}  r4uLhd8wBRea8H8sprBLUg   \n",
       "\n",
       "   business_num_id  business_stars               review_id  review_stars  \\\n",
       "0             3464             4.0  Y29fhhUpddT1DXDh-SajyA           3.0   \n",
       "1             3464             4.0  -DrDWlxrhUr0RCusOTl47Q           4.0   \n",
       "\n",
       "                                         review_text  \n",
       "0  A few problems with this place 1. Way too spic...  \n",
       "1  The line up for this place on a Saturday eveni...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Subset_Cols = df_Subset[['name', 'location', 'coordinates', 'business_id', 'business_num_id', 'business_stars', 'review_id', 'review_stars', 'review_text']]\n",
    "df_Subset_Cols.head(2)\n",
    "#df_Subset_Cols.to_csv(r'df_Subset_Cols.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check bussiness num id to restaurant name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Liuyishou Hotpot Downtown Toronto'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Subset_Cols.loc[df_Subset_Cols['business_num_id'] == 3464, 'name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3490], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Subset_Cols.loc[df_Subset_Cols['name'] == 'Pai Northern Thai Kitchen', 'business_num_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval: Vader Polarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use vader to evaluated sentiment of reviews, vader here!\n",
    "def evalSentences(sentences, to_df=False, columns=[]):\n",
    "    #Instantiate an instance to access SentimentIntensityAnalyzer class\n",
    "    pdlist = []\n",
    "    if to_df:\n",
    "        for sentence in tqdm(sentences):\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            pdlist.append([sentence]+[ss['compound']])\n",
    "        reviewDf = pd.DataFrame(pdlist)\n",
    "        reviewDf.columns = columns\n",
    "        return reviewDf\n",
    "    \n",
    "    else:\n",
    "        for sentence in tqdm(sentences):\n",
    "            print(sentence)\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            for k in sorted(ss):\n",
    "                print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vader Sentiment Testing Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sentences = [\"pretty\", \"decent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Vader Compound Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pretty</td>\n",
       "      <td>0.4939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decent</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentence  Vader Compound Score\n",
       "0   pretty                0.4939\n",
       "1   decent                0.0000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalSentences(testing_sentences, to_df = True, columns = ['Sentence','Vader Compound Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract from All Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyphrases Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 30651/30651 [00:00<00:00, 379285.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set the business ID below\n",
    "#3464 Hotpot, 2159 Hand-pulled noodle, 3490 Pai Northern Thai\n",
    "review_text = df_Subset_Cols.loc[df_Subset_Cols['business_num_id'] == 2159, 'review_text'].sum()\n",
    "#review_text\n",
    "for word in tqdm(review_text.split()):\n",
    "    if word.lower() in contractions:\n",
    "        review_text = review_text.replace(word, contractions[word.lower()])\n",
    "#print(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 32231/32231 [00:00<00:00, 3590804.62it/s]\n"
     ]
    }
   ],
   "source": [
    "tagger = PerceptronTagger()\n",
    "pos_tag = tagger.tag\n",
    "taggedToks = pos_tag([word.lower() for word in tqdm(re.findall(r'\\w+', review_text))])\n",
    "# needs to change the regex above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Original\n",
    "# grammar = r\"\"\"\n",
    "#     NBAR:\n",
    "#         {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "#     NP:\n",
    "#         {<NBAR>}\n",
    "#         {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "# \"\"\"\n",
    "#-------------------------------------------------------------------\n",
    "# Version 1\n",
    "# # This grammar is described in the paper by S. N. Kim,\n",
    "# # T. Baldwin, and M.-Y. Kan.\n",
    "# # Evaluating n-gram based evaluation metrics for automatic\n",
    "# # keyphrase extraction.\n",
    "# # Technical report, University of Melbourne, Melbourne 2010.\n",
    "# grammar = r\"\"\"\n",
    "#     NBAR:\n",
    "#         {<JJ|JJR|JJS>+<NN|NNS>+}  # Adj (1 or more) and Noun (1 or more)\n",
    "        \n",
    "#     NP:\n",
    "#         {<NBAR>} # Tree\n",
    "#         {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2\n",
    "grammar1 = r\"\"\"\n",
    "    NBAR:\n",
    "        {<RB|RBS>?<JJ|JJR|JJS>+<NN|NNS>+}\n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}\n",
    "\"\"\"\n",
    "\n",
    "grammar2 = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN|NNS><VBZ|VBD><RB>?<JJ>+}\n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create phrase tree\n",
    "chunker1 = nltk.RegexpParser(grammar1)\n",
    "tree1= chunker1.parse(taggedToks)\n",
    "chunker2 = nltk.RegexpParser(grammar2)\n",
    "tree2= chunker2.parse(taggedToks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun Phrase Extraction Support Functions\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "# generator, generate leaves one by one\n",
    "# Filter only on NP (The other two labels may not be useful)\n",
    "def leaves(tree):\n",
    "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP' or t.label()=='JJ' or t.label()=='RB'):\n",
    "        yield subtree.leaves()\n",
    "\n",
    "# stemming, lematizing, lower case...\n",
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    #word = stemmer.stem(word)\n",
    "    if word != 'was':\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "    return word\n",
    "        \n",
    "# stop-words and length control\n",
    "def acceptable_word(word):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "    special_non_stopwords = ['is','was','are','were'] # Kick out a few words from stopwords list\n",
    "    final_stop_words = list([word for word in stopwords.words('english') if word not in special_non_stopwords])\n",
    "    accepted = (bool(2 <= len(word) <= 40)\n",
    "    and word.lower() not in final_stop_words)\n",
    "    return accepted\n",
    "        \n",
    "# generator, create item once a time\n",
    "def get_terms(tree):\n",
    "    for leaf in leaves(tree):\n",
    "        # Normalize word in \n",
    "        term = [normalise(w) for w,t in leaf if acceptable_word(w) ]\n",
    "        # Phrase only\n",
    "        if len(term)>1:\n",
    "            yield term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traverse tree and get noun phrases\n",
    "npTokenList1 = [word for word in get_terms(tree1)]\n",
    "# npTokenList\n",
    "npTokenList2 = [word for word in get_terms(tree2)]\n",
    "# Combine np token list to one\n",
    "npTokenList_duplicate = npTokenList1 + npTokenList2\n",
    "npTokenList = []\n",
    "for i in npTokenList_duplicate:\n",
    "    if i not in npTokenList:\n",
    "        npTokenList.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten phrase lists to get tokens for analysis\n",
    "def flatten(npTokenList):\n",
    "    finalList =[]\n",
    "    for phrase in npTokenList:\n",
    "        token = ''\n",
    "        for word in phrase:\n",
    "            token += word + ' '\n",
    "        finalList.append(token.rstrip())\n",
    "    return finalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Extracted_list1 = flatten(npTokenList1)\n",
    "Extracted_list2 = flatten(npTokenList2)\n",
    "#Extracted_list = flatten([word\n",
    "#                           for word\n",
    "#                           in get_terms(chunker.parse(pos_tag([word.lower()\n",
    "#                                                               for word in re.findall(r'\\w+', review_text)])))])\n",
    "Extracted_list = Extracted_list1 + list(set(Extracted_list2) - set(Extracted_list1))\n",
    "len(Extracted_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Phrase (Extracted List) Vader Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 2022/2022 [00:00<00:00, 15434.49it/s]\n"
     ]
    }
   ],
   "source": [
    "Extracted_list_vader =  evalSentences(Extracted_list, to_df = True, columns= ('extracted list','vader'))\n",
    "Extracted_list_vader.sort_values(\"vader\", ascending = False, inplace = True)\n",
    "#Extracted_list_vader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key phrases contain top frequent Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !The Pos tag below is tagging the key pharses which might be different from review text POS Tag above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of NN from the Keyphrase list\n",
    "NN_List=[]\n",
    "# for phrase in pos_tag(word):]\n",
    "for word in npTokenList:\n",
    "    phrase = pos_tag(word)\n",
    "    for term in phrase:\n",
    "        if term[1]=='NN':\n",
    "            NN_List.append(term[0])\n",
    "NN_List=list(dict.fromkeys(NN_List))\n",
    "#print (NN_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !Need to imprve the efficiency of the following function, count vectorizer?\n",
    "### !! Also needs to change the stop words to make it same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/543 [00:00<?, ?it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 543/543 [00:00<00:00, 34283.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['noodle', 948], ['soup', 249], ['place', 205], ['beef', 193], ['good', 161], ['hand', 129], ['broth', 125], ['bowl', 123], ['get', 123], ['time', 113], ['spicy', 110], ['size', 100], ['dish', 96], ['restaurant', 87], ['sauce', 85], ['service', 83], ['table', 82], ['menu', 81], ['food', 78], ['pork', 75], ['line', 71], ['side', 69], ['bit', 67], ['try', 60], ['oil', 56], ['taste', 53], ['flat', 53], ['wait', 52], ['order', 52], ['choose', 51], ['lanzhou', 50], ['meat', 49], ['portion', 48], ['friend', 48], ['option', 48], ['chewy', 47], ['lunch', 43], ['flavour', 42], ['thick', 41], ['lot', 41], ['price', 40], ['thin', 40], ['chinese', 40], ['overall', 39], ['mega', 39], ['texture', 38], ['chili', 37], ['salty', 37], ['recommend', 37], ['thickness', 36], ['quick', 36], ['extra', 34], ['super', 32], ['meal', 32], ['type', 31], ['downtown', 31], ['dinner', 31], ['medium', 30], ['amount', 30], ['hot', 30], ['staff', 30], ['seating', 29], ['simple', 29], ['cucumber', 29], ['appetizer', 28], ['thing', 28], ['spice', 28], ['perfect', 28], ['fast', 26], ['egg', 25], ['tasty', 25], ['area', 25], ['chilli', 24], ['chef', 24], ['ask', 24], ['style', 23], ['seat', 23], ['day', 23], ['love', 23], ['le', 23], ['thought', 23], ['space', 22], ['water', 22], ['choice', 22], ['jellyfish', 22], ['slice', 21], ['veggie', 21], ['cilantro', 21], ['dough', 20], ['server', 20], ['fan', 19], ['feel', 19], ['spot', 19], ['way', 19], ['base', 19], ['pho', 19], ['kind', 19], ['add', 19], ['tender', 18], ['door', 18], ['toronto', 17], ['level', 17], ['visit', 16], ['review', 16], ['customer', 16], ['onion', 15], ['tip', 15], ['pull', 15], ['person', 15], ['regular', 15], ['round', 15], ['pulling', 15], ['decent', 14], ['quality', 14], ['experience', 14], ['hour', 14], ['kitchen', 14], ['everything', 14], ['lineup', 14], ['kimchi', 14], ['tea', 14], ['mind', 14], ['flavor', 13], ['vegetarian', 13], ['night', 13], ['store', 13], ['friday', 13], ['light', 13], ['mean', 13], ['thicker', 13], ['location', 12], ['hungry', 12], ['item', 12], ['mild', 12], ['bland', 12], ['craving', 11], ['bite', 11], ['loud', 11], ['waiter', 11], ['radish', 11], ['kick', 11], ['handmade', 11], ['topping', 11], ['winter', 11], ['house', 10], ['tendon', 10], ['bar', 10], ['hype', 10], ['work', 10], ['shank', 10], ['guy', 10], ['group', 9], ['dente', 9], ['tomato', 9], ['half', 9], ['cannot', 9], ['version', 9], ['note', 9], ['shop', 9], ['standard', 9], ['yummy', 9], ['markham', 9], ['carrot', 9], ['nothing', 9], ['cut', 9], ['average', 9], ['everyone', 9], ['waitress', 9], ['entrance', 9], ['look', 9], ['sweet', 8], ['coriander', 8], ['part', 8], ['msg', 8], ['rush', 8], ['someone', 8], ['variety', 8], ['chewiness', 8], ['fun', 8], ['serving', 8], ['disappointing', 8], ['cool', 8], ['something', 7], ['opening', 7], ['kuan', 7], ['salad', 7], ['establishment', 7], ['daikon', 7], ['form', 7], ['fair', 7], ['cook', 7], ['sound', 7], ['container', 7], ['chair', 7], ['inside', 7], ['peak', 7], ['piece', 7], ['name', 6], ['favorite', 6], ['reason', 6], ['joint', 6], ['selection', 6], ['ingredient', 6], ['max', 6], ['opt', 6], ['bottom', 6], ['choy', 6], ['saw', 6], ['set', 6], ['mine', 6], ['ground', 6], ['heat', 6], ['glad', 6], ['top', 6], ['comfort', 5], ['thursday', 5], ['deal', 5], ['sign', 5], ['weekday', 5], ['offering', 5], ['width', 5], ['range', 5], ['bok', 5], ['etc', 5], ['stick', 5], ['shape', 5], ['number', 5], ['thinnest', 5], ['magic', 5], ['sunday', 5], ['wider', 5], ['man', 5], ['zhou', 5], ['feeling', 5], ['parsley', 4], ['summer', 4], ['bang', 4], ['counter', 4], ['con', 4], ['list', 4], ['saucy', 4], ['bouncy', 4], ['room', 4], ['pricing', 4], ['wednesday', 4], ['faster', 4], ['mediocre', 4], ['fix', 4], ['lan', 4], ['saturday', 4], ['stomach', 4], ['evening', 4], ['patron', 4], ['guess', 4], ['sesame', 4], ['please', 4], ['slamming', 4], ['cheap', 4], ['vinegar', 4], ['bokchoy', 4], ['consistency', 4], ['ratio', 4], ['refill', 4], ['husband', 4], ['signature', 4], ['protein', 3], ['beefy', 3], ['udon', 3], ['peppercorn', 3], ['balance', 3], ['non', 3], ['conversation', 3], ['dine', 3], ['elbow', 3], ['twist', 3], ['video', 3], ['preference', 3], ['thickest', 3], ['parking', 3], ['regardless', 3], ['solo', 3], ['month', 3], ['package', 3], ['bomb', 3], ['crunch', 3], ['stuff', 3], ['stop', 3], ['eater', 3], ['lunchtime', 3], ['touch', 3], ['mistake', 3], ['lol', 3], ['freshness', 3], ['cause', 3], ['cheaper', 3], ['idea', 3], ['stuffed', 3], ['station', 3], ['nonetheless', 3], ['wise', 3], ['noisy', 3], ['value', 3], ['compare', 3], ['wow', 3], ['atmosphere', 3], ['serf', 3], ['colleague', 3], ['thinner', 3], ['multiple', 3], ['brisket', 3], ['seaweed', 3], ['wife', 3], ['lover', 2], ['step', 2], ['interior', 2], ['speed', 2], ['key', 2], ['veg', 2], ['smokey', 2], ['series', 2], ['slam', 2], ['convenient', 2], ['bonus', 2], ['savoury', 2], ['debit', 2], ['transaction', 2], ['herb', 2], ['background', 2], ['speedy', 2], ['change', 2], ['cleanser', 2], ['chew', 2], ['credit', 2], ['random', 2], ['noise', 2], ['maker', 2], ['entree', 2], ['mess', 2], ['air', 2], ['banging', 2], ['pepper', 2], ['god', 2], ['aroma', 2], ['vibe', 2], ['smile', 2], ['variant', 2], ['diner', 2], ['rude', 2], ['concept', 2], ['springy', 2], ['digress', 2], ['harder', 2], ['thank', 2], ['translation', 2], ['colour', 2], ['chunk', 2], ['vinegary', 2], ['omg', 2], ['pricey', 2], ['wooden', 2], ['english', 2], ['tenderness', 2], ['soak', 2], ['benefit', 2], ['garlic', 2], ['smell', 2], ['appetite', 2], ['prepare', 2], ['process', 2], ['quantity', 2], ['traffic', 2], ['system', 2], ['plastic', 2], ['absolute', 2], ['bone', 2], ['turnip', 2], ['plane', 2], ['shanghai', 2], ['presentation', 2], ['bean', 2], ['soy', 2], ['milk', 2], ['beverage', 2], ['tho', 2], ['bounce', 2], ['problem', 2], ['north', 2], ['bud', 2], ['turnover', 2], ['worker', 2], ['turnaround', 2], ['resto', 2], ['lady', 2], ['partiescame', 1], ['buddy', 1], ['player', 1], ['phase', 1], ['fake', 1], ['news', 1], ['monday', 1], ['batch', 1], ['granite', 1], ['spring', 1], ['roll', 1], ['variation', 1], ['bibimbap', 1], ['privacy', 1], ['cheese', 1], ['cooking', 1], ['chart', 1], ['york', 1], ['access', 1], ['countless', 1], ['adventure', 1], ['combination', 1], ['basis', 1], ['curtain', 1], ['palate', 1], ['partway', 1], ['reviewer', 1], ['card', 1], ['technology', 1], ['heft', 1], ['period', 1], ['event', 1], ['audience', 1], ['yup', 1], ['checkup', 1], ['degree', 1], ['soupi', 1], ['brand', 1], ['jib', 1], ['zing', 1], ['soupy', 1], ['wonton', 1], ['alternate', 1], ['rice', 1], ['bandwagon', 1], ['performance', 1], ['hippo', 1], ['camper', 1], ['afternoon', 1], ['sorta', 1], ['perusal', 1], ['gauge', 1], ['soya', 1], ['master', 1], ['luck', 1], ['mass', 1], ['mount', 1], ['train', 1], ['spoon', 1], ['investigation', 1], ['activity', 1], ['alsonuff', 1], ['saida', 1], ['beijing', 1], ['dumpling', 1], ['apparel', 1], ['preparation', 1], ['thud', 1], ['tenant', 1], ['nicer', 1], ['foodie', 1], ['fail', 1], ['pet', 1], ['afterward', 1], ['cute', 1], ['washroom', 1], ['score', 1], ['vein', 1], ['song', 1], ['halal', 1], ['mien', 1], ['coup', 1], ['pasta', 1], ['feature', 1], ['puller', 1], ['snack', 1], ['tolerance', 1], ['kid', 1], ['frill', 1], ['creation', 1], ['straightforward', 1], ['girth', 1], ['presence', 1], ['expectation', 1], ['soybean', 1], ['pan', 1], ['bag', 1], ['tackle', 1], ['app', 1], ['rotation', 1], ['underdone', 1], ['steal', 1], ['bare', 1], ['ala', 1], ['liquid', 1], ['memory', 1], ['online', 1], ['tdlr', 1], ['gummy', 1], ['health', 1], ['descent', 1], ['hmm', 1], ['sprout', 1], ['stimulation', 1], ['shoutout', 1], ['deeply', 1], ['imo', 1], ['homey', 1], ['daunting', 1], ['mmmmm', 1], ['ba', 1], ['pit', 1], ['meeting', 1], ['hearty', 1], ['lam', 1], ['america', 1], ['vat', 1], ['aunty', 1], ['restriction', 1], ['arrangement', 1], ['hate', 1], ['ventilation', 1], ['site', 1], ['hahagb', 1], ['mouthful', 1], ['decor', 1], ['dietary', 1], ['al', 0], ['st', 0], ['dt', 0], ['gb', 0], ['ok', 0], ['xi', 0], ['la', 0], ['大寬', 0], ['ga', 0], ['ey', 0], ['bf', 0], ['el', 0], ['v', 0]]\n"
     ]
    }
   ],
   "source": [
    "#get a dictionary of NN and its frequency\n",
    "stop = set(stopwords.words('english'))\n",
    "dict_Counter = {}\n",
    "list_Counter = []\n",
    "for term in NN_List:\n",
    "    c=[normalise(word) for word in re.findall(r'\\w+', review_text) if word.lower() not in stop and len(word) > 2].count(term)\n",
    "    dict_Counter[term] = c\n",
    "\n",
    "#create a list of ranked [NN,frequency]\n",
    "for key, value in tqdm(sorted(dict_Counter.items(), key=lambda item: item[1], reverse = True)):\n",
    "    temp = [key,value]\n",
    "    list_Counter.append(temp)\n",
    "print(list_Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the number of top selected noun pharses here\n",
    "list_Counter_values = []\n",
    "for i in range(len(list_Counter)):\n",
    "    if list_Counter[i][1] >= list_Counter[14][1]: #if it is greater than the frequency of the 15th most frequent words, append\n",
    "        list_Counter_values.append(list_Counter[i][1])\n",
    "topk_nounphrases_index = len(list_Counter_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['noodle', 'soup', 'place', 'beef', 'good', 'hand', 'broth', 'bowl', 'get', 'time', 'spicy', 'size', 'dish', 'restaurant', 'sauce']\n"
     ]
    }
   ],
   "source": [
    "# Extract the topK frequent words(NN)\n",
    "topk_freq_words = []\n",
    "for i in range(topk_nounphrases_index):\n",
    "    topk_freq_words.append(list_Counter[i][0])\n",
    "print(topk_freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find out if a particular Noun Phrase has the word from topk frequent word list (binary)\n",
    "freqNP = []\n",
    "for i in range(len(Extracted_list)):\n",
    "    tempCounter = Counter([word.lower() for word in re.findall(r'\\w+',Extracted_list[i])])\n",
    "    topkinNP_List = [1 if tempCounter[word] > 0 else 0 for word in topk_freq_words]\n",
    "    freqNP.append(topkinNP_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create freqNPDf (indicate which NP contains the Top K word)\n",
    "freqNPDf = pd.DataFrame(freqNP)\n",
    "dfColName = []\n",
    "for word in topk_freq_words:\n",
    "    dfColName.append(word)\n",
    "freqNPDf.columns = dfColName\n",
    "\n",
    "dfRowName = []\n",
    "for NN in Extracted_list:\n",
    "    dfRowName.append(NN)\n",
    "freqNPDf.index = dfRowName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noodle</th>\n",
       "      <th>soup</th>\n",
       "      <th>place</th>\n",
       "      <th>beef</th>\n",
       "      <th>good</th>\n",
       "      <th>hand</th>\n",
       "      <th>broth</th>\n",
       "      <th>bowl</th>\n",
       "      <th>get</th>\n",
       "      <th>time</th>\n",
       "      <th>spicy</th>\n",
       "      <th>size</th>\n",
       "      <th>dish</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>sauce</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>favourite noodle house</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quick place</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        noodle  soup  place  beef  good  hand  broth  bowl  \\\n",
       "favourite noodle house       1     0      0     0     0     0      0     0   \n",
       "quick place                  0     0      1     0     0     0      0     0   \n",
       "\n",
       "                        get  time  spicy  size  dish  restaurant  sauce  \n",
       "favourite noodle house    0     0      0     0     0           0      0  \n",
       "quick place               0     0      0     0     0           0      0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqNPDf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyphrase_set = set(freqNPDf[(freqNPDf[topk_freq_words[0]] == 1) | (freqNPDf[topk_freq_words[1]] == 1) | (freqNPDf[topk_freq_words[2]] == 1) | (freqNPDf[topk_freq_words[3]] == 1) | (freqNPDf[topk_freq_words[4]] == 1)].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for i in topk_freq_words:\n",
    "    arr = arr + list(freqNPDf[freqNPDf[i] == 1].index.values)\n",
    "keyphrase_set = set(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if it contained top noun phrases, then select the key pharse\n",
    "keyphrase_list = list(keyphrase_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mild special lanzhou beef noodle',\n",
       " 'noodle was popular',\n",
       " 'noodle was also good']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrase_list [:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Selected Key phrases Vader Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 51/51 [00:00<00:00, 17050.02it/s]\n"
     ]
    }
   ],
   "source": [
    "df_keyphrase = evalSentences(keyphrase_list, to_df=True,columns = ['keyphrase_list', 'Vader Compound Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyphrase_list</th>\n",
       "      <th>Vader Compound Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pretty good good variety</td>\n",
       "      <td>0.8402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quality is pretty good good</td>\n",
       "      <td>0.8402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>food is pretty impressive overall</td>\n",
       "      <td>0.7579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>helpful good dipping sauce</td>\n",
       "      <td>0.6908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>awesome hot pot</td>\n",
       "      <td>0.6249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       keyphrase_list  Vader Compound Score\n",
       "2            pretty good good variety                0.8402\n",
       "5         quality is pretty good good                0.8402\n",
       "10  food is pretty impressive overall                0.7579\n",
       "33         helpful good dipping sauce                0.6908\n",
       "18                    awesome hot pot                0.6249"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_keyphrase.sort_values(\"Vader Compound Score\", ascending = False, inplace = True)\n",
    "df_keyphrase.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pretty good good variety', 'quality is pretty good good',\n",
       "       'food is pretty impressive overall', 'helpful good dipping sauce',\n",
       "       'awesome hot pot', 'place is great', 'great sauce bar',\n",
       "       'free dessert', 'really good beef lam', 'place is really good',\n",
       "       'favorite meat', 'new favorite hot pot', 'good sign',\n",
       "       'broth good variety', 'good hotpot session', 'food was good',\n",
       "       'food was fresh', 'single time', 'extra time', 'hot pot',\n",
       "       'hot soup', 'several soup base', 'enough broth',\n",
       "       'extra spicy broth', 'actual mushroom soup', 'asap service',\n",
       "       'service is decent', 'hot pot experience tip',\n",
       "       'taste is much different', 'high quality', 'dipping sauce',\n",
       "       'favourable hot pot place', 'replenished3 pot', 'side dish',\n",
       "       'spicy was salty', 'sichuan hot pot', 'dual pot spicy',\n",
       "       'ayce place', 'hot pot place', 'large table', 'pricey side',\n",
       "       'expansive sauce selection', 'wide variety', 'coldest day',\n",
       "       'ayce hot pot', 'individual hotpot table',\n",
       "       'absolute favourite hot pot place', 'high quality meat',\n",
       "       'flavourful dipping mix', 'soup tomato', 'damn spicy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_keyphrase.keyphrase_list.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "array_review_3464 = df_Subset_Cols.loc[df_Subset_Cols['business_num_id'] == 3464, 'review_text'].values\n",
    "#array_review_3464"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Selected Keyphrase Vader Score based on entire reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases_array = df_keyphrase.keyphrase_list.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 981.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 494.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 250.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 974.29it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1014.59it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 495.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 506.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 978.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1004.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 495.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 502.25it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 489.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 331.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 494.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 334.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 508.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 495.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1040.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 495.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 494.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 499.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 494.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1012.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 331.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 331.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 495.37it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 494.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 495.31it/s]\n"
     ]
    }
   ],
   "source": [
    "#chunk\n",
    "ans_array = []\n",
    "for keyphrase in keyphrases_array:\n",
    "    lst = []\n",
    "    for i in array_review_3464:\n",
    "        if keyphrase in normalise(i):\n",
    "            temp_list = []\n",
    "            temp_list.append(i)\n",
    "            lst.append(evalSentences(temp_list, to_df=True,columns = ['keyphrase_list', 'Vader Compound Score'])[\"Vader Compound Score\"][0])\n",
    "    if len(lst) == 0:\n",
    "        lst = [0]\n",
    "    temp = statistics.mean(lst)\n",
    "    ans_array.append([keyphrase,temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['pretty good good variety', 0],\n",
       " ['quality is pretty good good', 0],\n",
       " ['food is pretty impressive overall', 0],\n",
       " ['helpful good dipping sauce', 0],\n",
       " ['awesome hot pot', 0.9801],\n",
       " ['place is great', 0.8625],\n",
       " ['great sauce bar', 0.9247],\n",
       " ['free dessert', 0.7136],\n",
       " ['really good beef lam', 0],\n",
       " ['place is really good', 0.9247],\n",
       " ['favorite meat', 0.9817],\n",
       " ['new favorite hot pot', 0.9817],\n",
       " ['good sign', 0.8625],\n",
       " ['broth good variety', 0],\n",
       " ['good hotpot session', 0.9492],\n",
       " ['food was good', 0.9698],\n",
       " ['food was fresh', 0.2382],\n",
       " ['single time', 0.9801],\n",
       " ['extra time', 0.9817],\n",
       " ['hot pot', 0.8168333333333333],\n",
       " ['hot soup', 0.9817],\n",
       " ['several soup base', 0.9492],\n",
       " ['enough broth', 0.9801],\n",
       " ['extra spicy broth', 0.7136],\n",
       " ['actual mushroom soup', 0.9492],\n",
       " ['asap service', 0],\n",
       " ['service is decent', 0.9916],\n",
       " ['hot pot experience tip', 0],\n",
       " ['taste is much different', 0],\n",
       " ['high quality', 0.8625],\n",
       " ['dipping sauce', 0.9817],\n",
       " ['favourable hot pot place', 0.7136],\n",
       " ['replenished3 pot', 0],\n",
       " ['side dish', 0.8526],\n",
       " ['spicy was salty', 0],\n",
       " ['sichuan hot pot', 0.8625],\n",
       " ['dual pot spicy', 0],\n",
       " ['ayce place', 0.7136],\n",
       " ['hot pot place', 0.8878333333333334],\n",
       " ['large table', 0.9916],\n",
       " ['pricey side', 0.7136],\n",
       " ['expansive sauce selection', 0.8625],\n",
       " ['wide variety', 0.9809],\n",
       " ['coldest day', 0.9492],\n",
       " ['ayce hot pot', 0.3933],\n",
       " ['individual hotpot table', 0.9492],\n",
       " ['absolute favourite hot pot place', 0.9801],\n",
       " ['high quality meat', 0.8625],\n",
       " ['flavourful dipping mix', 0.8625],\n",
       " ['soup tomato', 0],\n",
       " ['damn spicy', 0.9801]]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Result1_df = pd.DataFrame(ans_array)\n",
    "Result1_df.rename(columns={0: \"restaurants\", 1: \"Vader\"},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurants</th>\n",
       "      <th>Vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>service is decent</td>\n",
       "      <td>0.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>large table</td>\n",
       "      <td>0.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>favorite meat</td>\n",
       "      <td>0.981700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>extra time</td>\n",
       "      <td>0.981700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>new favorite hot pot</td>\n",
       "      <td>0.981700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hot soup</td>\n",
       "      <td>0.981700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>dipping sauce</td>\n",
       "      <td>0.981700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>wide variety</td>\n",
       "      <td>0.980900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awesome hot pot</td>\n",
       "      <td>0.980100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>absolute favourite hot pot place</td>\n",
       "      <td>0.980100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>enough broth</td>\n",
       "      <td>0.980100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>single time</td>\n",
       "      <td>0.980100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>damn spicy</td>\n",
       "      <td>0.980100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>food was good</td>\n",
       "      <td>0.969800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>coldest day</td>\n",
       "      <td>0.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>individual hotpot table</td>\n",
       "      <td>0.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>several soup base</td>\n",
       "      <td>0.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>actual mushroom soup</td>\n",
       "      <td>0.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>good hotpot session</td>\n",
       "      <td>0.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>great sauce bar</td>\n",
       "      <td>0.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>place is really good</td>\n",
       "      <td>0.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>hot pot place</td>\n",
       "      <td>0.887833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>flavourful dipping mix</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>good sign</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>high quality meat</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>sichuan hot pot</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>expansive sauce selection</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>place is great</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>high quality</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>side dish</td>\n",
       "      <td>0.852600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hot pot</td>\n",
       "      <td>0.816833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>favourable hot pot place</td>\n",
       "      <td>0.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>extra spicy broth</td>\n",
       "      <td>0.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>free dessert</td>\n",
       "      <td>0.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>pricey side</td>\n",
       "      <td>0.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ayce place</td>\n",
       "      <td>0.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ayce hot pot</td>\n",
       "      <td>0.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>food was fresh</td>\n",
       "      <td>0.238200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>soup tomato</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pretty good good variety</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>dual pot spicy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>spicy was salty</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>replenished3 pot</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>taste is much different</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hot pot experience tip</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quality is pretty good good</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>broth good variety</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>really good beef lam</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>helpful good dipping sauce</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food is pretty impressive overall</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>asap service</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          restaurants     Vader\n",
       "26                  service is decent  0.991600\n",
       "39                        large table  0.991600\n",
       "10                      favorite meat  0.981700\n",
       "18                         extra time  0.981700\n",
       "11               new favorite hot pot  0.981700\n",
       "20                           hot soup  0.981700\n",
       "30                      dipping sauce  0.981700\n",
       "42                       wide variety  0.980900\n",
       "4                     awesome hot pot  0.980100\n",
       "46   absolute favourite hot pot place  0.980100\n",
       "22                       enough broth  0.980100\n",
       "17                        single time  0.980100\n",
       "50                         damn spicy  0.980100\n",
       "15                      food was good  0.969800\n",
       "43                        coldest day  0.949200\n",
       "45            individual hotpot table  0.949200\n",
       "21                  several soup base  0.949200\n",
       "24               actual mushroom soup  0.949200\n",
       "14                good hotpot session  0.949200\n",
       "6                     great sauce bar  0.924700\n",
       "9                place is really good  0.924700\n",
       "38                      hot pot place  0.887833\n",
       "48             flavourful dipping mix  0.862500\n",
       "12                          good sign  0.862500\n",
       "47                  high quality meat  0.862500\n",
       "35                    sichuan hot pot  0.862500\n",
       "41          expansive sauce selection  0.862500\n",
       "5                      place is great  0.862500\n",
       "29                       high quality  0.862500\n",
       "33                          side dish  0.852600\n",
       "19                            hot pot  0.816833\n",
       "31           favourable hot pot place  0.713600\n",
       "23                  extra spicy broth  0.713600\n",
       "7                        free dessert  0.713600\n",
       "40                        pricey side  0.713600\n",
       "37                         ayce place  0.713600\n",
       "44                       ayce hot pot  0.393300\n",
       "16                     food was fresh  0.238200\n",
       "49                        soup tomato  0.000000\n",
       "0            pretty good good variety  0.000000\n",
       "36                     dual pot spicy  0.000000\n",
       "34                    spicy was salty  0.000000\n",
       "32                   replenished3 pot  0.000000\n",
       "28            taste is much different  0.000000\n",
       "27             hot pot experience tip  0.000000\n",
       "1         quality is pretty good good  0.000000\n",
       "13                 broth good variety  0.000000\n",
       "8                really good beef lam  0.000000\n",
       "3          helpful good dipping sauce  0.000000\n",
       "2   food is pretty impressive overall  0.000000\n",
       "25                       asap service  0.000000"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result1_df.sort_values(\"Vader\", ascending = False, inplace = True)\n",
    "Result1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Vader score keyphrase based on sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_sent = tokenize.sent_tokenize(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A few problems with this place 1.',\n",
       " 'Way too spicy2.',\n",
       " 'Hard to get service.',\n",
       " 'Soup do not get refilled in time.',\n",
       " 'And dish by sauce station do not get replenished3.',\n",
       " 'Pots are very shadow therefore easy to spill overIngredients are very fresh compared with other ayce hot pot, which is a big plusThe line up for this place on a Saturday evening is nightmarish.',\n",
       " 'We waited for about 2 hours before being seated.',\n",
       " 'However, they have a system where they text you when your seat is ready and will hold your table for 10 minutes.',\n",
       " 'Personally I generally prefer other places for hot pot.',\n",
       " 'We ordered the dual pot (Spicy and Pork Bone).',\n",
       " 'I found that the soup lacked in flavour and was quite oily.',\n",
       " 'They do however, have an expansive sauce selection so I was able to create a flavourful dipping mix for my hot pot.',\n",
       " 'Their meat cuts are of high quality.',\n",
       " 'The beef slices had fatty streaks which gives it a tender and melt-in-your-mouth texture.',\n",
       " 'The lamb did not taste too lamby which is a good sign of high quality meat.',\n",
       " 'I enjoyed eating the different housemade balls (e.g.',\n",
       " 'fish, shrimp) as they prepare it as a paste for you.',\n",
       " 'The variety of choices for food is pretty impressive.',\n",
       " 'Overall, I think this place is great for its quality and variety but i would still personally choose to have Sichuan hot pot.Hearing lots about this hot pot place we decided to venture and try it out!',\n",
       " 'Reading yelp reviews we decided to make a reservation for 6 and thank goodness we did as when we arrived we saw many people waiting even though our reservation was 8:15.',\n",
       " 'We sat in the middle table and they set up little dividers to ensure we still felt that we had a private space between our group and the others who shared the table.Food was good and came very quickly!',\n",
       " 'My favorite was the fish paste!',\n",
       " 'Fresh and delicious.',\n",
       " 'They did not have dessert and had fruits by the sauce station which I did not eat as I was not sure how long the fruits been there for!But overall it was great as everyone got their own pot and could choose what soup base they wanted.',\n",
       " \"Only thing we did not like was that you could only order pitchers of beer no other options.I'm not a hotpot fan but this place is really good.\",\n",
       " 'Beef, lam and beef tongue were so tender and thin, just need to let them stay in the pot for 5 seconds and they shall / they will literally melt in your mouth.',\n",
       " 'The homemade shrimp/ fish/ cuttle were fresh too.',\n",
       " 'They have great sauce bar with various options, also mixture instructions on the side.Do make reservation, otherwise you may have to wait for ~1 hour...Liuyishou is my new favorite hot pot in downtown!',\n",
       " 'I really like the broth, and they have a wide variety of food to choose from.',\n",
       " 'There are many types of broth, but I normally just go for the spicy one.',\n",
       " 'I have tried the one where they put a shaped oil in the center and melt as you pour hot soup over it.',\n",
       " \"it has / it is more for the 'gram and the taste is not much different from the regular one.\",\n",
       " 'We had to wait extra time for it as well.',\n",
       " 'My favorite meat is actually the pork neck.',\n",
       " \"it has / it is super tender even if you accidentally cook it for longer.There's often all you can eat fruits near the dipping sauce.\",\n",
       " 'There are instructions for you to mix your own dipping sauce as well.',\n",
       " 'Very helpful!',\n",
       " 'Good dipping sauce can definitely enhance your hot pot experience.TIP OF THE DAY: \\xa0Make reservations!Liuyoushou is apparently very well known throughout Canada & in Asia - this translates directly to how busy the place gets in Toronto.After reading the reviews I HAD to try it.I came with my family on a Wednesday night and it felt like it was the coldest day of the year.',\n",
       " \"All the individual hotpot tables were already booked so we shared one of the bigger ones that you can split in half.The place was fully crammed and the people who came in before us without a reservation was told a waiting time of 'at least an hour'.\",\n",
       " 'This is a huge PLUS - I have no idea why but Asian restaurants are somewhat dishonest with the wait times but here they give it to you straight.We were handed 2 menus and after the initial ordering where a server comes to you with an iPad strapped to their wrists, they leave you with 1 menu.',\n",
       " 'The menus are extremely extensive and comprehensive of everything you had / you would want for a good hotpot session.There are several soup bases to choose from but my go to is definitely the mushroom.',\n",
       " 'It has a aromatic Chinese-herb flavor that is super appetizing.You get your own sauces at their Sauce Bar.',\n",
       " 'It consists of your usual fixings and a lot more such as soy sauce, vinegar, chili oil, dried chili, raw egg, cilantro etc.',\n",
       " 'They also serve cut oranges at this bar too (yay).The food comes out very quickly and in huge portions.',\n",
       " 'My mom complains about me only eating vegetables and vermicelli at HP.',\n",
       " 'But damn it tastes soooo good!',\n",
       " 'The other soups (tomato and pork bone) were refilled with water but mine was refilled with the actual mushroom soup (YES).',\n",
       " 'The bill comes to about $40 including tip (on a week day!).',\n",
       " 'It is a little pricey for me because I just graduated but I defo think this is worth the money if you want to splurge!My fiancé is from China, and this is his absolute favourite hot pot place in Toronto.',\n",
       " 'He loves the authenticity of the broths.',\n",
       " 'I usually get the spicy one, but just as a forewarning it is damn spicy even with one pepper level.',\n",
       " 'The prickly ash in the broth probably amplifies the spiciness though.',\n",
       " 'There is a wide variety of options on the menu and everything is fresh- there has not been a single time where the food was not fresh.',\n",
       " 'I love sauces and the fact that liuyishou has a sauce bar with instructions on how to make some sauces makes it that much better.',\n",
       " 'Along with the sauce bar you can grab some kimchi, seaweed salad, eggs, and fruit for dessert.',\n",
       " 'Oh- also you can grab some extra napkin and utensils if you need them ASAP.',\n",
       " 'Service is amazing here.',\n",
       " 'We have been remembered and the staff is always making sure that we have everything from enough broth to water with a smile.',\n",
       " 'I really appreciate this because I feel like a lot of Chinese restaurants lack this sort of service.',\n",
       " 'Definitely check our liuyishou for some awesome hot pot.A little slow on the service, the spicy was too salty.',\n",
       " 'The food was fresh but the self serve bar was a little skimpy.',\n",
       " 'I had to eat a ton of fruit to balance the salt.',\n",
       " 'I think they ran out the pineapple and it didnt look to fresh.',\n",
       " 'Pricy as well, damage was over 100 somehow.This is a chain from Sichuan.',\n",
       " 'There is a branch in Richmond Hill and this one recently opened.',\n",
       " 'Pretty solid all you can eat hotpot.',\n",
       " 'Came here with group of 3 on a Sunday evening, and still got a 45m-1 hour estimate.',\n",
       " 'We wanted to reserve via phone the day before but they do not take reservations for small groups.',\n",
       " 'They take your phone number in their system so you can hang out elsewhere while waiting.',\n",
       " 'The estimate was fairly accurate.',\n",
       " 'We had a large table toward the back.',\n",
       " 'Ordered a lighter broth and a spicy broth.',\n",
       " 'Good variety of vegetable and meat dishes.',\n",
       " 'Food quality is pretty good, good variety of sauces and side dishes.',\n",
       " 'Service is decent.',\n",
       " 'Knowing Mandarin obviously helps get attention from the staff.',\n",
       " 'Interior is modern and clean.',\n",
       " 'Mainly Asian clientele, but there are English speaking staff.',\n",
       " 'it has / it is really crowded even at 11pm on Sunday, just goes to show how popular the restaurant is.',\n",
       " 'Overall really enjoyed our time and would recommend if you are looking for a more upscale and clean eating experience in Chinatown.I have mixed feelings about this place.',\n",
       " 'One of my favourable hot pot places downtown from the selection + side dishes.',\n",
       " 'I got the 3x extra spicy broth and it was not that flavourful but mouth numbing which was not a fun experience but 100% my fault for choosing the spiciest one, I usually like spice for the flavour but this did not taste spicy..the MSG or oilyness of the broth must have numbed my mouth.',\n",
       " 'Honestly because of the servers (well female servers) my experience was not the best because as I kept trying to get their attention they would look at me and walk away or take my order down then forget it so I had to ask for multiple items 2-3x...which was annoying.No AYCE or at least 1 free dessert too which most AYCE places have and even w/o the free dessert it was on the pricey side of AYCE hotpot places.',\n",
       " 'I would still recommend but definitely will not be a regular here...']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1022.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1039.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1012.63it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 978.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 978.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 977.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1005.11it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1014.10it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1015.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1012.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1029.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1007.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "sent_ans_array = []\n",
    "for keyphrase in keyphrases_array:\n",
    "    lst = []\n",
    "    for i in review_sent:\n",
    "        if keyphrase in normalise(i):\n",
    "            temp_list = []\n",
    "            temp_list.append(i)\n",
    "            lst.append(evalSentences(temp_list, to_df=True,columns = ['keyphrase_list', 'Vader Compound Score'])[\"Vader Compound Score\"][0])\n",
    "    if len(lst) == 0:\n",
    "        lst = [0]\n",
    "    temp = statistics.mean(lst)\n",
    "    sent_ans_array.append([keyphrase,temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['pretty good good variety', 0],\n",
       " ['quality is pretty good good', 0],\n",
       " ['food is pretty impressive overall', 0],\n",
       " ['helpful good dipping sauce', 0],\n",
       " ['awesome hot pot', 0.7783],\n",
       " ['place is great', 0.4295],\n",
       " ['great sauce bar', 0.8122],\n",
       " ['free dessert', 0.6522],\n",
       " ['really good beef lam', 0],\n",
       " ['place is really good', 0.3926],\n",
       " ['favorite meat', 0.4588],\n",
       " ['new favorite hot pot', 0.8122],\n",
       " ['good sign', 0.4404],\n",
       " ['broth good variety', 0],\n",
       " ['good hotpot session', 0.7416],\n",
       " ['food was good', 0.7863],\n",
       " ['food was fresh', 0.1655],\n",
       " ['single time', 0.5574],\n",
       " ['extra time', 0.2732],\n",
       " ['hot pot', 0.48146666666666665],\n",
       " ['hot soup', 0.0],\n",
       " ['several soup base', 0.7416],\n",
       " ['enough broth', 0.5859],\n",
       " ['extra spicy broth', -0.834],\n",
       " ['actual mushroom soup', 0.0],\n",
       " ['asap service', 0],\n",
       " ['service is decent', 0.0],\n",
       " ['hot pot experience tip', 0],\n",
       " ['taste is much different', 0],\n",
       " ['high quality', 0.07220000000000001],\n",
       " ['dipping sauce', 0.5009666666666667],\n",
       " ['favourable hot pot place', 0.0],\n",
       " ['replenished3 pot', 0],\n",
       " ['side dish', 0.4201],\n",
       " ['spicy was salty', 0],\n",
       " ['sichuan hot pot', 0.4295],\n",
       " ['dual pot spicy', 0],\n",
       " ['ayce place', 0.6522],\n",
       " ['hot pot place', 0.3016],\n",
       " ['large table', 0.0],\n",
       " ['pricey side', 0.6522],\n",
       " ['expansive sauce selection', 0.2732],\n",
       " ['wide variety', 0.48875],\n",
       " ['coldest day', 0.8685],\n",
       " ['ayce hot pot', 0.6962],\n",
       " ['individual hotpot table', 0.3976],\n",
       " ['absolute favourite hot pot place', 0.4753],\n",
       " ['high quality meat', 0.4404],\n",
       " ['flavourful dipping mix', 0.2732],\n",
       " ['soup tomato', 0],\n",
       " ['damn spicy', -0.5499]]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_ans_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_Result1_df = pd.DataFrame(sent_ans_array)\n",
    "sent_Result1_df.rename(columns={0: \"restaurants\", 1: \"Vader\"},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurants</th>\n",
       "      <th>Vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>coldest day</td>\n",
       "      <td>0.868500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>great sauce bar</td>\n",
       "      <td>0.812200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>new favorite hot pot</td>\n",
       "      <td>0.812200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>food was good</td>\n",
       "      <td>0.786300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awesome hot pot</td>\n",
       "      <td>0.778300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>good hotpot session</td>\n",
       "      <td>0.741600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>several soup base</td>\n",
       "      <td>0.741600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ayce hot pot</td>\n",
       "      <td>0.696200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ayce place</td>\n",
       "      <td>0.652200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>pricey side</td>\n",
       "      <td>0.652200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>free dessert</td>\n",
       "      <td>0.652200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>enough broth</td>\n",
       "      <td>0.585900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>single time</td>\n",
       "      <td>0.557400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>dipping sauce</td>\n",
       "      <td>0.500967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>wide variety</td>\n",
       "      <td>0.488750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hot pot</td>\n",
       "      <td>0.481467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>absolute favourite hot pot place</td>\n",
       "      <td>0.475300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>favorite meat</td>\n",
       "      <td>0.458800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>good sign</td>\n",
       "      <td>0.440400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>high quality meat</td>\n",
       "      <td>0.440400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>sichuan hot pot</td>\n",
       "      <td>0.429500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>place is great</td>\n",
       "      <td>0.429500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>side dish</td>\n",
       "      <td>0.420100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>individual hotpot table</td>\n",
       "      <td>0.397600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>place is really good</td>\n",
       "      <td>0.392600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>hot pot place</td>\n",
       "      <td>0.301600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>extra time</td>\n",
       "      <td>0.273200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>expansive sauce selection</td>\n",
       "      <td>0.273200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>flavourful dipping mix</td>\n",
       "      <td>0.273200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>food was fresh</td>\n",
       "      <td>0.165500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>high quality</td>\n",
       "      <td>0.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pretty good good variety</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>large table</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>soup tomato</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>dual pot spicy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>asap service</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>spicy was salty</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>replenished3 pot</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>favourable hot pot place</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>taste is much different</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hot pot experience tip</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>service is decent</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quality is pretty good good</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>actual mushroom soup</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hot soup</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>broth good variety</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>really good beef lam</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>helpful good dipping sauce</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food is pretty impressive overall</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>damn spicy</td>\n",
       "      <td>-0.549900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>extra spicy broth</td>\n",
       "      <td>-0.834000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          restaurants     Vader\n",
       "43                        coldest day  0.868500\n",
       "6                     great sauce bar  0.812200\n",
       "11               new favorite hot pot  0.812200\n",
       "15                      food was good  0.786300\n",
       "4                     awesome hot pot  0.778300\n",
       "14                good hotpot session  0.741600\n",
       "21                  several soup base  0.741600\n",
       "44                       ayce hot pot  0.696200\n",
       "37                         ayce place  0.652200\n",
       "40                        pricey side  0.652200\n",
       "7                        free dessert  0.652200\n",
       "22                       enough broth  0.585900\n",
       "17                        single time  0.557400\n",
       "30                      dipping sauce  0.500967\n",
       "42                       wide variety  0.488750\n",
       "19                            hot pot  0.481467\n",
       "46   absolute favourite hot pot place  0.475300\n",
       "10                      favorite meat  0.458800\n",
       "12                          good sign  0.440400\n",
       "47                  high quality meat  0.440400\n",
       "35                    sichuan hot pot  0.429500\n",
       "5                      place is great  0.429500\n",
       "33                          side dish  0.420100\n",
       "45            individual hotpot table  0.397600\n",
       "9                place is really good  0.392600\n",
       "38                      hot pot place  0.301600\n",
       "18                         extra time  0.273200\n",
       "41          expansive sauce selection  0.273200\n",
       "48             flavourful dipping mix  0.273200\n",
       "16                     food was fresh  0.165500\n",
       "29                       high quality  0.072200\n",
       "0            pretty good good variety  0.000000\n",
       "39                        large table  0.000000\n",
       "49                        soup tomato  0.000000\n",
       "36                     dual pot spicy  0.000000\n",
       "25                       asap service  0.000000\n",
       "34                    spicy was salty  0.000000\n",
       "32                   replenished3 pot  0.000000\n",
       "31           favourable hot pot place  0.000000\n",
       "28            taste is much different  0.000000\n",
       "27             hot pot experience tip  0.000000\n",
       "26                  service is decent  0.000000\n",
       "1         quality is pretty good good  0.000000\n",
       "24               actual mushroom soup  0.000000\n",
       "20                           hot soup  0.000000\n",
       "13                 broth good variety  0.000000\n",
       "8                really good beef lam  0.000000\n",
       "3          helpful good dipping sauce  0.000000\n",
       "2   food is pretty impressive overall  0.000000\n",
       "50                         damn spicy -0.549900\n",
       "23                  extra spicy broth -0.834000"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_Result1_df.sort_values(\"Vader\", ascending = False, inplace = True)\n",
    "sent_Result1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_Result1_df.reset_index(inplace = True)\n",
    "top_5_list = topk_freq_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sauce': 'great sauce bar',\n",
       " 'pot': 'new favorite hot pot',\n",
       " 'place': 'ayce place',\n",
       " 'broth': 'enough broth',\n",
       " 'good': 'food was good'}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict = {}\n",
    "for word in top_5_list:\n",
    "    for i in range(sent_Result1_df.restaurants.values.shape[0]):\n",
    "        if word in sent_Result1_df[\"restaurants\"][i]:\n",
    "            if word not in res_dict:\n",
    "                res_dict[word] = sent_Result1_df[\"restaurants\"][i]\n",
    "            if word in res_dict:\n",
    "                if sent_Result1_df[\"Vader\"][i] > sent_Result1_df[\"Vader\"][list(sent_Result1_df[\"restaurants\"].values).index(sent_Result1_df[\"restaurants\"][i])]:\n",
    "                    res_dict[word] = sent_Result1_df[\"restaurants\"][i]\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For loop (Inactive)\n",
    "## Get unique Business IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3464,  865, 3933, 2159, 1860, 1814, 3721, 3684, 2782, 3020,  925,\n",
       "        132, 3181, 2204, 1903,  352, 2735, 1470,  338, 3490, 1798, 3580,\n",
       "       1367, 3880, 2289,  149, 3390, 1670], dtype=int64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_business_num_id = df_Subset_Cols['business_num_id'].unique()\n",
    "unique_business_num_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_num_id</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11365</th>\n",
       "      <td>3464</td>\n",
       "      <td>A few problems with this place 1. Way too spic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11366</th>\n",
       "      <td>3464</td>\n",
       "      <td>The line up for this place on a Saturday eveni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11367</th>\n",
       "      <td>3464</td>\n",
       "      <td>Hearing lots about this hot pot place we decid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11368</th>\n",
       "      <td>3464</td>\n",
       "      <td>I'm not a hotpot fan but this place is really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11369</th>\n",
       "      <td>3464</td>\n",
       "      <td>Liuyishou is my new favorite hot pot in downto...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       business_num_id                                        review_text\n",
       "11365             3464  A few problems with this place 1. Way too spic...\n",
       "11366             3464  The line up for this place on a Saturday eveni...\n",
       "11367             3464  Hearing lots about this hot pot place we decid...\n",
       "11368             3464  I'm not a hotpot fan but this place is really ...\n",
       "11369             3464  Liuyishou is my new favorite hot pot in downto..."
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Subset_Cols_Agg = df_Subset_Cols[['business_num_id', 'review_text']]\n",
    "df_Subset_Cols_Agg.reset_index()\n",
    "df_Subset_Cols_Agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in unique_business_num_id:\n",
    "    review_text = df_Subset_Cols_Agg.loc[df_Subset_Cols_Agg['business_num_id'] == i, 'review_text'].sum()\n",
    "    extracted_temp = flatten([word\n",
    "                              for word\n",
    "                              in get_terms(chunker.parse(pos_tag([word.lower()\n",
    "                                                                  for word in re.findall(r'\\w+', review_text)])))])\n",
    "    print(extracted_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#extracted_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Postivie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vader Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df_Subset['review_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3229/3229 [00:03<00:00, 897.65it/s]\n"
     ]
    }
   ],
   "source": [
    "reviewDF = evalSentences(reviews, to_df=True, columns = ['reviewCol','vader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Subset_Cols_wGT = df_Subset_Cols.merge(reviewDF, left_index=True, right_index=True, how='inner')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
