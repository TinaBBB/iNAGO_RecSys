{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.sparse import csr_matrix, load_npz, save_npz\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import yaml\n",
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewJson = \"..\\\\data\\\\Export_CleanedReview.json\"\n",
    "reviewJsonWithClosedRes = \"..\\\\data\\\\Export_CleanedReviewWithClosedRes.json\"\n",
    "\n",
    "# with open(reviewJson, encoding = 'utf-8') as json_file:\n",
    "#     data = json_file.readlines()\n",
    "#     data = list(map(json.loads, data))\n",
    "\n",
    "#This appeared to be a neasted list, just use data[0] to read\n",
    "# df_review = pd.DataFrame(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Select top frenquent user and top frequenty restaurants that had at least 1 review >= 4 stars (Kickking out users that gave all  reviews <=3 and restaurants that never got start >= 4 stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yelp_df(path = 'data/', filename = 'Export_CleanedReview.json', sampling=False, top_user_num=6100, top_item_num=4000):\n",
    "    \"\"\"\n",
    "    Get the pandas dataframe\n",
    "    Sampling only the top users/items by density \n",
    "    Implicit representation applies\n",
    "    \"\"\"\n",
    "    with open(filename,'r') as f:\n",
    "        data = f.readlines()\n",
    "        data = list(map(json.loads, data))\n",
    "    \n",
    "    data = data[0]\n",
    "    #Get all the data from the data file\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df.rename(columns={'stars': 'review_stars', 'text': 'review_text', 'cool': 'review_cool',\n",
    "                       'funny': 'review_funny', 'useful': 'review_useful'},\n",
    "              inplace=True)\n",
    "\n",
    "    df['business_num_id'] = df.business_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.business_id.nunique()))\n",
    "    df['business_num_id'] = df['business_num_id'].astype('int')\n",
    "\n",
    "    df['user_num_id'] = df.user_id.astype('category').\\\n",
    "    cat.rename_categories(range(0, df.user_id.nunique()))\n",
    "    df['user_num_id'] = df['user_num_id'].astype('int')\n",
    "\n",
    "    df['timestamp'] = df['date'].apply(date_to_timestamp)\n",
    "\n",
    "    if sampling:\n",
    "        df = filter_yelp_df(df, top_user_num=top_user_num, top_item_num=top_item_num)\n",
    "        # Refresh num id\n",
    "        df['business_num_id'] = df.business_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.business_id.nunique()))\n",
    "        df['business_num_id'] = df['business_num_id'].astype('int')\n",
    "        \n",
    "        df['user_num_id'] = df.user_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.user_id.nunique()))\n",
    "        df['user_num_id'] = df['user_num_id'].astype('int')\n",
    "#     drop_list = ['date','review_id','review_funny','review_cool','review_useful']\n",
    "#     df = df.drop(drop_list, axis=1)\n",
    "\n",
    "    df = df.reset_index(drop = True)\n",
    "\n",
    "    return df \n",
    "\n",
    "\n",
    "def filter_yelp_df(df, top_user_num=6100, top_item_num=4000):\n",
    "    #Getting the reviews where starts are above 3\n",
    "    df_implicit = df[df['review_stars']>3]\n",
    "    frequent_user_id = df_implicit['user_num_id'].value_counts().head(top_user_num).index.values\n",
    "    frequent_item_id = df_implicit['business_num_id'].value_counts().head(top_item_num).index.values\n",
    "    return df.loc[(df['user_num_id'].isin(frequent_user_id)) & (df['business_num_id'].isin(frequent_item_id))]\n",
    "\n",
    "\n",
    "def date_to_timestamp(date):\n",
    "    dt = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    return time.mktime(dt.timetuple())\n",
    "\n",
    "def df_to_sparse(df, row_name='userId', col_name='movieId', value_name='rating',\n",
    "                 shape=(138494, 131263)):\n",
    "    rows = df[row_name]\n",
    "    cols = df[col_name]\n",
    "    if value_name is not None:\n",
    "        values = df[value_name]\n",
    "    else:\n",
    "        values = [1]*len(rows)\n",
    "\n",
    "    return csr_matrix((values, (rows, cols)), shape=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rating-UI and timestamp-UI matrix from original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_timestamp_matrix(df, sampling=False, top_user_num=6100, top_item_num=4000):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #make the df implicit with top frenquent users and \n",
    "    #no need to sample anymore if df was sampled before \n",
    "    if sampling:\n",
    "        df = filter_yelp_df(df, top_user_num=top_user_num, top_item_num=top_item_num)\n",
    "\n",
    "    rating_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "                                 col_name='business_num_id',\n",
    "                                 value_name='review_stars',\n",
    "                                 shape=None)\n",
    "    \n",
    "    #Have same dimension and data entries with rating_matrix, except that the review stars are - user avg\n",
    "    ratingWuserAvg_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "                                 col_name='business_num_id',\n",
    "                                 value_name='reviewStars_userAvg',\n",
    "                                 shape=None)\n",
    "    \n",
    "    timestamp_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "                                    col_name='business_num_id',\n",
    "                                    value_name='timestamp',\n",
    "                                    shape=None)\n",
    "\n",
    "    return rating_matrix, ratingWuserAvg_matrix, timestamp_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time ordered split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_ordered_split(rating_matrix, ratingWuserAvg_matrix, timestamp_matrix, ratio=[0.5, 0.2, 0.3],\n",
    "                       implicit=True, remove_empty=False, threshold=3,\n",
    "                       sampling=False, sampling_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Split the data to train,valid,test by time\n",
    "    ratio:  train:valid:test\n",
    "    threshold: for implicit representation\n",
    "    \"\"\"\n",
    "    if implicit:\n",
    "        temp_rating_matrix = sparse.csr_matrix(rating_matrix.shape)\n",
    "        temp_rating_matrix[(rating_matrix > threshold).nonzero()] = 1\n",
    "        rating_matrix = temp_rating_matrix\n",
    "        timestamp_matrix = timestamp_matrix.multiply(rating_matrix)\n",
    "        #ratingWuserAvg_matrix = ratingWuserAvg_matrix.multiply(rating_matrix)\n",
    "\n",
    "    nonzero_index = None\n",
    "\n",
    "    #Default false, not removing empty columns and rows\n",
    "    #Should not have this case, since users should have at least 1 record of 4,5 \n",
    "    #And restuarant should have at least 1 record of 4,5 \n",
    "    if remove_empty:\n",
    "        # Remove empty columns. record original item index\n",
    "        nonzero_index = np.unique(rating_matrix.nonzero()[1])\n",
    "        rating_matrix = rating_matrix[:, nonzero_index]\n",
    "        timestamp_matrix = timestamp_matrix[:, nonzero_index]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[:, nonzero_index]\n",
    "\n",
    "        # Remove empty rows. record original user index\n",
    "        nonzero_rows = np.unique(rating_matrix.nonzero()[0])\n",
    "        rating_matrix = rating_matrix[nonzero_rows]\n",
    "        timestamp_matrix = timestamp_matrix[nonzero_rows]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[nonzero_rows]\n",
    "\n",
    "    user_num, item_num = rating_matrix.shape\n",
    "\n",
    "    rtrain = []\n",
    "    rtrain_userAvg = []\n",
    "    rtime = []\n",
    "    rvalid = []\n",
    "    rvalid_userAvg = []\n",
    "    rtest = []\n",
    "    rtest_userAvg = []\n",
    "    # Get the index list corresponding to item for train,valid,test\n",
    "    item_idx_train = []\n",
    "    item_idx_valid = []\n",
    "    item_idx_test = []\n",
    "    \n",
    "    for i in tqdm(range(user_num)):\n",
    "        #Get the non_zero indexs, restuarants where the user visited/liked if implicit \n",
    "        item_indexes = rating_matrix[i].nonzero()[1]\n",
    "        \n",
    "        #Get the data for the user\n",
    "        data = rating_matrix[i].data\n",
    "        \n",
    "        #Get time stamp value \n",
    "        timestamp = timestamp_matrix[i].data\n",
    "        \n",
    "        #Get review stars with user avg data \n",
    "        if implicit == False:\n",
    "            dataWuserAvg = ratingWuserAvg_matrix[i].data\n",
    "        \n",
    "        #Non zero reviews for this user\n",
    "        num_nonzeros = len(item_indexes)\n",
    "        \n",
    "        #If the user has at least one review\n",
    "        if num_nonzeros >= 1:\n",
    "            #Get number of test and valid data \n",
    "            #train is 30%\n",
    "            num_test = int(num_nonzeros * ratio[2])\n",
    "            #validate is 50%\n",
    "            num_valid = int(num_nonzeros * (ratio[1] + ratio[2]))\n",
    "\n",
    "            valid_offset = num_nonzeros - num_valid\n",
    "            test_offset = num_nonzeros - num_test\n",
    "\n",
    "            #Sort the timestamp for each review for the user\n",
    "            argsort = np.argsort(timestamp)\n",
    "            \n",
    "            #Sort the reviews for the user according to the time stamp \n",
    "            data = data[argsort]\n",
    "            \n",
    "            #Sort the review with user avg accoridng to the time stamp\n",
    "            dataWuserAvg = dataWuserAvg[argsort]\n",
    "            \n",
    "            #Non-zero review index sort according to time\n",
    "            item_indexes = item_indexes[argsort]\n",
    "            \n",
    "            #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "            rtrain.append([data[:valid_offset], np.full(valid_offset, i), item_indexes[:valid_offset]])\n",
    "            \n",
    "            #Changing valid set to binary\n",
    "            count=valid_offset\n",
    "            for eachData in data[valid_offset:test_offset]:\n",
    "                #if rating-avgRating > 0 then like\n",
    "                if eachData >= 4:\n",
    "                    data[count] = 1\n",
    "                else:\n",
    "                    data[count] = 0\n",
    "                count += 1\n",
    "                \n",
    "            #50%-70%\n",
    "            rvalid.append([data[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                           item_indexes[valid_offset:test_offset]])\n",
    "            #remaining 30%\n",
    "            rtest.append([data[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "            \n",
    "            #Now for the rating matrix that considers user average rating\n",
    "            #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "            rtrain_userAvg.append([dataWuserAvg[:valid_offset], np.full(valid_offset, i), item_indexes[:valid_offset]])\n",
    "            #50%-70%\n",
    "            \n",
    "            #Changing valid set to binary\n",
    "            count=valid_offset\n",
    "            for eachData in dataWuserAvg[valid_offset:test_offset]:\n",
    "                #if rating-avgRating > 0 then like\n",
    "                if eachData > 0:\n",
    "                    dataWuserAvg[count] = 1\n",
    "                else:\n",
    "                    dataWuserAvg[count] = 0\n",
    "                count += 1\n",
    "                \n",
    "            rvalid_userAvg.append([dataWuserAvg[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                           item_indexes[valid_offset:test_offset]])\n",
    "            \n",
    "            #Change test set to binary even we don't use it\n",
    "            countTest = test_offset\n",
    "            for eachData in dataWuserAvg[test_offset:]:\n",
    "                #if rating-avgRating > 0 then like\n",
    "                if eachData > 0:\n",
    "                    dataWuserAvg[count] = 1\n",
    "                else:\n",
    "                    dataWuserAvg[count] = 0\n",
    "                count += 1\n",
    "            \n",
    "            \n",
    "            #remaining 30%\n",
    "            rtest_userAvg.append([dataWuserAvg[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "                \n",
    "            item_idx_train.append(item_indexes[:valid_offset])\n",
    "            \n",
    "#             item_idx_valid.append(item_indexes[valid_offset:test_offset])\n",
    "#             item_idx_test.append(item_indexes[test_offset:])\n",
    "        else:\n",
    "            item_idx_train.append([])\n",
    "#             item_idx_valid.append([])\n",
    "#             item_idx_test.append([])\n",
    "    \n",
    "    rtrain = np.array(rtrain)\n",
    "    rvalid = np.array(rvalid)\n",
    "    rtest = np.array(rtest)\n",
    "    rtrain_userAvg = np.array(rtrain_userAvg)\n",
    "    rvalid_userAvg = np.array(rvalid_userAvg)\n",
    "    rtest_userAvg = np.array(rtest_userAvg)\n",
    "    \n",
    "    #print(rtrain)\n",
    "    \n",
    "    \n",
    "    #take non-zeros values, row index, and column (non-zero) index and store into sparse matrix\n",
    "    rtrain = sparse.csr_matrix((np.hstack(rtrain[:, 0]), (np.hstack(rtrain[:, 1]), np.hstack(rtrain[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rvalid = sparse.csr_matrix((np.hstack(rvalid[:, 0]), (np.hstack(rvalid[:, 1]), np.hstack(rvalid[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rtest = sparse.csr_matrix((np.hstack(rtest[:, 0]), (np.hstack(rtest[:, 1]), np.hstack(rtest[:, 2]))),\n",
    "                              shape=rating_matrix.shape, dtype=np.float32)\n",
    "    \n",
    "    rtrain_userAvg = sparse.csr_matrix((np.hstack(rtrain_userAvg[:, 0]), (np.hstack(rtrain_userAvg[:, 1]), np.hstack(rtrain_userAvg[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rvalid_userAvg = sparse.csr_matrix((np.hstack(rvalid_userAvg[:, 0]), (np.hstack(rvalid_userAvg[:, 1]), np.hstack(rvalid_userAvg[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rtest_userAvg = sparse.csr_matrix((np.hstack(rtest_userAvg[:, 0]), (np.hstack(rtest_userAvg[:, 1]), np.hstack(rtest_userAvg[:, 2]))),\n",
    "                              shape=rating_matrix.shape, dtype=np.float32)\n",
    "\n",
    "\n",
    "    return rtrain, rvalid, rtest,rtrain_userAvg, rvalid_userAvg, rtest_userAvg, nonzero_index, timestamp_matrix, item_idx_train, item_idx_valid, item_idx_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get df for training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Item idex matrix stores the reivews starts\n",
    "#This function returns a list of index for the reviews included in training set \n",
    "def get_corpus_idx_list(df, item_idx_matrix):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    df: total dataframe\n",
    "    item_idx_matrix: train index list got from time_split \n",
    "    Output: row index in original dataframe for training data by time split\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    #For all the users: 5791\n",
    "    for i in tqdm(range(len(item_idx_matrix))):\n",
    "        \n",
    "        #find row index where user_num_id is i\n",
    "        a = df.index[df['user_num_id'] == i].tolist()\n",
    "        \n",
    "        #loop through the busienss id that the user i reviewed for in offvalid set \n",
    "        for item_idx in  item_idx_matrix[i]:\n",
    "            \n",
    "            #get the row index for reviews for business that the user liked in the train set\n",
    "            b = df.index[df['business_num_id'] == item_idx].tolist()\n",
    "            \n",
    "            #Find the index for which this user liked, one user only rate a business once\n",
    "            idx_to_add = list(set(a).intersection(b))\n",
    "            \n",
    "            if idx_to_add not in lst:\n",
    "                lst.extend(idx_to_add)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess using Term Frequency - CounterVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shenti10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shenti10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Stemming and Lemmatisation\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Get corpus and CountVector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "new_words = ['not_the']\n",
    "stop_words = stop_words.union(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#Should 'because' added?\n",
    "def preprocess(df, reset_list = [',','.','?',';','however','but']):\n",
    "    corpus = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        text = df['review_text'][i]\n",
    "        change_flg = 0\n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        ##Convert to list from string, loop through the review text\n",
    "        text = text.split()\n",
    "        \n",
    "        #any sentence that encounters a not, the folloing words will become not phrase until hit the sentence end\n",
    "        for j in range(len(text)):\n",
    "            #Make the not_ hack\n",
    "            if text[j] == 'not':\n",
    "                change_flg = 1\n",
    "#                 print 'changes is made after ', i\n",
    "                continue\n",
    "            #if was 1 was round and not hit a 'not' in this round\n",
    "            if change_flg == 1 and any(reset in text[j] for reset in reset_list):\n",
    "                text[j] = 'not_' + text[j]\n",
    "                change_flg = 0\n",
    "#                 print 'reset at ', i\n",
    "            if change_flg == 1:\n",
    "                text[j] = 'not_' + text[j]\n",
    "        \n",
    "        #Convert back to string\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "        #Remove punctuations\n",
    "#       text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        \n",
    "        #remove tags\n",
    "        text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "        \n",
    "        # remove special characters and digits\n",
    "        text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "        \n",
    "        ##Convert to list from string\n",
    "        text = text.split()\n",
    "        \n",
    "        ##Stemming\n",
    "        ps=PorterStemmer()\n",
    "        \n",
    "        #Lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        text = [lem.lemmatize(word) for word in text if not word in  \n",
    "                stop_words] \n",
    "        text = \" \".join(text)\n",
    "        corpus.append(text)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def train(matrix_train):\n",
    "    similarity = cosine_similarity(X=matrix_train, Y=None, dense_output=True)\n",
    "    return similarity\n",
    "\n",
    "def get_I_K(df, X, row_name = 'business_num_id', binary = True, shape = (121994,6000)):\n",
    "    \"\"\"\n",
    "    get the item-keyphrase matrix\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    cols = []\n",
    "    vals = []\n",
    "    #For each review history\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        #Get the array of frequencies for document/review i \n",
    "        arr = X[i].toarray() \n",
    "        nonzero_element = arr.nonzero()[1]  # Get nonzero element in each line, keyphrase that appears index \n",
    "        length_of_nonzero = len(nonzero_element) #number of important keyphrase that appears\n",
    "        \n",
    "        # df[row_name][i] is the item idex\n",
    "        #Get a list row index that indicates the document/review\n",
    "        rows.extend(np.array([df[row_name][i]]*length_of_nonzero)) ## Item index\n",
    "        #print(rows)\n",
    "        \n",
    "        #Get a list of column index indicating the key phrase that appears in i document/review\n",
    "        cols.extend(nonzero_element) ## Keyword Index\n",
    "        if binary:\n",
    "            #Create a bunch of 1s\n",
    "            vals.extend(np.array([1]*length_of_nonzero))\n",
    "        else:\n",
    "            #If not binary \n",
    "            vals.extend(arr[arr.nonzero()])    \n",
    "    return csr_matrix((vals, (rows, cols)), shape=shape)\n",
    "\n",
    "\n",
    "#Get a UI matrix if it's not item_similarity based or else IU\n",
    "def predict(matrix_train, k, similarity, item_similarity_en = False):\n",
    "    prediction_scores = []\n",
    "    \n",
    "    #inverse to IU matrix\n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    #for each user or item, depends UI or IU \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "        # Get user u's prediction scores for all items\n",
    "        #Get prediction/similarity score for each user 1*num or user or num of items\n",
    "        vector_u = similarity[user_index]\n",
    "\n",
    "        # Get closest K neighbors excluding user u self\n",
    "        #Decending accoding to similarity score, select top k\n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        \n",
    "        # Get neighbors similarity weights and ratings\n",
    "        similar_users_weights = similarity[user_index][similar_users]\n",
    "        \n",
    "        #similar_users_weights_sum = np.sum(similar_users_weights)\n",
    "        #print(similar_users_weights.shape)\n",
    "        #shape: num of res * k\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "        \n",
    "        \n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "        #print(prediction_scores_u)\n",
    "        \n",
    "        \"\"\"should divide by the sum of the weights if explicit\"\"\"\n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "        \n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    return res\n",
    "\n",
    "#Preidction score is UI or IU?\n",
    "def prediction(prediction_score, topK, matrix_Train):\n",
    "\n",
    "    prediction = []\n",
    "\n",
    "    #for each user\n",
    "    for user_index in tqdm(range(matrix_Train.shape[0])):\n",
    "        \n",
    "        #take the prediction scores for user 1 * num res\n",
    "        vector_u = prediction_score[user_index]\n",
    "        \n",
    "        #The restuarant the user rated\n",
    "        vector_train = matrix_Train[user_index]\n",
    "        \n",
    "        if len(vector_train.nonzero()[0]) > 0:\n",
    "            vector_predict = sub_routine(vector_u, vector_train, topK=topK)\n",
    "        else:\n",
    "            vector_predict = np.zeros(topK, dtype=np.float32)\n",
    "\n",
    "        prediction.append(vector_predict)\n",
    "\n",
    "    return np.vstack(prediction)\n",
    "\n",
    "#topK: the number of restuarants we are suggesting \n",
    "def sub_routine(vector_u, vector_train, topK=500):\n",
    "\n",
    "    #index where non-zero\n",
    "    train_index = vector_train.nonzero()[1]\n",
    "    \n",
    "    vector_u = vector_u\n",
    "    \n",
    "    #get topk + num rated res prediction score descending, top index \n",
    "    candidate_index = np.argpartition(-vector_u, topK+len(train_index))[:topK+len(train_index)]\n",
    "    \n",
    "    #sort top prediction score index in range topK+len(train_index) into vector_u`\n",
    "    vector_u = candidate_index[vector_u[candidate_index].argsort()[::-1]]\n",
    "    \n",
    "    #deleted the rated res from the topk+train_index prediction score vector for user u \n",
    "    #Delete the user rated res index from the topk+numRated index\n",
    "    vector_u = np.delete(vector_u, np.isin(vector_u, train_index).nonzero()[0])\n",
    "\n",
    "    #so we only include the top K prediction score here\n",
    "    return vector_u[:topK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recallk(vector_true_dense, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "\n",
    "def precisionk(vector_predict, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_predict)\n",
    "\n",
    "\n",
    "def average_precisionk(vector_predict, hits, **unused):\n",
    "    precisions = np.cumsum(hits, dtype=np.float32)/range(1, len(vector_predict)+1)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "def r_precision(vector_true_dense, vector_predict, **unused):\n",
    "    vector_predict_short = vector_predict[:len(vector_true_dense)]\n",
    "    hits = len(np.isin(vector_predict_short, vector_true_dense).nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "\n",
    "def _dcg_support(size):\n",
    "    arr = np.arange(1, size+1)+1\n",
    "    return 1./np.log2(arr)\n",
    "\n",
    "\n",
    "def ndcg(vector_true_dense, vector_predict, hits):\n",
    "    idcg = np.sum(_dcg_support(len(vector_true_dense)))\n",
    "    dcg_base = _dcg_support(len(vector_predict))\n",
    "    dcg_base[np.logical_not(hits)] = 0\n",
    "    dcg = np.sum(dcg_base)\n",
    "    return dcg/idcg\n",
    "\n",
    "\n",
    "def click(hits, **unused):\n",
    "    first_hit = next((i for i, x in enumerate(hits) if x), None)\n",
    "    if first_hit is None:\n",
    "        return 5\n",
    "    else:\n",
    "        return first_hit/10\n",
    "\n",
    "\n",
    "def evaluate(matrix_Predict, matrix_Test, metric_names =['R-Precision', 'NDCG', 'Precision', 'Recall', 'MAP'], atK = [5, 10, 15, 20, 50], analytical=False):\n",
    "    \"\"\"\n",
    "    :param matrix_U: Latent representations of users, for LRecs it is RQ, for ALSs it is U\n",
    "    :param matrix_V: Latent representations of items, for LRecs it is Q, for ALSs it is V\n",
    "    :param matrix_Train: Rating matrix for training, features.\n",
    "    :param matrix_Test: Rating matrix for evaluation, true labels.\n",
    "    :param k: Top K retrieval\n",
    "    :param metric_names: Evaluation metrics\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global_metrics = {\n",
    "        #\"R-Precision\": r_precision,\n",
    "        #\"NDCG\": ndcg,\n",
    "        #\"Clicks\": click\n",
    "    }\n",
    "\n",
    "    local_metrics = {\n",
    "        \"Precision\": precisionk,\n",
    "        \"Recall\": recallk,\n",
    "        \"MAP\": average_precisionk\n",
    "    }\n",
    "\n",
    "    output = dict()\n",
    "\n",
    "    num_users = matrix_Predict.shape[0]\n",
    "\n",
    "    for k in atK:\n",
    "\n",
    "        local_metric_names = list(set(metric_names).intersection(local_metrics.keys()))\n",
    "        results = {name: [] for name in local_metric_names}\n",
    "        topK_Predict = matrix_Predict[:, :k]\n",
    "\n",
    "        for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "            vector_predict = topK_Predict[user_index]\n",
    "            if len(vector_predict.nonzero()[0]) > 0:\n",
    "                vector_true = matrix_Test[user_index]\n",
    "                vector_true_dense = vector_true.nonzero()[1]\n",
    "                hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "                if vector_true_dense.size > 0:\n",
    "                    for name in local_metric_names:\n",
    "                        results[name].append(local_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                                 vector_predict=vector_predict,\n",
    "                                                                 hits=hits))\n",
    "\n",
    "        results_summary = dict()\n",
    "        if analytical:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = round(results[name],4)\n",
    "        else:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = (round((np.average(results[name])),4),\n",
    "                                                              round((1.96*np.std(results[name])/np.sqrt(num_users)),4))\n",
    "        output.update(results_summary)\n",
    "\n",
    "    global_metric_names = list(set(metric_names).intersection(global_metrics.keys()))\n",
    "    results = {name: [] for name in global_metric_names}\n",
    "\n",
    "    topK_Predict = matrix_Predict[:]\n",
    "\n",
    "    for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "        vector_predict = topK_Predict[user_index]\n",
    "\n",
    "        if len(vector_predict.nonzero()[0]) > 0:\n",
    "            vector_true = matrix_Test[user_index]\n",
    "            vector_true_dense = vector_true.nonzero()[1]\n",
    "            hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "            # if user_index == 1:\n",
    "            #     import ipdb;\n",
    "            #     ipdb.set_trace()\n",
    "\n",
    "            if vector_true_dense.size > 0:\n",
    "                for name in global_metric_names:\n",
    "                    results[name].append(global_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                              vector_predict=vector_predict,\n",
    "                                                              hits=hits))\n",
    "\n",
    "    results_summary = dict()\n",
    "    if analytical:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = round(results[name],4)\n",
    "    else:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = (round(np.average(results[name]),4), round((1.96*np.std(results[name])/np.sqrt(num_users)),4))\n",
    "    output.update(results_summary)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add 2 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility functions\n",
    "#3906 restuarant, 3000 keyphrase, 5791 user \n",
    "def add_two_matrix(ratio, U_I_matrix,I_K_matrix, shape = (3906, 3000+5791)):\n",
    "    # ratio determine Keywords/User in the matrix\n",
    "    rows = []\n",
    "    cols = []\n",
    "    datas = []\n",
    "    I_U_matrix = U_I_matrix.transpose()\n",
    "    \n",
    "    #for each restuarant\n",
    "    for i in tqdm(range(I_K_matrix.shape[0])):\n",
    "        #key phrase that this item has, column(key phrase) index\n",
    "        nonzero1 = I_K_matrix[i].nonzero()\n",
    "        \n",
    "        #user that rated this item, column(user) index \n",
    "        nonzero2 = I_U_matrix[i].nonzero()\n",
    "        \n",
    "        #Trying to create a sparse matrix that stores \n",
    "        #index of restuarant for (K + U) times\n",
    "        row = [i]*(len(nonzero1[1])+len(nonzero2[1]))\n",
    "        \n",
    "        #column index for key phrase and users that are non-zero\n",
    "        col = nonzero1[1].tolist()+ nonzero2[1].tolist()\n",
    "        \n",
    "        \n",
    "        data = [ratio]*len(nonzero1[1])+[1-ratio]*len(nonzero2[1]) # Binary representation of I-K/U matrix\n",
    "        \n",
    "        rows.extend(row)\n",
    "        cols.extend(col)\n",
    "        datas.extend(data)\n",
    "    return csr_matrix( (datas,(rows,cols)), shape=shape )\n",
    "\n",
    "def transfer_to_implicit(rating_matrix, threshold = 0):\n",
    "    temp_rating_matrix = sparse.csr_matrix(rating_matrix.shape)\n",
    "    temp_rating_matrix[(rating_matrix > threshold).nonzero()] = 1\n",
    "    rating_matrix = temp_rating_matrix\n",
    "    return rating_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get original dataframe out of the review datastet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the yelp data -> reviews, only get the data for the top frequent users & restuarants that had a history of rating > 3\n",
    "df = get_yelp_df(path ='', filename=reviewJsonWithClosedRes, sampling= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviewStars_userAvg'] = df['review_stars'] - df['user_avg_stars']\n",
    "df.loc[df['reviewStars_userAvg'] == 0, \"reviewStars_userAvg\"] = 0.01\n",
    "#(df['reviewStars_userAvg'] == 0.01).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1074"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(df['reviewStars_userAvg'] == 0.01).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get rating-UI matrix and timestepm-UI matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix, ratingWuserAvg_matrix, timestamp_matrix = get_rating_timestamp_matrix(df)\n",
    "# rating_matrix\n",
    "# ratingWuserAvg_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split to get rtrain-UI matrix and valid and test.. item_index_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6085/6085 [00:01<00:00, 3667.01it/s]\n"
     ]
    }
   ],
   "source": [
    "rtrain, rvalid, rtest, rtrain_userAvg, rvalid_userAvg, rtest_userAvg, nonzero_index, rtime, item_idx_matrix_train,item_idx_matrix_valid, item_idx_matrix_test = time_ordered_split(rating_matrix=rating_matrix, ratingWuserAvg_matrix=ratingWuserAvg_matrix, timestamp_matrix=timestamp_matrix,\n",
    "                                                                     ratio=[0.5,0.2,0.3],\n",
    "                                                                     implicit=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get df shrink to df_train for rtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6087/6087 [02:25<00:00, 41.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(125879, 14)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the list of row index for the training set \n",
    "lst_train = get_corpus_idx_list(df, item_idx_matrix_train)\n",
    "\n",
    "# Get the training dataframe from the original dataframe\n",
    "df_train = df.loc[lst_train]\n",
    "\n",
    "#Resetting the index of the train data\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If using term frequency only to compute corpus and X(review vs. terms) CounterVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire corpus\n",
    "#corpus = preprocess(df_train)\n",
    "# X row: df_train row, column: key words frequency \n",
    "# When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n",
    "#cv=CountVectorizer(max_df=0.9,stop_words=stop_words, max_features=5000, ngram_range=(1,1))\n",
    "#X=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If using TD-IDF to compute corpus and X (business vs. terms) TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 125794/125794 [00:47<00:00, 2662.22it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = preprocess(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary to store business: review text\n",
    "dict_text = {}\n",
    "for i in range(len(corpus)):\n",
    "    if df_train['business_num_id'][i] not in dict_text:\n",
    "        dict_text[df_train['business_num_id'][i]] = corpus[i]\n",
    "    else:\n",
    "        temp = dict_text[df_train['business_num_id'][i]]\n",
    "        temp = temp + corpus[i]\n",
    "        dict_text[df_train['business_num_id'][i]] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list for the review text, where the row dimension = total business ids\n",
    "list_text = []\n",
    "for key in range(0,max(list(dict_text.keys()))+1) :\n",
    "    if key not in dict_text.keys():\n",
    "        list_text.extend([\"\"])\n",
    "    else:\n",
    "        list_text.extend([dict_text[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the X vector, where dimension is #business vs #terms\n",
    "vectorizer = TfidfVectorizer(max_df=0.9,stop_words=stop_words, max_features=5000, ngram_range=(1,1))\n",
    "X_cleaned = vectorizer.fit_transform(list_text).toarray()\n",
    "X_cleaned_sparse = csr_matrix(X_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-rating KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. With ratings that subtracts user average rating, cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6085/6085 [00:08<00:00, 706.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6085/6085 [00:01<00:00, 4358.25it/s]\n"
     ]
    }
   ],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "similarity1 = train(rtrain)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "user_item_prediction_score1 = predict(rtrain, 90, similarity1, item_similarity_en= False)\n",
    "user_item_predict1 = prediction(user_item_prediction_score1, 50, rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6085/6085 [00:00<00:00, 6638.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6085/6085 [00:00<00:00, 6719.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6085/6085 [00:00<00:00, 6779.31it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6085/6085 [00:00<00:00, 6660.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6085/6085 [00:00<00:00, 6726.79it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6085/6085 [00:00<00:00, 8917.52it/s]\n"
     ]
    }
   ],
   "source": [
    "user_item_res1 = evaluate(user_item_predict1, rvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Recall@5': (0.0546, 0.0035),\n",
       " 'MAP@5': (0.0583, 0.0035),\n",
       " 'Precision@5': (0.0534, 0.0027),\n",
       " 'Recall@10': (0.0963, 0.0046),\n",
       " 'MAP@10': (0.0539, 0.0026),\n",
       " 'Precision@10': (0.0474, 0.0019),\n",
       " 'Recall@15': (0.1297, 0.0053),\n",
       " 'MAP@15': (0.0508, 0.0022),\n",
       " 'Precision@15': (0.0434, 0.0015),\n",
       " 'Recall@20': (0.1577, 0.0058),\n",
       " 'MAP@20': (0.0485, 0.0019),\n",
       " 'Precision@20': (0.0399, 0.0013),\n",
       " 'Recall@50': (0.2938, 0.0076),\n",
       " 'MAP@50': (0.0401, 0.0012),\n",
       " 'Precision@50': (0.0307, 0.0008)}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item_res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. With ratings that subtracts user average rating, pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6087/6087 [00:02<00:00, 2850.04it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6087/6087 [00:01<00:00, 3991.41it/s]\n"
     ]
    }
   ],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "similarity2 = train(rtrain_userAvg)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "user_item_prediction_score2 = predict(rtrain_userAvg, 5, similarity2, item_similarity_en= False)\n",
    "user_item_predict2 = prediction(user_item_prediction_score2, 50, rtrain_userAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6087/6087 [00:01<00:00, 5917.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6087/6087 [00:01<00:00, 5891.35it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6087/6087 [00:01<00:00, 6045.66it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6087/6087 [00:01<00:00, 6011.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6087/6087 [00:01<00:00, 5997.56it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6087/6087 [00:00<00:00, 7671.14it/s]\n"
     ]
    }
   ],
   "source": [
    "user_item_res2 = evaluate(user_item_predict2, rvalid_userAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP@5': (0.024, 0.0023),\n",
       " 'Recall@5': (0.0268, 0.0027),\n",
       " 'Precision@5': (0.0215, 0.0017),\n",
       " 'MAP@10': (0.022, 0.0017),\n",
       " 'Recall@10': (0.0468, 0.0036),\n",
       " 'Precision@10': (0.019, 0.0012),\n",
       " 'MAP@15': (0.0208, 0.0014),\n",
       " 'Recall@15': (0.0637, 0.0041),\n",
       " 'Precision@15': (0.0179, 0.001),\n",
       " 'MAP@20': (0.0199, 0.0012),\n",
       " 'Recall@20': (0.0777, 0.0044),\n",
       " 'Precision@20': (0.0168, 0.0008),\n",
       " 'MAP@50': (0.0161, 0.0008),\n",
       " 'Recall@50': (0.1181, 0.0051),\n",
       " 'Precision@50': (0.0112, 0.0004)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item_res2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. With raw ratings, cosinesimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6088/6088 [00:01<00:00, 3162.98it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6088/6088 [00:01<00:00, 4485.06it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6088/6088 [00:00<00:00, 6445.65it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6088/6088 [00:00<00:00, 6693.53it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6088/6088 [00:00<00:00, 6656.77it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6088/6088 [00:00<00:00, 6752.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6088/6088 [00:00<00:00, 6620.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6088/6088 [00:01<00:00, 6073.71it/s]\n"
     ]
    }
   ],
   "source": [
    "#UU similarity\n",
    "similarity3 = train(rtrain)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "user_item_prediction_score3 = predict(rtrain, 10, similarity3, item_similarity_en= False)\n",
    "user_item_predict3 = prediction(user_item_prediction_score3, 50, rtrain)\n",
    "#Check user item prediction score\n",
    "user_item_res3 = evaluate(user_item_predict3, rvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP@5': (0.05883270191621369, 0.003517370334190897),\n",
       " 'Recall@5': (0.042632702401899814, 0.0028266920440702154),\n",
       " 'Precision@5': (0.054389721627408995, 0.0027142638458765704),\n",
       " 'MAP@10': (0.0548657290841445, 0.0026588385851928705),\n",
       " 'Recall@10': (0.07643605521575313, 0.0038849508231148296),\n",
       " 'Precision@10': (0.04916817657717015, 0.001978290890612154),\n",
       " 'MAP@15': (0.052140155030947315, 0.0022436203793396023),\n",
       " 'Recall@15': (0.10326745787861508, 0.0044265401445831905),\n",
       " 'Precision@15': (0.044934936583758855, 0.001605212328363105),\n",
       " 'MAP@20': (0.04992850089845435, 0.001983997240030478),\n",
       " 'Recall@20': (0.1279055916007848, 0.004838089463166916),\n",
       " 'Precision@20': (0.042241805303903805, 0.0013838452831107387),\n",
       " 'MAP@50': (0.041935725762608074, 0.0013297313645239325),\n",
       " 'Recall@50': (0.23258012080259813, 0.0062362778163048286),\n",
       " 'Precision@50': (0.03246252676659529, 0.0008858237147598104),\n",
       " 'R-Precision': (0.04934540882028921, 0.00239628350301933),\n",
       " 'NDCG': (0.12601380812534357, 0.003491651689476126)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item_res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rtrain_userAvg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Base KNN using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "IK_MATRIX = X_cleaned_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:05<00:00, 760.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6088/6088 [00:01<00:00, 3433.24it/s]\n"
     ]
    }
   ],
   "source": [
    "I_I_similarity = train(IK_MATRIX)\n",
    "item_based_prediction_score4 = predict(rtrain, 10, I_I_similarity, item_similarity_en= True)\n",
    "#for each restuarant top50 users \n",
    "item_based_predict4 = prediction(item_based_prediction_score4, 50, rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6088/6088 [00:00<00:00, 6455.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6088/6088 [00:00<00:00, 6746.07it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6088/6088 [00:00<00:00, 6796.31it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6088/6088 [00:00<00:00, 6843.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6088/6088 [00:00<00:00, 6664.89it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6088/6088 [00:00<00:00, 6098.03it/s]\n"
     ]
    }
   ],
   "source": [
    "item_based_res_TFIDF = evaluate(item_based_predict4, rvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP@5': (0.025720090045571842, 0.0023778194577824324),\n",
       " 'Recall@5': (0.016655483869688705, 0.0017397463525098502),\n",
       " 'Precision@5': (0.022467468291879424, 0.0017674622991041998),\n",
       " 'MAP@10': (0.022609687481207827, 0.0017360525467602446),\n",
       " 'Recall@10': (0.027194281753721176, 0.002222645640702759),\n",
       " 'Precision@10': (0.018283643551309504, 0.0011753292576658),\n",
       " 'MAP@15': (0.020781676396896296, 0.00142907310519044),\n",
       " 'Recall@15': (0.036517627228885796, 0.0025924958256981607),\n",
       " 'Precision@15': (0.0165266567836161, 0.0009373298279886963),\n",
       " 'MAP@20': (0.019509841866004404, 0.0012405775324402586),\n",
       " 'Recall@20': (0.044301527297699116, 0.002866338517957658),\n",
       " 'Precision@20': (0.015112831493987811, 0.0007917345810550951),\n",
       " 'MAP@50': (0.015555777760667203, 0.0007797415508435789),\n",
       " 'Recall@50': (0.08003652685068291, 0.003781329780486565),\n",
       " 'Precision@50': (0.011391862955032118, 0.0004649805491420753),\n",
       " 'R-Precision': (0.019076629311042183, 0.0015214914170442837),\n",
       " 'NDCG': (0.04568113642606309, 0.0021907588988182157)}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_based_res_TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
