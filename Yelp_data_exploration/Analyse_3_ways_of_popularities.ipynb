{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Downloading https://files.pythonhosted.org/packages/80/93/d384479da0ead712bdaf697a8399c13a9a89bd856ada5a27d462fb45e47b/geopy-1.20.0-py2.py3-none-any.whl (100kB)\n",
      "Collecting geographiclib<2,>=1.49 (from geopy)\n",
      "  Downloading https://files.pythonhosted.org/packages/8b/62/26ec95a98ba64299163199e95ad1b0e34ad3f4e176e221c40245f211e425/geographiclib-1.50-py3-none-any.whl\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-1.50 geopy-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.sparse import csr_matrix, load_npz, save_npz\n",
    "from tqdm import tqdm\n",
    "import statistics as stats\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import yaml\n",
    "import scipy.sparse as sparse\n",
    "import yaml\n",
    "from tkinter import *\n",
    "import tkinter\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy import distance\n",
    "from geopy import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewJson = \"..\\\\data\\\\Export_CleanedReview.json\"\n",
    "reviewJsonWithClosedRes = \"..\\\\data\\\\Export_CleanedReviewWithClosedRes.json\"\n",
    "reviewJsonToronto = \"..\\\\data\\\\Export_TorontoData.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Select top frenquent user and top frequenty restaurants that had at least 1 review >= 4 stars (Kickking out users that gave all  reviews <=3 and restaurants that never got start >= 4 stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yelp_df(path = 'data/', filename = 'Export_CleanedReview.json', sampling=False, top_user_num=6100, top_item_num=4000):\n",
    "    \"\"\"\n",
    "    Get the pandas dataframe\n",
    "    Sampling only the top users/items by density \n",
    "    Implicit representation applies\n",
    "    \"\"\"\n",
    "    with open(filename,'r') as f:\n",
    "        data = f.readlines()\n",
    "        data = list(map(json.loads, data))\n",
    "    \n",
    "    data = data[0]\n",
    "    #Get all the data from the dggeata file\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df.rename(columns={'stars': 'review_stars', 'text': 'review_text', 'cool': 'review_cool',\n",
    "                       'funny': 'review_funny', 'useful': 'review_useful'},\n",
    "              inplace=True)\n",
    "\n",
    "    df['business_num_id'] = df.business_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.business_id.nunique()))\n",
    "    df['business_num_id'] = df['business_num_id'].astype('int')\n",
    "\n",
    "    df['user_num_id'] = df.user_id.astype('category').\\\n",
    "    cat.rename_categories(range(0, df.user_id.nunique()))\n",
    "    df['user_num_id'] = df['user_num_id'].astype('int')\n",
    "\n",
    "    df['timestamp'] = df['date'].apply(date_to_timestamp)\n",
    "\n",
    "    if sampling:\n",
    "        df = filter_yelp_df(df, top_user_num=top_user_num, top_item_num=top_item_num)\n",
    "        # Refresh num id\n",
    "        df['business_num_id'] = df.business_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.business_id.nunique()))\n",
    "        df['business_num_id'] = df['business_num_id'].astype('int')\n",
    "        \n",
    "        df['user_num_id'] = df.user_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.user_id.nunique()))\n",
    "        df['user_num_id'] = df['user_num_id'].astype('int')\n",
    "#     drop_list = ['date','review_id','review_funny','review_cool','review_useful']\n",
    "#     df = df.drop(drop_list, axis=1)\n",
    "\n",
    "    df = df.reset_index(drop = True)\n",
    "\n",
    "    return df \n",
    "\n",
    "def filter_yelp_df(df, top_user_num=6100, top_item_num=4000):\n",
    "    #Getting the reviews where starts are above 3\n",
    "    df_implicit = df[df['review_stars']>3]\n",
    "    frequent_user_id = df_implicit['user_num_id'].value_counts().head(top_user_num).index.values\n",
    "    frequent_item_id = df_implicit['business_num_id'].value_counts().head(top_item_num).index.values\n",
    "    return df.loc[(df['user_num_id'].isin(frequent_user_id)) & (df['business_num_id'].isin(frequent_item_id))]\n",
    "\n",
    "def date_to_timestamp(date):\n",
    "    dt = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    return time.mktime(dt.timetuple())\n",
    "\n",
    "def df_to_sparse(df, row_name='userId', col_name='movieId', value_name='rating',\n",
    "                 shape=None):\n",
    "    rows = df[row_name]\n",
    "    cols = df[col_name]\n",
    "    if value_name is not None:\n",
    "        values = df[value_name]\n",
    "    else:\n",
    "        values = [1]*len(rows)\n",
    "\n",
    "    return csr_matrix((values, (rows, cols)), shape=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rating-UI and timestamp-UI matrix from original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_timestamp_matrix(df, sampling=False, top_user_num=6100, top_item_num=4000):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #make the df implicit with top frenquent users and \n",
    "    #no need to sample anymore if df was sampled before \n",
    "    if sampling:\n",
    "        df = filter_yelp_df(df, top_user_num=top_user_num, top_item_num=top_item_num)\n",
    "\n",
    "    rating_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "                                 col_name='business_num_id',\n",
    "                                 value_name='review_stars',\n",
    "                                 shape=None)\n",
    "    \n",
    "    #Have same dimension and data entries with rating_matrix, except that the review stars are - user avg\n",
    "#     ratingWuserAvg_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "#                                  col_name='business_num_id',\n",
    "#                                  value_name='reviewStars_userAvg',\n",
    "#                                  shape=None)\n",
    "    \n",
    "    timestamp_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "                                    col_name='business_num_id',\n",
    "                                    value_name='timestamp',\n",
    "                                    shape=None)\n",
    "    \n",
    "    \n",
    "    IC_matrix = get_I_C(df)\n",
    "#     ratingWuserAvg_matrix\n",
    "    return rating_matrix, timestamp_matrix, IC_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_I_C(df):\n",
    "    lst = df.categories.values.tolist()\n",
    "    cat = []\n",
    "    for i in range(len(lst)):\n",
    "        cat.extend(lst[i].split(', '))\n",
    "    unique_cat = set(cat)\n",
    "    #     set categories id\n",
    "    df_cat = pd.DataFrame(list(unique_cat),columns=[\"Categories\"])\n",
    "    df_cat['cat_id'] = df_cat.Categories.astype('category').cat.rename_categories(range(0, df_cat.Categories.nunique()))\n",
    "    dict_cat = df_cat.set_index('Categories')['cat_id'].to_dict()\n",
    "    \n",
    "    df_I_C = pd.DataFrame(columns=['business_num_id', 'cat_id'])\n",
    "    \n",
    "    for i in range((df['business_num_id'].unique().shape)[0]):\n",
    "        df_temp = df[df['business_num_id'] == i].iloc[:1]\n",
    "        temp_lst = df_temp['categories'].to_list()[0].split(\",\")\n",
    "        for j in range(len(temp_lst)):\n",
    "            df_I_C = df_I_C.append({'business_num_id' : i  , 'cat_id' : dict_cat[temp_lst[j].strip()]} , ignore_index=True)\n",
    "    \n",
    "    IC_Matrix = df_to_sparse(df_I_C, row_name='business_num_id',\n",
    "                                 col_name='cat_id',\n",
    "                                 value_name=None,\n",
    "                                 shape=None)    \n",
    "    return IC_Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time ordered split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_ordered_split(rating_matrix, ratingWuserAvg_matrix, timestamp_matrix, ratio=[0.5, 0.2, 0.3],\n",
    "                       implicit=True, remove_empty=False, threshold=3,\n",
    "                       sampling=False, sampling_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Split the data to train,valid,test by time\n",
    "    ratio:  train:valid:test\n",
    "    threshold: for implicit representation\n",
    "    \"\"\"\n",
    "    if implicit:\n",
    "        temp_rating_matrix = sparse.csr_matrix(rating_matrix.shape)\n",
    "        temp_rating_matrix[(rating_matrix > threshold).nonzero()] = 1\n",
    "        rating_matrix = temp_rating_matrix\n",
    "        timestamp_matrix = timestamp_matrix.multiply(rating_matrix)\n",
    "        #ratingWuserAvg_matrix = ratingWuserAvg_matrix.multiply(rating_matrix)\n",
    "\n",
    "    nonzero_index = None\n",
    "\n",
    "    #Default false, not removing empty columns and rows\n",
    "    #Should not have this case, since users should have at least 1 record of 4,5 \n",
    "    #And restuarant should have at least 1 record of 4,5 \n",
    "    if remove_empty:\n",
    "        # Remove empty columns. record original item index\n",
    "        nonzero_index = np.unique(rating_matrix.nonzero()[1])\n",
    "        rating_matrix = rating_matrix[:, nonzero_index]\n",
    "        timestamp_matrix = timestamp_matrix[:, nonzero_index]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[:, nonzero_index]\n",
    "\n",
    "        # Remove empty rows. record original user index\n",
    "        nonzero_rows = np.unique(rating_matrix.nonzero()[0])\n",
    "        rating_matrix = rating_matrix[nonzero_rows]\n",
    "        timestamp_matrix = timestamp_matrix[nonzero_rows]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[nonzero_rows]\n",
    "\n",
    "    user_num, item_num = rating_matrix.shape\n",
    "\n",
    "    rtrain = []\n",
    "    rtrain_userAvg = []\n",
    "    rtime = []\n",
    "    rvalid = []\n",
    "    rvalid_userAvg = []\n",
    "    rtest = []\n",
    "    rtest_userAvg = []\n",
    "    # Get the index list corresponding to item for train,valid,test\n",
    "    item_idx_train = []\n",
    "    item_idx_valid = []\n",
    "    item_idx_test = []\n",
    "    \n",
    "    for i in tqdm(range(user_num)):\n",
    "        #Get the non_zero indexs, restuarants where the user visited/liked if implicit \n",
    "        item_indexes = rating_matrix[i].nonzero()[1]\n",
    "        \n",
    "        #Get the data for the user\n",
    "        data = rating_matrix[i].data\n",
    "        \n",
    "        #Get time stamp value \n",
    "        timestamp = timestamp_matrix[i].data\n",
    "        \n",
    "        #Get review stars with user avg data \n",
    "        if implicit == False:\n",
    "            dataWuserAvg = ratingWuserAvg_matrix[i].data\n",
    "        \n",
    "        #Non zero reviews for this user\n",
    "        num_nonzeros = len(item_indexes)\n",
    "        \n",
    "        #If the user has at least one review\n",
    "        if num_nonzeros >= 1:\n",
    "            #Get number of test and valid data \n",
    "            #train is 30%\n",
    "            num_test = int(num_nonzeros * ratio[2])\n",
    "            #validate is 50%\n",
    "            num_valid = int(num_nonzeros * (ratio[1] + ratio[2]))\n",
    "\n",
    "            valid_offset = num_nonzeros - num_valid\n",
    "            test_offset = num_nonzeros - num_test\n",
    "\n",
    "            #Sort the timestamp for each review for the user\n",
    "            argsort = np.argsort(timestamp)\n",
    "            \n",
    "            #Sort the reviews for the user according to the time stamp \n",
    "            data = data[argsort]\n",
    "            \n",
    "            #Sort the review with user avg accoridng to the time stamp\n",
    "            if implicit == False:\n",
    "                dataWuserAvg = dataWuserAvg[argsort]\n",
    "            \n",
    "            #Non-zero review index sort according to time\n",
    "            item_indexes = item_indexes[argsort]\n",
    "            \n",
    "            #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "            rtrain.append([data[:valid_offset], np.full(valid_offset, i), item_indexes[:valid_offset]])\n",
    "            \n",
    "            if implicit == False:\n",
    "                #Changing valid set to binary\n",
    "                count=valid_offset\n",
    "                for eachData in data[valid_offset:test_offset]:\n",
    "                    #if rating-avgRating > 0 then like\n",
    "                    if eachData >= 4:\n",
    "                        data[count] = 1\n",
    "                    else:\n",
    "                        data[count] = 0\n",
    "                    count += 1\n",
    "                \n",
    "            #50%-70%\n",
    "            rvalid.append([data[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                           item_indexes[valid_offset:test_offset]])\n",
    "            #remaining 30%\n",
    "            rtest.append([data[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "            \n",
    "            if implicit == False:\n",
    "                #Now for the rating matrix that considers user average rating\n",
    "                #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "                rtrain_userAvg.append([dataWuserAvg[:valid_offset], np.full(valid_offset, i), item_indexes[:valid_offset]])\n",
    "                #50%-70%\n",
    "\n",
    "                #Changing valid set to binary\n",
    "                count=valid_offset\n",
    "                for eachData in dataWuserAvg[valid_offset:test_offset]:\n",
    "                    #if rating-avgRating > 0 then like\n",
    "                    if eachData > 0:\n",
    "                        dataWuserAvg[count] = 1\n",
    "                    else:\n",
    "                        dataWuserAvg[count] = 0\n",
    "                    count += 1\n",
    "\n",
    "                rvalid_userAvg.append([dataWuserAvg[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                               item_indexes[valid_offset:test_offset]])\n",
    "\n",
    "                #Change test set to binary even we don't use it\n",
    "                countTest = test_offset\n",
    "                for eachData in dataWuserAvg[test_offset:]:\n",
    "                    #if rating-avgRating > 0 then like\n",
    "                    if eachData > 0:\n",
    "                        dataWuserAvg[count] = 1\n",
    "                    else:\n",
    "                        dataWuserAvg[count] = 0\n",
    "                    count += 1\n",
    "\n",
    "\n",
    "                #remaining 30%\n",
    "                rtest_userAvg.append([dataWuserAvg[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "                \n",
    "            item_idx_train.append(item_indexes[:valid_offset])\n",
    "            \n",
    "#             item_idx_valid.append(item_indexes[valid_offset:test_offset])\n",
    "#             item_idx_test.append(item_indexes[test_offset:])\n",
    "        else:\n",
    "            item_idx_train.append([])\n",
    "#             item_idx_valid.append([])\n",
    "#             item_idx_test.append([])\n",
    "    \n",
    "    rtrain = np.array(rtrain)\n",
    "    rvalid = np.array(rvalid)\n",
    "    rtest = np.array(rtest)\n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = np.array(rtrain_userAvg)\n",
    "        rvalid_userAvg = np.array(rvalid_userAvg)\n",
    "        rtest_userAvg = np.array(rtest_userAvg)\n",
    "\n",
    "    #take non-zeros values, row index, and column (non-zero) index and store into sparse matrix\n",
    "    rtrain = sparse.csr_matrix((np.hstack(rtrain[:, 0]), (np.hstack(rtrain[:, 1]), np.hstack(rtrain[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rvalid = sparse.csr_matrix((np.hstack(rvalid[:, 0]), (np.hstack(rvalid[:, 1]), np.hstack(rvalid[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rtest = sparse.csr_matrix((np.hstack(rtest[:, 0]), (np.hstack(rtest[:, 1]), np.hstack(rtest[:, 2]))),\n",
    "                              shape=rating_matrix.shape, dtype=np.float32)\n",
    "    \n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = sparse.csr_matrix((np.hstack(rtrain_userAvg[:, 0]), (np.hstack(rtrain_userAvg[:, 1]), np.hstack(rtrain_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rvalid_userAvg = sparse.csr_matrix((np.hstack(rvalid_userAvg[:, 0]), (np.hstack(rvalid_userAvg[:, 1]), np.hstack(rvalid_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rtest_userAvg = sparse.csr_matrix((np.hstack(rtest_userAvg[:, 0]), (np.hstack(rtest_userAvg[:, 1]), np.hstack(rtest_userAvg[:, 2]))),\n",
    "                                  shape=rating_matrix.shape, dtype=np.float32)\n",
    "\n",
    "    return rtrain, rvalid, rtest,rtrain_userAvg, rvalid_userAvg, rtest_userAvg, nonzero_index, timestamp_matrix, item_idx_train, item_idx_valid, item_idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_ordered_splitModified(rating_matrix, ratingWuserAvg_matrix, timestamp_matrix, ratio=[0.5, 0.2, 0.3],\n",
    "                       implicit=True, remove_empty=False, threshold=3,\n",
    "                       sampling=False, sampling_ratio=0.1, trainSampling=1):\n",
    "    \"\"\"\n",
    "    Split the data to train,valid,test by time\n",
    "    ratio:  train:valid:test\n",
    "    threshold: for implicit representation\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if implicit:\n",
    "        temp_rating_matrix = sparse.csr_matrix(rating_matrix.shape)\n",
    "        temp_rating_matrix[(rating_matrix > threshold).nonzero()] = 1\n",
    "        rating_matrix = temp_rating_matrix\n",
    "        timestamp_matrix = timestamp_matrix.multiply(rating_matrix)\n",
    "        #ratingWuserAvg_matrix = ratingWuserAvg_matrix.multiply(rating_matrix)\n",
    "\n",
    "    nonzero_index = None\n",
    "\n",
    "    #Default false, not removing empty columns and rows\n",
    "    #Should not have this case, since users should have at least 1 record of 4,5 \n",
    "    #And restuarant should have at least 1 record of 4,5 \n",
    "    if remove_empty:\n",
    "        # Remove empty columns. record original item index\n",
    "        nonzero_index = np.unique(rating_matrix.nonzero()[1])\n",
    "        rating_matrix = rating_matrix[:, nonzero_index]\n",
    "        timestamp_matrix = timestamp_matrix[:, nonzero_index]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[:, nonzero_index]\n",
    "\n",
    "        # Remove empty rows. record original user index\n",
    "        nonzero_rows = np.unique(rating_matrix.nonzero()[0])\n",
    "        rating_matrix = rating_matrix[nonzero_rows]\n",
    "        timestamp_matrix = timestamp_matrix[nonzero_rows]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[nonzero_rows]\n",
    "\n",
    "    user_num, item_num = rating_matrix.shape\n",
    "\n",
    "    rtrain = []\n",
    "    rtrain_userAvg = []\n",
    "    rtime = []\n",
    "    rvalid = []\n",
    "    rvalid_userAvg = []\n",
    "    rtest = []\n",
    "    rtest_userAvg = []\n",
    "    # Get the index list corresponding to item for train,valid,test\n",
    "    item_idx_train = []\n",
    "    item_idx_valid = []\n",
    "    item_idx_test = []\n",
    "    \n",
    "    for i in tqdm(range(user_num)):\n",
    "        #Get the non_zero indexs, restuarants where the user visited/liked if implicit \n",
    "        item_indexes = rating_matrix[i].nonzero()[1]        \n",
    "        #Get the data for the user\n",
    "        data = rating_matrix[i].data      \n",
    "        #Get time stamp value \n",
    "        timestamp = timestamp_matrix[i].data \n",
    "        #Get review stars with user avg data \n",
    "        if implicit == False:\n",
    "            dataWuserAvg = ratingWuserAvg_matrix[i].data\n",
    "\n",
    "            \n",
    "        #Non zero reviews for this user\n",
    "        num_nonzeros = len(item_indexes)\n",
    "        \n",
    "        #If the user has at least one review\n",
    "        if num_nonzeros >= 1:\n",
    "            num_test = int(num_nonzeros * ratio[2])\n",
    "            num_valid = int(num_nonzeros * (ratio[1] + ratio[2]))\n",
    "            valid_offset = num_nonzeros - num_valid\n",
    "            \n",
    "            # Adding this for sampling for training set\n",
    "            valid_offsetSample = int(valid_offset*trainSampling)\n",
    "            test_offset = num_nonzeros - num_test\n",
    "            \n",
    "            #Sort the timestamp for each review for the user\n",
    "            argsort = np.argsort(timestamp)\n",
    "            \n",
    "            #Sort the reviews for the user according to the time stamp \n",
    "            data = data[argsort]\n",
    "            \n",
    "            #Sort the review with user avg accoridng to the time stamp\n",
    "            if implicit == False:\n",
    "                dataWuserAvg = dataWuserAvg[argsort]\n",
    "            \n",
    "            #Non-zero review index sort according to time\n",
    "            item_indexes = item_indexes[argsort]\n",
    "            \n",
    "            #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "            #if take from old to new\n",
    "            #rtrain.append([data[:valid_offsetSample], np.full(valid_offsetSample, i), item_indexes[:valid_offsetSample]])\n",
    "            #if take from new to old\n",
    "            rtrain.append([data[valid_offset-valid_offsetSample:valid_offset], np.full(valid_offsetSample, i), item_indexes[valid_offset-valid_offsetSample:valid_offset]])\n",
    "            rvalid.append([data[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                           item_indexes[valid_offset:test_offset]])\n",
    "            rtest.append([data[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "            \n",
    "            if implicit == False:\n",
    "                #Now for the rating matrix that considers user average rating\n",
    "                #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "                #from old to new\n",
    "                #rtrain_userAvg.append([dataWuserAvg[:valid_offsetSample], np.full(valid_offsetSample, i), item_indexes[:valid_offsetSample]])\n",
    "                #take nearest\n",
    "                rtrain_userAvg.append([dataWuserAvg[valid_offset-valid_offsetSample:valid_offset], np.full(valid_offsetSample, i), item_indexes[valid_offset-valid_offsetSample:valid_offset]])                \n",
    "                    \n",
    "                rvalid_userAvg.append([dataWuserAvg[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                               item_indexes[valid_offset:test_offset]])\n",
    "                \n",
    "                rtest_userAvg.append([dataWuserAvg[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "                \n",
    "            item_idx_train.append(item_indexes[:valid_offsetSample])\n",
    "            \n",
    "        else:\n",
    "            item_idx_train.append([])\n",
    "    \n",
    "    rtrain = np.array(rtrain)\n",
    "    rvalid = np.array(rvalid)\n",
    "    rtest = np.array(rtest)\n",
    "   \n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = np.array(rtrain_userAvg)\n",
    "        rvalid_userAvg = np.array(rvalid_userAvg)\n",
    "        rtest_userAvg = np.array(rtest_userAvg)\n",
    "\n",
    "    #take non-zeros values, row index, and column (non-zero) index and store into sparse matrix\n",
    "    rtrain = sparse.csr_matrix((np.hstack(rtrain[:, 0]), (np.hstack(rtrain[:, 1]), np.hstack(rtrain[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rvalid = sparse.csr_matrix((np.hstack(rvalid[:, 0]), (np.hstack(rvalid[:, 1]), np.hstack(rvalid[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rtest = sparse.csr_matrix((np.hstack(rtest[:, 0]), (np.hstack(rtest[:, 1]), np.hstack(rtest[:, 2]))),\n",
    "                              shape=rating_matrix.shape, dtype=np.float32)\n",
    "    \n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = sparse.csr_matrix((np.hstack(rtrain_userAvg[:, 0]), (np.hstack(rtrain_userAvg[:, 1]), np.hstack(rtrain_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rvalid_userAvg = sparse.csr_matrix((np.hstack(rvalid_userAvg[:, 0]), (np.hstack(rvalid_userAvg[:, 1]), np.hstack(rvalid_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rtest_userAvg = sparse.csr_matrix((np.hstack(rtest_userAvg[:, 0]), (np.hstack(rtest_userAvg[:, 1]), np.hstack(rtest_userAvg[:, 2]))),\n",
    "                                  shape=rating_matrix.shape, dtype=np.float32)\n",
    "\n",
    "    return rtrain, rvalid, rtest,rtrain_userAvg, rvalid_userAvg, rtest_userAvg, nonzero_index, timestamp_matrix, item_idx_train, item_idx_valid, item_idx_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get df for training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Item idex matrix stores the reivews starts\n",
    "#This function returns a list of index for the reviews included in training set \n",
    "def get_corpus_idx_list(df, item_idx_matrix):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    df: total dataframe\n",
    "    item_idx_matrix: train index list got from time_split \n",
    "    Output: row index in original dataframe for training data by time split\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    #For all the users: 5791\n",
    "    for i in tqdm(range(len(item_idx_matrix))):\n",
    "        \n",
    "        #find row index where user_num_id is i\n",
    "        a = df.index[df['user_num_id'] == i].tolist()\n",
    "        \n",
    "        #loop through the busienss id that the user i reviewed for in offvalid set \n",
    "        for item_idx in  item_idx_matrix[i]:\n",
    "            \n",
    "            #get the row index for reviews for business that the user liked in the train set\n",
    "            b = df.index[df['business_num_id'] == item_idx].tolist()\n",
    "            \n",
    "            #Find the index for which this user liked, one user only rate a business once\n",
    "            idx_to_add = list(set(a).intersection(b))\n",
    "            \n",
    "            if idx_to_add not in lst:\n",
    "                lst.extend(idx_to_add)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess using Term Frequency - CounterVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\songya25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\songya25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Stemming and Lemmatisation\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Get corpus and CountVector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "new_words = ['not_the']\n",
    "stop_words = stop_words.union(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#Should 'because' added?\n",
    "def preprocess(df, reset_list = [',','.','?',';','however','but']):\n",
    "    corpus = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        text = df['review_text'][i]\n",
    "        change_flg = 0\n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        ##Convert to list from string, loop through the review text\n",
    "        text = text.split()\n",
    "        \n",
    "        #any sentence that encounters a not, the folloing words will become not phrase until hit the sentence end\n",
    "        for j in range(len(text)):\n",
    "            #Make the not_ hack\n",
    "            if text[j] == 'not':\n",
    "                change_flg = 1\n",
    "#                 print 'changes is made after ', i\n",
    "                continue\n",
    "            #if was 1 was round and not hit a 'not' in this round\n",
    "            if change_flg == 1 and any(reset in text[j] for reset in reset_list):\n",
    "                text[j] = 'not_' + text[j]\n",
    "                change_flg = 0\n",
    "#                 print 'reset at ', i\n",
    "            if change_flg == 1:\n",
    "                text[j] = 'not_' + text[j]\n",
    "        \n",
    "        #Convert back to string\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "        #Remove punctuations\n",
    "#       text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        \n",
    "        #remove tags\n",
    "        text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "        \n",
    "        # remove special characters and digits\n",
    "        text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "        \n",
    "        ##Convert to list from string\n",
    "        text = text.split()\n",
    "        \n",
    "        ##Stemming\n",
    "        ps=PorterStemmer()\n",
    "        \n",
    "        #Lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        text = [lem.lemmatize(word) for word in text if not word in  \n",
    "                stop_words] \n",
    "        text = \" \".join(text)\n",
    "        corpus.append(text)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def train(matrix_train):\n",
    "    similarity = cosine_similarity(X=matrix_train, Y=None, dense_output=True)\n",
    "    return similarity\n",
    "\n",
    "def get_I_K(df, X, row_name = 'business_num_id', binary = True, shape = (121994,6000)):\n",
    "    \"\"\"\n",
    "    get the item-keyphrase matrix\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    cols = []\n",
    "    vals = []\n",
    "    #For each review history\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        #Get the array of frequencies for document/review i \n",
    "        arr = X[i].toarray() \n",
    "        nonzero_element = arr.nonzero()[1]  # Get nonzero element in each line, keyphrase that appears index \n",
    "        length_of_nonzero = len(nonzero_element) #number of important keyphrase that appears\n",
    "        \n",
    "        # df[row_name][i] is the item idex\n",
    "        #Get a list row index that indicates the document/review\n",
    "        rows.extend(np.array([df[row_name][i]]*length_of_nonzero)) ## Item index\n",
    "        #print(rows)\n",
    "        \n",
    "        #Get a list of column index indicating the key phrase that appears in i document/review\n",
    "        cols.extend(nonzero_element) ## Keyword Index\n",
    "        if binary:\n",
    "            #Create a bunch of 1s\n",
    "            vals.extend(np.array([1]*length_of_nonzero))\n",
    "        else:\n",
    "            #If not binary \n",
    "            vals.extend(arr[arr.nonzero()])    \n",
    "    return csr_matrix((vals, (rows, cols)), shape=shape)\n",
    "\n",
    "\n",
    "#Get a UI matrix if it's not item_similarity based or else IU\n",
    "def predict(matrix_train, k, similarity, item_similarity_en = False):\n",
    "    prediction_scores = []\n",
    "    \n",
    "    #inverse to IU matrix\n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    #for each user or item, depends UI or IU \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "        # Get user u's prediction scores for all items\n",
    "        #Get prediction/similarity score for each user 1*num or user or num of items\n",
    "        vector_u = similarity[user_index]\n",
    "\n",
    "        # Get closest K neighbors excluding user u self\n",
    "        #Decending accoding to similarity score, select top k\n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        \n",
    "        # Get neighbors similarity weights and ratings\n",
    "        similar_users_weights = similarity[user_index][similar_users]\n",
    "        \n",
    "        #similar_users_weights_sum = np.sum(similar_users_weights)\n",
    "        #print(similar_users_weights.shape)\n",
    "        #shape: num of res * k\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "              \n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "        #print(prediction_scores_u)\n",
    "        \n",
    "        \n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "        \n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    return res\n",
    "\n",
    "\n",
    "#Preidction score is UI or IU?\n",
    "def prediction(prediction_score, topK, matrix_Train):\n",
    "\n",
    "    prediction = []\n",
    "\n",
    "    #for each user\n",
    "    for user_index in tqdm(range(matrix_Train.shape[0])):\n",
    "        \n",
    "        #take the prediction scores for user 1 * num res\n",
    "        vector_u = prediction_score[user_index]\n",
    "        \n",
    "        #The restuarant the user rated\n",
    "        vector_train = matrix_Train[user_index]\n",
    "        \n",
    "        if len(vector_train.nonzero()[0]) > 0:\n",
    "            vector_predict = sub_routine(vector_u, vector_train, topK=topK)\n",
    "        else:\n",
    "            vector_predict = np.zeros(topK, dtype=np.float32)\n",
    "\n",
    "        prediction.append(vector_predict)\n",
    "\n",
    "    return np.vstack(prediction)\n",
    "\n",
    "#topK: the number of restuarants we are suggesting \n",
    "#if vector_train has number, then the user has visited\n",
    "def sub_routine(vector_u, vector_train, topK=500):\n",
    "\n",
    "    #index where non-zero\n",
    "    train_index = vector_train.nonzero()[1]\n",
    "    \n",
    "    vector_u = vector_u\n",
    "    \n",
    "    #get topk + num rated res prediction score descending, top index \n",
    "    candidate_index = np.argpartition(-vector_u, topK+len(train_index))[:topK+len(train_index)]\n",
    "    \n",
    "    #sort top prediction score index in range topK+len(train_index) into vector_u`\n",
    "    vector_u = candidate_index[vector_u[candidate_index].argsort()[::-1]]\n",
    "    \n",
    "    #deleted the rated res from the topk+train_index prediction score vector for user u \n",
    "    #Delete the user rated res index from the topk+numRated index\n",
    "    vector_u = np.delete(vector_u, np.isin(vector_u, train_index).nonzero()[0])\n",
    "\n",
    "    #so we only include the top K prediction score here\n",
    "    return vector_u[:topK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recallk(vector_true_dense, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "\n",
    "def precisionk(vector_predict, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_predict)\n",
    "\n",
    "\n",
    "def average_precisionk(vector_predict, hits, **unused):\n",
    "    precisions = np.cumsum(hits, dtype=np.float32)/range(1, len(vector_predict)+1)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "def r_precision(vector_true_dense, vector_predict, **unused):\n",
    "    vector_predict_short = vector_predict[:len(vector_true_dense)]\n",
    "    hits = len(np.isin(vector_predict_short, vector_true_dense).nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "\n",
    "def _dcg_support(size):\n",
    "    arr = np.arange(1, size+1)+1\n",
    "    return 1./np.log2(arr)\n",
    "\n",
    "\n",
    "def ndcg(vector_true_dense, vector_predict, hits):\n",
    "    idcg = np.sum(_dcg_support(len(vector_true_dense)))\n",
    "    dcg_base = _dcg_support(len(vector_predict))\n",
    "    dcg_base[np.logical_not(hits)] = 0\n",
    "    dcg = np.sum(dcg_base)\n",
    "    return dcg/idcg\n",
    "\n",
    "\n",
    "def click(hits, **unused):\n",
    "    first_hit = next((i for i, x in enumerate(hits) if x), None)\n",
    "    if first_hit is None:\n",
    "        return 5\n",
    "    else:\n",
    "        return first_hit/10\n",
    "\n",
    "\n",
    "def evaluate(matrix_Predict, matrix_Test, metric_names =['R-Precision', 'NDCG', 'Precision', 'Recall', 'MAP'], atK = [5, 10, 15, 20, 50], analytical=False):\n",
    "    \"\"\"\n",
    "    :param matrix_U: Latent representations of users, for LRecs it is RQ, for ALSs it is U\n",
    "    :param matrix_V: Latent representations of items, for LRecs it is Q, for ALSs it is V\n",
    "    :param matrix_Train: Rating matrix for training, features.\n",
    "    :param matrix_Test: Rating matrix for evaluation, true labels.\n",
    "    :param k: Top K retrieval\n",
    "    :param metric_names: Evaluation metrics\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global_metrics = {\n",
    "        #\"R-Precision\": r_precision,\n",
    "        #\"NDCG\": ndcg,\n",
    "        #\"Clicks\": click\n",
    "    }\n",
    "\n",
    "    local_metrics = {\n",
    "        #\"Precision\": precisionk,\n",
    "        #\"Recall\": recallk,\n",
    "        \"MAP\": average_precisionk\n",
    "    }\n",
    "\n",
    "    output = dict()\n",
    "\n",
    "    num_users = matrix_Predict.shape[0]\n",
    "\n",
    "    for k in atK:\n",
    "\n",
    "        local_metric_names = list(set(metric_names).intersection(local_metrics.keys()))\n",
    "        results = {name: [] for name in local_metric_names}\n",
    "        topK_Predict = matrix_Predict[:, :k]\n",
    "\n",
    "        for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "            vector_predict = topK_Predict[user_index]\n",
    "            if len(vector_predict.nonzero()[0]) > 0:\n",
    "                vector_true = matrix_Test[user_index]\n",
    "                vector_true_dense = vector_true.nonzero()[1]\n",
    "                hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "                if vector_true_dense.size > 0:\n",
    "                    for name in local_metric_names:\n",
    "                        results[name].append(local_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                                 vector_predict=vector_predict,\n",
    "                                                                 hits=hits))\n",
    "        results_summary = dict()\n",
    "        if analytical:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = round(results[name],4)\n",
    "        else:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = (round((np.average(results[name])),4),\n",
    "                                                              round((1.96*np.std(results[name])/np.sqrt(num_users)),4))\n",
    "        output.update(results_summary)\n",
    "\n",
    "    global_metric_names = list(set(metric_names).intersection(global_metrics.keys()))\n",
    "    results = {name: [] for name in global_metric_names}\n",
    "\n",
    "    topK_Predict = matrix_Predict[:]\n",
    "\n",
    "    for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "        vector_predict = topK_Predict[user_index]\n",
    "\n",
    "        if len(vector_predict.nonzero()[0]) > 0:\n",
    "            vector_true = matrix_Test[user_index]\n",
    "            vector_true_dense = vector_true.nonzero()[1]\n",
    "            hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "            # if user_index == 1:\n",
    "            #     import ipdb;\n",
    "            #     ipdb.set_trace()\n",
    "\n",
    "            if vector_true_dense.size > 0:\n",
    "                for name in global_metric_names:\n",
    "                    results[name].append(global_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                              vector_predict=vector_predict,\n",
    "                                                              hits=hits))\n",
    "    results_summary = dict()\n",
    "    if analytical:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = round(results[name],4)\n",
    "    else:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = (round(np.average(results[name]),4), round((1.96*np.std(results[name])/np.sqrt(num_users)),4))\n",
    "    output.update(results_summary)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add 2 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility functions\n",
    "#3906 restuarant, 3000 keyphrase, 5791 user \n",
    "def add_two_matrix(ratio, U_I_matrix,I_K_matrix, shape = (3906, 3000+5791)):\n",
    "    # ratio determine Keywords/User in the matrix\n",
    "    rows = []\n",
    "    cols = []\n",
    "    datas = []\n",
    "    I_U_matrix = U_I_matrix.transpose()\n",
    "    \n",
    "    #for each restuarant\n",
    "    for i in tqdm(range(I_K_matrix.shape[0])):\n",
    "        #key phrase that this item has, column(key phrase) index\n",
    "        nonzero1 = I_K_matrix[i].nonzero()\n",
    "        \n",
    "        #user that rated this item, column(user) index \n",
    "        nonzero2 = I_U_matrix[i].nonzero()\n",
    "        \n",
    "        #Trying to create a sparse matrix that stores \n",
    "        #index of restuarant for (K + U) times\n",
    "        row = [i]*(len(nonzero1[1])+len(nonzero2[1]))\n",
    "        \n",
    "        #column index for key phrase and users that are non-zero\n",
    "        col = nonzero1[1].tolist()+ nonzero2[1].tolist()\n",
    "        \n",
    "        \n",
    "        data = [ratio]*len(nonzero1[1])+[1-ratio]*len(nonzero2[1]) # Binary representation of I-K/U matrix\n",
    "        \n",
    "        rows.extend(row)\n",
    "        cols.extend(col)\n",
    "        datas.extend(data)\n",
    "    return csr_matrix( (datas,(rows,cols)), shape=shape )\n",
    "\n",
    "def transfer_to_implicit(rating_matrix, threshold = 0):\n",
    "    temp_rating_matrix = sparse.csr_matrix(rating_matrix.shape)\n",
    "    temp_rating_matrix[(rating_matrix > threshold).nonzero()] = 1\n",
    "    rating_matrix = temp_rating_matrix\n",
    "    return rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImplicitMatrix(sparseMatrix, threashold=0):\n",
    "    temp_matrix = sparse.csr_matrix(sparseMatrix.shape)\n",
    "    temp_matrix[(sparseMatrix > threashold).nonzero()] = 1\n",
    "    return temp_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictUU(matrix_train, k, chooseWeigthMethod, similarity1=None, similarity2=None, similarity3=None, similarity4=None, similarity5=None, item_similarity_en = False):\n",
    "    prediction_scores = []\n",
    "    #Convert from list to ndarray, add an axis\n",
    "    if isinstance(chooseWeigthMethod, list):\n",
    "        chooseWeigthMethod = np.array(chooseWeigthMethod)[:, np.newaxis]\n",
    "   \n",
    "    \"make sure that when passing in chooseWeightMethod, the weight must be aligned with similarity metrices, even if set to None\"\n",
    "    \"They should add to 1 as well\"\n",
    "    #inverse to IU matrix\n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    #for each user or item, depends UI or IU \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "    #for user_index in tqdm(range(10,20)):\n",
    "        \n",
    "        numberSimilarMatrix = 0\n",
    "        # Get user u's prediction scores for all items \n",
    "        #Get prediction/similarity score for each user 1*num or user or num of items\n",
    "        if similarity1 is not None:\n",
    "            vector_u1 = similarity1[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u1 = [0]*matrix_train.shape[0]\n",
    "            \n",
    "        if similarity2 is not None:\n",
    "            vector_u2 = similarity2[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u2 = [0]*len(vector_u1)\n",
    "            \n",
    "        if similarity3 is not None:\n",
    "            vector_u3 = similarity3[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u3 = [0]*len(vector_u1)\n",
    "            \n",
    "        if similarity4 is not None:\n",
    "            vector_u4 = similarity4[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u4 = [0]*len(vector_u1)\n",
    "        \n",
    "        if similarity5 is not None:\n",
    "            vector_u5 = similarity5[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u5 = [0]*len(vector_u1)\n",
    "        \n",
    "        #Temperary vector that stacks all 4 vectors together\n",
    "        tempVector = np.array([vector_u1,vector_u2,vector_u3,vector_u4, vector_u5])\n",
    "        \n",
    "        if chooseWeigthMethod is None:\n",
    "            #Get the similarity score from the first similarity matrix anyways \n",
    "            vector_u = vector_u1.copy()\n",
    "            \n",
    "        #If we are choosing the max, min, avg or similarity scores\n",
    "        if chooseWeigthMethod is not None:\n",
    "            if chooseWeigthMethod == 'max':\n",
    "                vector_u = tempVector.max(axis=0)\n",
    "            elif chooseWeigthMethod == 'min':\n",
    "                vector_u = tempVector.min(axis=0)\n",
    "            elif chooseWeigthMethod == 'average':\n",
    "                vector_u = tempVector.mean(axis=0)\n",
    "            elif isinstance(chooseWeigthMethod, np.ndarray):\n",
    "                #Validate that number of weights passed in equals number of matrices\n",
    "                #assert(len(chooseWeigthMethod) == numberSimilarMatrix)\n",
    "                #Get the new combined similarity vector \n",
    "                weighted_u = tempVector * chooseWeigthMethod\n",
    "                vector_u =np.sum(weighted_u, axis=0)\n",
    "                #print((vector_u == vector_u4).sum())\n",
    "                \n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        \n",
    "        # Get neighbors similarity weights and ratings\n",
    "        #similar_users_weights = similarity1[user_index][similar_users]\n",
    "        similar_users_weights = vector_u[similar_users]\n",
    "        \n",
    "        #similar_users_weights_sum = np.sum(similar_users_weights)\n",
    "        #print(similar_users_weights.shape)\n",
    "        #shape: num of res * k\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "              \n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "        #print(prediction_scores_u)\n",
    "        \n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "        \n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get original dataframe out of the review datastet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_yelp_df(path ='', filename=reviewJsonToronto, sampling= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get rating-UI matrix and timestepm-UI matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix, timestamp_matrix , I_C_matrix = get_rating_timestamp_matrix(df)\n",
    "\n",
    "# get ratingWuserAvg_matrix\n",
    "rating_array = rating_matrix.toarray()\n",
    "user_average_array = rating_array.sum(axis = 1)/np.count_nonzero(rating_array,axis = 1)\n",
    "init_UI = np.zeros(rating_array.shape)\n",
    "init_UI[rating_array.nonzero()] = 1\n",
    "for i in range(user_average_array.shape[0]):\n",
    "    init_UI[i] = init_UI[i] * (user_average_array[i]-0.001) \n",
    "user_average_array = init_UI\n",
    "ratingWuserAvg_array = rating_array - user_average_array\n",
    "ratingWuserAvg_matrix=sparse.csr_matrix(ratingWuserAvg_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split to get rtrain-UI matrix and valid and test.. item_index_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  del sys.path[0]\n",
      "100%|| 6099/6099 [00:01<00:00, 5045.36it/s]\n"
     ]
    }
   ],
   "source": [
    "rtrain_implicit, rvalid_implicit, rtest_implicit, rtrain_userAvg_implicit, rvalid_userAvg_implicit, rtest_userAvg_implicit, nonzero_index, rtime, item_idx_matrix_train_implicit,item_idx_matrix_valid_implicit, item_idx_matrix_test_implicit = time_ordered_splitModified(rating_matrix=rating_matrix, ratingWuserAvg_matrix=ratingWuserAvg_matrix, timestamp_matrix=timestamp_matrix,\n",
    "                                                                     ratio=[0.5,0.2,0.3],\n",
    "                                                                     implicit=True,\n",
    "                                                                     remove_empty=False, threshold=3,sampling=False, \n",
    "                                                                     sampling_ratio=0.1, trainSampling=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6099/6099 [00:01<00:00, 3957.59it/s]\n"
     ]
    }
   ],
   "source": [
    "rtrain, rvalid, rtest, rtrain_userAvg, rvalid_userAvg, rtest_userAvg, nonzero_index, rtime, item_idx_matrix_train,item_idx_matrix_valid, item_idx_matrix_test = time_ordered_splitModified(rating_matrix=rating_matrix, ratingWuserAvg_matrix=ratingWuserAvg_matrix, timestamp_matrix=timestamp_matrix,\n",
    "                                                                     ratio=[0.5,0.2,0.3],\n",
    "                                                                     implicit=False,\n",
    "                                                                     remove_empty=False, threshold=3,\n",
    "                                                                     sampling=False, sampling_ratio=0.1, \n",
    "                                                                     trainSampling=0.95)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get df shrink to df_train for rtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6099/6099 [01:25<00:00, 71.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(91381, 43)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the list of row index for the training set \n",
    "lst_train = get_corpus_idx_list(df, item_idx_matrix_train)\n",
    "\n",
    "# Get the training dataframe from the original dataframe\n",
    "df_train = df.loc[lst_train]\n",
    "\n",
    "#Resetting the index of the train data\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>Unnamed: 0_y</th>\n",
       "      <th>Updated</th>\n",
       "      <th>Year</th>\n",
       "      <th>alias</th>\n",
       "      <th>business_id</th>\n",
       "      <th>business_stars</th>\n",
       "      <th>categories</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>date</th>\n",
       "      <th>display_phone</th>\n",
       "      <th>distance</th>\n",
       "      <th>friend_count</th>\n",
       "      <th>ghost</th>\n",
       "      <th>image_url</th>\n",
       "      <th>img_dsc</th>\n",
       "      <th>img_url</th>\n",
       "      <th>is_closed</th>\n",
       "      <th>location</th>\n",
       "      <th>name</th>\n",
       "      <th>nr</th>\n",
       "      <th>phone</th>\n",
       "      <th>photo_count</th>\n",
       "      <th>price</th>\n",
       "      <th>review_count_x</th>\n",
       "      <th>review_count_y</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_language</th>\n",
       "      <th>review_stars</th>\n",
       "      <th>review_text</th>\n",
       "      <th>transactions</th>\n",
       "      <th>ufc</th>\n",
       "      <th>url</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_loc</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>business_num_id</th>\n",
       "      <th>user_num_id</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>628892</td>\n",
       "      <td>628892</td>\n",
       "      <td>8039</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>bobbette-and-belle-toronto</td>\n",
       "      <td>vcxvQyAggPqxcHwvJXvjGg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[{'alias': 'catering', 'title': 'Caterers'}, {...</td>\n",
       "      <td>{'latitude': 43.66201, 'longitude': -79.33476}</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>+1 416-466-8800</td>\n",
       "      <td>7046.832404</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>https://s3-media3.fl.yelpcdn.com/bphoto/7DRlzP...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>{'address1': '1121 Queen Street E', 'address2'...</td>\n",
       "      <td>Bobbette &amp; Belle</td>\n",
       "      <td>False</td>\n",
       "      <td>1.416467e+10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>$$</td>\n",
       "      <td>78</td>\n",
       "      <td>154</td>\n",
       "      <td>['1', '13', '2017']</td>\n",
       "      <td>9e3cmOBflFYd98XZVN8YXQ</td>\n",
       "      <td>en</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Food and drink: Loved the alfajores (mini lemo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://www.yelp.com/biz/bobbette-and-belle-to...</td>\n",
       "      <td>--BumyUHiO_7YsHurb9Hkw</td>\n",
       "      <td>Old Toronto, Toronto, Canada</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3724</td>\n",
       "      <td>0</td>\n",
       "      <td>1.484284e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>582871</td>\n",
       "      <td>582871</td>\n",
       "      <td>15015</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>pai-northern-thai-kitchen-toronto-5</td>\n",
       "      <td>r_BrIgzYcwo1NAuG9dLbpg</td>\n",
       "      <td>4.5</td>\n",
       "      <td>[{'alias': 'thai', 'title': 'Thai'}]</td>\n",
       "      <td>{'latitude': 43.647866, 'longitude': -79.3886415}</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>+1 416-901-4724</td>\n",
       "      <td>4202.070047</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>https://s3-media2.fl.yelpcdn.com/bphoto/KzTHwC...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>{'address1': '18 Duncan Street', 'address2': '...</td>\n",
       "      <td>Pai Northern Thai Kitchen</td>\n",
       "      <td>False</td>\n",
       "      <td>1.416901e+10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>$$</td>\n",
       "      <td>78</td>\n",
       "      <td>2144</td>\n",
       "      <td>['1', '13', '2017']</td>\n",
       "      <td>El28CtPV5fnMXyofHoGhoA</td>\n",
       "      <td>en</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Food and drink: One of my favourite places for...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://www.yelp.com/biz/pai-northern-thai-kit...</td>\n",
       "      <td>--BumyUHiO_7YsHurb9Hkw</td>\n",
       "      <td>Old Toronto, Toronto, Canada</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3467</td>\n",
       "      <td>0</td>\n",
       "      <td>1.484284e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>437873</td>\n",
       "      <td>437873</td>\n",
       "      <td>10150</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>college-falafel-toronto</td>\n",
       "      <td>xsl-d_opm3AU5H2Z-im33g</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[{'alias': 'mideastern', 'title': 'Middle East...</td>\n",
       "      <td>{'latitude': 43.65455, 'longitude': -79.42289}</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>+1 416-532-8698</td>\n",
       "      <td>4471.118815</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>https://s3-media1.fl.yelpcdn.com/bphoto/NjObd5...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>{'address1': '450 Ossington Ave', 'address2': ...</td>\n",
       "      <td>College Falafel</td>\n",
       "      <td>False</td>\n",
       "      <td>1.416533e+10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>$</td>\n",
       "      <td>78</td>\n",
       "      <td>86</td>\n",
       "      <td>['1', '23', '2017']</td>\n",
       "      <td>jq-96Xeg9ilhl_jxyN49PA</td>\n",
       "      <td>en</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Food: The falafel sandwich was great - fresh, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://www.yelp.com/biz/college-falafel-toron...</td>\n",
       "      <td>--BumyUHiO_7YsHurb9Hkw</td>\n",
       "      <td>Old Toronto, Toronto, Canada</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3863</td>\n",
       "      <td>0</td>\n",
       "      <td>1.485148e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>385263</td>\n",
       "      <td>385263</td>\n",
       "      <td>2748</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>bar-raval-toronto</td>\n",
       "      <td>41o1FUbCYKJv2djtnlkzlg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[{'alias': 'spanish', 'title': 'Spanish'}, {'a...</td>\n",
       "      <td>{'latitude': 43.6559438482769, 'longitude': -7...</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>+1 647-344-8001</td>\n",
       "      <td>3530.003000</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>https://s3-media2.fl.yelpcdn.com/bphoto/Bx6Tl9...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>{'address1': '505 College Street', 'address2':...</td>\n",
       "      <td>Bar Raval</td>\n",
       "      <td>False</td>\n",
       "      <td>1.647345e+10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>$$$</td>\n",
       "      <td>78</td>\n",
       "      <td>228</td>\n",
       "      <td>['1', '23', '2017']</td>\n",
       "      <td>htHfoKD30p9sgQoFwye9jQ</td>\n",
       "      <td>en</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Food and drink: Went for cocktails only; I had...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://www.yelp.com/biz/bar-raval-toronto?adj...</td>\n",
       "      <td>--BumyUHiO_7YsHurb9Hkw</td>\n",
       "      <td>Old Toronto, Toronto, Canada</td>\n",
       "      <td>0.0</td>\n",
       "      <td>317</td>\n",
       "      <td>0</td>\n",
       "      <td>1.485148e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>88984</td>\n",
       "      <td>88984</td>\n",
       "      <td>11232</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>bar-isabel-toronto</td>\n",
       "      <td>q5xrVJ4kivx_yEfJeOKNYQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[{'alias': 'spanish', 'title': 'Spanish'}, {'a...</td>\n",
       "      <td>{'latitude': 43.65463, 'longitude': -79.42075}</td>\n",
       "      <td>2017-01-23</td>\n",
       "      <td>+1 416-532-2222</td>\n",
       "      <td>4230.791736</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>https://s3-media1.fl.yelpcdn.com/bphoto/Ne8Ykv...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>{'address1': '797 College Street', 'address2':...</td>\n",
       "      <td>Bar Isabel</td>\n",
       "      <td>False</td>\n",
       "      <td>1.416532e+10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>$$$</td>\n",
       "      <td>78</td>\n",
       "      <td>379</td>\n",
       "      <td>['1', '23', '2017']</td>\n",
       "      <td>jIgYSZAWS0TX4097BN1u8g</td>\n",
       "      <td>en</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Food and drink: As a vegetarian, the options a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>https://www.yelp.com/biz/bar-isabel-toronto?ad...</td>\n",
       "      <td>--BumyUHiO_7YsHurb9Hkw</td>\n",
       "      <td>Old Toronto, Toronto, Canada</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3391</td>\n",
       "      <td>0</td>\n",
       "      <td>1.485148e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day  Month  Unnamed: 0.1  Unnamed: 0_x  Unnamed: 0_y  Updated  Year  \\\n",
       "0   13      1        628892        628892          8039    False  2017   \n",
       "1   13      1        582871        582871         15015    False  2017   \n",
       "2   23      1        437873        437873         10150    False  2017   \n",
       "3   23      1        385263        385263          2748    False  2017   \n",
       "4   23      1         88984         88984         11232    False  2017   \n",
       "\n",
       "                                 alias             business_id  \\\n",
       "0           bobbette-and-belle-toronto  vcxvQyAggPqxcHwvJXvjGg   \n",
       "1  pai-northern-thai-kitchen-toronto-5  r_BrIgzYcwo1NAuG9dLbpg   \n",
       "2              college-falafel-toronto  xsl-d_opm3AU5H2Z-im33g   \n",
       "3                    bar-raval-toronto  41o1FUbCYKJv2djtnlkzlg   \n",
       "4                   bar-isabel-toronto  q5xrVJ4kivx_yEfJeOKNYQ   \n",
       "\n",
       "   business_stars                                         categories  \\\n",
       "0             4.0  [{'alias': 'catering', 'title': 'Caterers'}, {...   \n",
       "1             4.5               [{'alias': 'thai', 'title': 'Thai'}]   \n",
       "2             4.0  [{'alias': 'mideastern', 'title': 'Middle East...   \n",
       "3             4.0  [{'alias': 'spanish', 'title': 'Spanish'}, {'a...   \n",
       "4             4.0  [{'alias': 'spanish', 'title': 'Spanish'}, {'a...   \n",
       "\n",
       "                                         coordinates        date  \\\n",
       "0     {'latitude': 43.66201, 'longitude': -79.33476}  2017-01-13   \n",
       "1  {'latitude': 43.647866, 'longitude': -79.3886415}  2017-01-13   \n",
       "2     {'latitude': 43.65455, 'longitude': -79.42289}  2017-01-23   \n",
       "3  {'latitude': 43.6559438482769, 'longitude': -7...  2017-01-23   \n",
       "4     {'latitude': 43.65463, 'longitude': -79.42075}  2017-01-23   \n",
       "\n",
       "     display_phone     distance  friend_count  ghost  \\\n",
       "0  +1 416-466-8800  7046.832404             4  False   \n",
       "1  +1 416-901-4724  4202.070047             4  False   \n",
       "2  +1 416-532-8698  4471.118815             4  False   \n",
       "3  +1 647-344-8001  3530.003000             4  False   \n",
       "4  +1 416-532-2222  4230.791736             4  False   \n",
       "\n",
       "                                           image_url img_dsc img_url  \\\n",
       "0  https://s3-media3.fl.yelpcdn.com/bphoto/7DRlzP...      []      []   \n",
       "1  https://s3-media2.fl.yelpcdn.com/bphoto/KzTHwC...      []      []   \n",
       "2  https://s3-media1.fl.yelpcdn.com/bphoto/NjObd5...      []      []   \n",
       "3  https://s3-media2.fl.yelpcdn.com/bphoto/Bx6Tl9...      []      []   \n",
       "4  https://s3-media1.fl.yelpcdn.com/bphoto/Ne8Ykv...      []      []   \n",
       "\n",
       "   is_closed                                           location  \\\n",
       "0      False  {'address1': '1121 Queen Street E', 'address2'...   \n",
       "1      False  {'address1': '18 Duncan Street', 'address2': '...   \n",
       "2      False  {'address1': '450 Ossington Ave', 'address2': ...   \n",
       "3      False  {'address1': '505 College Street', 'address2':...   \n",
       "4      False  {'address1': '797 College Street', 'address2':...   \n",
       "\n",
       "                        name     nr         phone  photo_count price  \\\n",
       "0           Bobbette & Belle  False  1.416467e+10         13.0    $$   \n",
       "1  Pai Northern Thai Kitchen  False  1.416901e+10         13.0    $$   \n",
       "2            College Falafel  False  1.416533e+10         13.0     $   \n",
       "3                  Bar Raval  False  1.647345e+10         13.0   $$$   \n",
       "4                 Bar Isabel  False  1.416532e+10         13.0   $$$   \n",
       "\n",
       "   review_count_x  review_count_y          review_date  \\\n",
       "0              78             154  ['1', '13', '2017']   \n",
       "1              78            2144  ['1', '13', '2017']   \n",
       "2              78              86  ['1', '23', '2017']   \n",
       "3              78             228  ['1', '23', '2017']   \n",
       "4              78             379  ['1', '23', '2017']   \n",
       "\n",
       "                review_id review_language  review_stars  \\\n",
       "0  9e3cmOBflFYd98XZVN8YXQ              en           5.0   \n",
       "1  El28CtPV5fnMXyofHoGhoA              en           4.0   \n",
       "2  jq-96Xeg9ilhl_jxyN49PA              en           4.0   \n",
       "3  htHfoKD30p9sgQoFwye9jQ              en           3.0   \n",
       "4  jIgYSZAWS0TX4097BN1u8g              en           4.0   \n",
       "\n",
       "                                         review_text transactions        ufc  \\\n",
       "0  Food and drink: Loved the alfajores (mini lemo...           []  [0, 0, 0]   \n",
       "1  Food and drink: One of my favourite places for...           []  [0, 0, 0]   \n",
       "2  Food: The falafel sandwich was great - fresh, ...           []  [0, 0, 0]   \n",
       "3  Food and drink: Went for cocktails only; I had...           []  [0, 0, 0]   \n",
       "4  Food and drink: As a vegetarian, the options a...           []  [0, 0, 0]   \n",
       "\n",
       "                                                 url                 user_id  \\\n",
       "0  https://www.yelp.com/biz/bobbette-and-belle-to...  --BumyUHiO_7YsHurb9Hkw   \n",
       "1  https://www.yelp.com/biz/pai-northern-thai-kit...  --BumyUHiO_7YsHurb9Hkw   \n",
       "2  https://www.yelp.com/biz/college-falafel-toron...  --BumyUHiO_7YsHurb9Hkw   \n",
       "3  https://www.yelp.com/biz/bar-raval-toronto?adj...  --BumyUHiO_7YsHurb9Hkw   \n",
       "4  https://www.yelp.com/biz/bar-isabel-toronto?ad...  --BumyUHiO_7YsHurb9Hkw   \n",
       "\n",
       "                       user_loc  vote_count  business_num_id  user_num_id  \\\n",
       "0  Old Toronto, Toronto, Canada         0.0             3724            0   \n",
       "1  Old Toronto, Toronto, Canada         0.0             3467            0   \n",
       "2  Old Toronto, Toronto, Canada         0.0             3863            0   \n",
       "3  Old Toronto, Toronto, Canada         0.0              317            0   \n",
       "4  Old Toronto, Toronto, Canada         0.0             3391            0   \n",
       "\n",
       "      timestamp  \n",
       "0  1.484284e+09  \n",
       "1  1.484284e+09  \n",
       "2  1.485148e+09  \n",
       "3  1.485148e+09  \n",
       "4  1.485148e+09  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If using term frequency only to compute corpus and X(review vs. terms) CounterVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 91381/91381 [00:31<00:00, 2919.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# The entire corpus\n",
    "corpus = preprocess(df_train)\n",
    "# X row: df_train row, column: key words frequency \n",
    "# When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n",
    "cv=CountVectorizer(max_df=0.9,stop_words=stop_words, max_features=5000, ngram_range=(1,1))\n",
    "X=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If using TD-IDF to compute corpus and X (business vs. terms) TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 91381/91381 [00:33<00:00, 2716.39it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = preprocess(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary to store business: review text\n",
    "dict_text = {}\n",
    "for i in range(len(corpus)):\n",
    "    if df_train['business_num_id'][i] not in dict_text:\n",
    "        dict_text[df_train['business_num_id'][i]] = corpus[i]\n",
    "    else:\n",
    "        temp = dict_text[df_train['business_num_id'][i]]\n",
    "        temp = temp + corpus[i]\n",
    "        dict_text[df_train['business_num_id'][i]] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list for the review text, where the row dimension = total business ids\n",
    "list_text = []\n",
    "for key in range(0,max(list(dict_text.keys()))+1) :\n",
    "    if key not in dict_text.keys():\n",
    "        list_text.extend([\"\"])\n",
    "    else:\n",
    "        list_text.extend([dict_text[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the X vector, where dimension is #business vs #terms\n",
    "vectorizer = TfidfVectorizer(max_df=0.9,stop_words=stop_words, max_features=5000, ngram_range=(1,1))\n",
    "X_cleaned = vectorizer.fit_transform(list_text).toarray()\n",
    "X_cleaned_sparse = csr_matrix(X_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-rating KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. With ratings that subtracts user average rating, cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6099/6099 [00:09<00:00, 649.26it/s]\n",
      "100%|| 6099/6099 [00:01<00:00, 4583.52it/s]\n"
     ]
    }
   ],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "similarity_1 = train(rtrain)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "UI_predictionScore_Explicit = predict(rtrain_userAvg, 100, similarity_1, item_similarity_en= False)\n",
    "UI_predict_Explicit = prediction(UI_predictionScore_Explicit, 50, rtrain_userAvg)\n",
    "#user_item_res1 = evaluate(user_item_predict1, rvalid_userAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2397., 1812., 3393., 3591., 2577.,  102.,  360., 2597., 3467.,\n",
       "       2006., 1366.,  421., 2875., 3813., 3522., 3795.,  379.,  587.,\n",
       "        588., 2713., 1047.,  881.,  747.,  621.,  914., 2277., 2540.,\n",
       "       2527., 1050., 2460., 2801.,  331., 3140., 3613., 2660., 3614.,\n",
       "       2896.,  307., 1711., 1579., 1902., 3419.,  211.,  828.,  109.,\n",
       "       3032., 3945., 2510., 1384., 1074.])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UI_predict_Explicit[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implicit User-rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6099/6099 [00:09<00:00, 660.41it/s]\n",
      "100%|| 6099/6099 [00:01<00:00, 4683.62it/s]\n"
     ]
    }
   ],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "similarity_2 = train(rtrain_implicit)\n",
    "UI_predictionScore_Implicit = predict(rtrain_implicit, 100, similarity_2, item_similarity_en= False)\n",
    "UI_predict_Implicit = prediction(UI_predictionScore_Implicit, 50, rtrain_implicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3637., 2397., 2234.,  747., 2741.,  102., 2875., 3032.,  588.,\n",
       "       2713., 2726., 1711., 2930., 2517., 1440., 2577., 2880., 3615.,\n",
       "       2560., 1366., 1790., 2524., 2597.,  621., 1880.,  295.,  833.,\n",
       "       3160., 1083., 3155.,  768., 2896., 2841., 1741., 2732., 3933.,\n",
       "        683., 2219., 2450.,  164., 3204.,  587.,  815.,  722., 3390.,\n",
       "       1356., 2038., 3326., 2571., 1862.])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UI_predict_Implicit[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implicit similarity, Explicit user-rating combination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6099/6099 [00:09<00:00, 668.23it/s]\n",
      "100%|| 6099/6099 [00:01<00:00, 4460.23it/s]\n"
     ]
    }
   ],
   "source": [
    "similarity_3 = train(rtrain_implicit)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "UI_predictionScore_IECombined = predict(rtrain, 100, similarity_3, item_similarity_en= False)\n",
    "UI_predict_IECombined = prediction(UI_predictionScore_IECombined, 50, rtrain)\n",
    "# user_item_res1 = evaluate(UI_predict_IECombined, rvalid)\n",
    "# user_item_res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. NO NEED Something random, to be filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6099/6099 [00:09<00:00, 657.16it/s]\n",
      "100%|| 6099/6099 [00:01<00:00, 4622.96it/s]\n"
     ]
    }
   ],
   "source": [
    "similarity_4 = train(rtrain)\n",
    "UI_predictionScore_random1 = predict(rtrain_implicit, 100, similarity_4, item_similarity_en= False)\n",
    "UI_predict_random1 = prediction(UI_predictionScore_random1, 50, rtrain_implicit)\n",
    "# user_item_res1 = evaluate(UI_predict_IECombined, rvalid)\n",
    "# user_item_res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. something random, to be filled 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "userVisitMatrix = getImplicitMatrix(rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity1 = train(rtrain)\n",
    "similarity2 = train(rtrain_implicit)\n",
    "similarity3 =train(userVisitMatrix)\n",
    "similarity4 = train(rtrain_userAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6099/6099 [00:18<00:00, 327.42it/s]\n",
      "100%|| 6099/6099 [00:01<00:00, 4381.99it/s]\n"
     ]
    }
   ],
   "source": [
    "UI_predictionScore_max = predictUU(rtrain, 100, 'max', similarity1, similarity2, similarity3, similarity4)\n",
    "UI_predict_max = prediction(UI_predictionScore_max, 50, rtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. IU, Explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3996/3996 [00:10<00:00, 392.51it/s]\n",
      "100%|| 6099/6099 [00:01<00:00, 3850.54it/s]\n"
     ]
    }
   ],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "similarity_6 = train(rtrain.T)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "IU_predictionScore_Explicit = predict(rtrain, 100, similarity_6, item_similarity_en= True)\n",
    "IU_predict_Explicit = prediction(IU_predictionScore_Explicit, 50, rtrain)\n",
    "#user_item_res1 = evaluate(user_item_predict1, rvalid_userAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3637., 3467.,  747., 2234., 2397., 1887., 3160.,  144., 1511.,\n",
       "       1246., 1510.,  902., 2193., 1440.,  683., 2841., 1138., 2076.,\n",
       "       3393.,  881.,  492., 2142., 1536., 3297., 1109., 1334., 3010.,\n",
       "       3613., 1619., 2006., 3214., 2460.,  102., 3615., 2913., 3415.,\n",
       "       3176., 2597., 3959.,  621., 2724.,  316.,  380., 3371., 2219.,\n",
       "        864., 3401.,  331., 3324.,  816.])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IU_predict_Explicit[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Item_based TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3996/3996 [00:03<00:00, 1016.37it/s]\n",
      "100%|| 6099/6099 [00:01<00:00, 4052.49it/s]\n"
     ]
    }
   ],
   "source": [
    "IK_MATRIX = X_cleaned_sparse\n",
    "I_I_similarity = train(IK_MATRIX)\n",
    "item_based_prediction_score4 = predict(rtrain, 10, I_I_similarity, item_similarity_en= True)\n",
    "#for each restuarant top50 users \n",
    "Item_predict_tfidf = prediction(item_based_prediction_score4, 50, rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1126.,   24.,   14., 3711., 1211., 1945., 2822., 2148.,  993.,\n",
       "       1554., 1866., 1334., 3390., 2969.,  206.,  677., 3615.,  146.,\n",
       "       1626.,  714., 2625., 3948., 1194.,  961., 1915., 2389., 1812.,\n",
       "       1007.,  665., 2248., 3036., 3607., 2865.,  628.,  325., 3801.,\n",
       "       2052., 2287., 2446., 1382., 3854., 2624.,  360., 3411., 1265.,\n",
       "       3710., 3819., 3184., 3679., 3311.])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Item_predict_tfidf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find restaurant recommendation within set location radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the popular items in three ways\n",
    "1. avg stars\n",
    "2. number of reviews\n",
    "3. percentage liked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff = df.copy()\n",
    "dff_popular = df.copy()\n",
    "dff_popular = dff_popular.sort_values(by=[\"review_count_y\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "popular_list = dff_popular[\"business_num_id\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_three_popularity_matrix(df_original,rtrain):\n",
    "    # get the list of popular items by ranking the number of reviews\n",
    "    dff_popular = df_original.copy()\n",
    "    dff_popular = dff_popular.sort_values(by=[\"review_count_y\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "    popular_list_num_of_reviews = dff_popular[\"business_num_id\"].tolist()\n",
    "    \n",
    "    # get the list of popular items by ranking average rating score\n",
    "    dff_popular_rating = df_original.copy()\n",
    "    dff_popular_rating = dff_popular.sort_values(by=[\"business_stars\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "    popular_list_avg_stars = dff_popular_rating[\"business_num_id\"].tolist()\n",
    "    \n",
    "    \n",
    "    # get the popularity items by using the percentage liked method(number of liked items / total items)\n",
    "    numUsers = rtrain.shape[0]\n",
    "    numItems = rtrain.shape[1]\n",
    "    \n",
    "    predictionMatrix = np.zeros((numUsers , numItems))\n",
    "\n",
    "    # Define function for converting 1-5 rating to 0/1 (like / don't like)\n",
    "    vf = np.vectorize(lambda x: 1 if x >= 4 else 0)\n",
    "    rtrain_array = rtrain.toarray()\n",
    "    # For every item calculate the number of people liked (4-5) divided by the number of people that rated\n",
    "    itemPopularity = np.zeros((numItems))\n",
    "    for item in range(numItems):\n",
    "        numOfUsersRated = len(rtrain_array[:, item].nonzero()[0])\n",
    "        numOfUsersLiked = len(vf(rtrain_array[:, item]).nonzero()[0])\n",
    "#         if numOfUsersRated == 0:\n",
    "        # set a threshold to filter out restaurants with very few reviews\n",
    "        if numOfUsersRated <= 100:\n",
    "            itemPopularity[item] = 0\n",
    "        else:\n",
    "            itemPopularity[item] = numOfUsersLiked/numOfUsersRated\n",
    "    popular_list_liked_ratio = itemPopularity.argsort()\n",
    "    \n",
    "    return np.asarray(popular_list_num_of_reviews),np.asarray(popular_list_avg_stars),popular_list_liked_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of users and number of items\n",
    "numUsers = rtrain.shape[0]\n",
    "numItems = rtrain.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 1d array of avg. stars, number of reviews and percentage liked ratio for the three popular metric\n",
    "popular_list_num_of_reviews,popular_list_avg_stars,popular_list_liked_ratio = get_three_popularity_matrix(df,rtrain)\n",
    "matrix_popular_list_num_of_reviews = np.tile(popular_list_num_of_reviews,(numUsers,1))\n",
    "matrix_popular_list_avg_stars = np.tile(popular_list_avg_stars,(numUsers,1))\n",
    "matrix_popular_list_liked_ratio = np.tile(popular_list_liked_ratio,(numUsers,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### small analysis on the three popularity methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6099/6099 [00:00<00:00, 6871.66it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 6913.50it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 7032.90it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 6902.41it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 6907.37it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 7044.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@5': (0.0205, 0.0022),\n",
       " 'MAP@10': (0.0179, 0.0016),\n",
       " 'MAP@15': (0.0165, 0.0013),\n",
       " 'MAP@20': (0.0157, 0.0011),\n",
       " 'MAP@50': (0.013, 0.0007)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_list_num_of_reviews_result = evaluate(matrix_popular_list_num_of_reviews, rvalid)\n",
    "popular_list_num_of_reviews_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6099/6099 [00:00<00:00, 6863.43it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 7086.27it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 7102.72it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 7045.23it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 6902.52it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 7144.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@5': (0.0001, 0.0001),\n",
       " 'MAP@10': (0.0002, 0.0001),\n",
       " 'MAP@15': (0.0002, 0.0001),\n",
       " 'MAP@20': (0.0003, 0.0001),\n",
       " 'MAP@50': (0.0003, 0.0001)}"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_list_avg_stars_result = evaluate(matrix_popular_list_avg_stars, rvalid)\n",
    "popular_list_avg_stars_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6099/6099 [00:00<00:00, 6933.35it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 7004.87it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 6972.90it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 7004.68it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 6848.15it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 7237.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@5': (0.0013, 0.0005),\n",
       " 'MAP@10': (0.0012, 0.0004),\n",
       " 'MAP@15': (0.0012, 0.0003),\n",
       " 'MAP@20': (0.0012, 0.0003),\n",
       " 'MAP@50': (0.0013, 0.0002)}"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_list_liked_ratio_result = evaluate(matrix_popular_list_liked_ratio, rvalid)\n",
    "popular_list_liked_ratio_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0012"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_list_liked_ratio_result[\"MAP@10\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[\"Numer of Reviews\",popular_list_num_of_reviews_result[\"MAP@10\"][0]],\\\n",
    "        [\"Average Stars\",popular_list_avg_stars_result[\"MAP@10\"][0]],\\\n",
    "        [\"Liked Ratio(threshold@0)\",0.0004],\\\n",
    "        [\"Liked Ratio(threshold@50)\",0.0007,],\\\n",
    "        [\"Liked Ratio(threshold@100)\",popular_list_liked_ratio_result[\"MAP@10\"][0]]\\\n",
    "        ]\n",
    "df_small_analysis = pd.DataFrame(data, columns = ['Method', 'Map@10']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Map@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Numer of Reviews</td>\n",
       "      <td>0.0179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Average Stars</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Liked Ratio(threshold@0)</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liked Ratio(threshold@50)</td>\n",
       "      <td>0.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Liked Ratio(threshold@100)</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Method  Map@10\n",
       "0            Numer of Reviews  0.0179\n",
       "1               Average Stars  0.0002\n",
       "2    Liked Ratio(threshold@0)  0.0004\n",
       "3   Liked Ratio(threshold@50)  0.0007\n",
       "4  Liked Ratio(threshold@100)  0.0012"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "popular_list_liked_ratio_result --> filter out items with reviews less than 100. Threshold cannot be larger since otherwise it could not differentiate with the \"number of reviews\" method. It could not be 100 and less since the precision is too bad, it could not represent popular items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyze and visualize locations of the popular items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gmplot\n",
      "  Downloading https://files.pythonhosted.org/packages/e2/b1/e1429c31a40b3ef5840c16f78b506d03be9f27e517d3870a6fd0b356bd46/gmplot-1.2.0.tar.gz (115kB)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from gmplot) (2.21.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->gmplot) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->gmplot) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->gmplot) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->gmplot) (3.0.4)\n",
      "Building wheels for collected packages: gmplot\n",
      "  Building wheel for gmplot (setup.py): started\n",
      "  Building wheel for gmplot (setup.py): finished with status 'done'\n",
      "  Created wheel for gmplot: filename=gmplot-1.2.0-cp37-none-any.whl size=143770 sha256=99ddcb5169eae91ceb60661ce76598c8fb73be4e53beb3df67e342da1b9b3bd4\n",
      "  Stored in directory: C:\\Users\\songya25\\AppData\\Local\\pip\\Cache\\wheels\\81\\6a\\76\\4dd6a7cc310ba765894159ee84871e8cd55221d82ef14b81a1\n",
      "Successfully built gmplot\n",
      "Installing collected packages: gmplot\n",
      "Successfully installed gmplot-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gmplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "import gmplot \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GoogleMapPlotter return Map object \n",
    "# Pass the center latitude and \n",
    "# center longitude \n",
    "gmap1 = gmplot.GoogleMapPlotter(43.663149, -79.395718, 13 ) \n",
    "  \n",
    "# Pass the absolute path \n",
    "gmap1.draw( \"..\\\\data\\\\map11.html\" ) \n",
    "\n",
    "# empty map\n",
    "# file:///W:/Capstone/Tina/iNAGO_RecSys/data/map11.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0, 2636, 2637, ..., 3234, 3351, 1812], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_list_liked_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_coordinates = dff.coordinates.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = []\n",
    "long = []\n",
    "for item in range(array_coordinates.shape[0]):\n",
    "    coordinateDict = yaml.safe_load(array_coordinates[item])\n",
    "    lat.append(coordinateDict[\"latitude\"])\n",
    "    long.append(coordinateDict[\"longitude\"])\n",
    "    \n",
    "latitude_list = lat\n",
    "  \n",
    "longitude_list = long\n",
    "  \n",
    "gmap4 = gmplot.GoogleMapPlotter(43.663149, -79.395718, 13 ) \n",
    "  \n",
    "# heatmap plot heating Type \n",
    "# points on the Google map \n",
    "gmap4.heatmap( latitude_list, longitude_list ) \n",
    "  \n",
    "gmap4.draw( \"..\\\\data\\\\toronto_mapping.html\" ) \n",
    "# file:///W:/Capstone/Tina/iNAGO_RecSys/data/toronto_mapping.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommend 3 lists for three locations by using \"popular_list_liked_ratio\" method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either a list or \n",
    "def geographical_dist(prediction_matrix,intersection,user_id, bus_indexRange):\n",
    "    lst = []\n",
    "    length = 0\n",
    "    if isinstance(prediction_matrix, list):\n",
    "        length = len(prediction_matrix)\n",
    "    else:\n",
    "        length = prediction_matrix[user_id].shape[0]\n",
    "    for j in range(length):\n",
    "        if isinstance(prediction_matrix, list):\n",
    "            coordinateDict = yaml.safe_load(dff[dff[\"business_num_id\"] == prediction_matrix[j]].iloc[0].coordinates)\n",
    "        else:    \n",
    "            coordinateDict = yaml.safe_load(dff[dff[\"business_num_id\"] == prediction_matrix[user_id][j]].iloc[0].coordinates)\n",
    "        test_point = Point(coordinateDict['latitude'],coordinateDict['longitude'])\n",
    "\n",
    "        result = distance.distance(intersection,test_point).kilometers\n",
    "        if result<=0.6:\n",
    "            if isinstance(prediction_matrix, list):\n",
    "                lst.append(prediction_matrix[j])\n",
    "            else:\n",
    "                lst.append(prediction_matrix[user_id][j])\n",
    "        lst = lst[0:bus_indexRange]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # either a list or \n",
    "# def geographical_dist(prediction_matrix,intersection,user_id, bus_indexRange):\n",
    "#     list = []\n",
    "#     length = 0\n",
    "#     if isinstance(prediction_matrix, type(list)):\n",
    "#         length = len(popular_list)\n",
    "#     else:\n",
    "#         length = prediction_matrix[user_id].shape[0]\n",
    "#     for j in range(length):\n",
    "#         if isinstance(prediction_matrix, type(list)):\n",
    "#             coordinateDict = yaml.safe_load(dff[dff[\"business_num_id\"] == popular_list[j]].iloc[0].coordinates)\n",
    "#         else:    \n",
    "#             coordinateDict = yaml.safe_load(dff[dff[\"business_num_id\"] == prediction_matrix[user_id][j]].iloc[0].coordinates)\n",
    "#         test_point = Point(coordinateDict['latitude'],coordinateDict['longitude'])\n",
    "\n",
    "#         result = distance.distance(intersection,test_point).kilometers\n",
    "#         if result<=0.5:\n",
    "#             if isinstance(prediction_matrix, type(list)):\n",
    "#                 list.append(popular_list[j])\n",
    "#             else:\n",
    "#                 list.append(prediction_matrix[user_id][j])\n",
    "#         list = list[0:bus_indexRange]\n",
    "#     return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three locations for user input\n",
    "yonge_and_finch = Point(\"43.779824, -79.415665\")\n",
    "bloor_and_bathurst = Point(\"43.665194,-79.411208\")\n",
    "queen_and_spadina = Point(\"43.648772,-79.396259\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three locations for user recommendation\n",
    "bloor_and_yonge = Point(\"43.670409,-79.386814\")\n",
    "dundas_and_yonge = Point(\"43.6561,-79.3802\")\n",
    "spadina_and_dundas = Point(\"43.653004,-79.398082\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# bay_and_queens = Point(\"43.6518,-79.3802\")\n",
    "# king_and_jarvis = Point(\"43.650577,-79.371887\")\n",
    "\n",
    "# yonge_and_eglinton = Point(\"43.7064,-79.3986\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of percentage liked ratio \n",
    "list_popular_liked_ratio = popular_list_liked_ratio.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 items for each location for user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2655, 2682, 2584, 2931, 2807, 2182, 2063, 2354, 2360, 2255]"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yonge_and_finch_list = geographical_dist(list_popular_liked_ratio,yonge_and_finch,0, 10)\n",
    "yonge_and_finch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2678, 2482, 2490, 2497, 2571, 2567, 2731, 2910, 2963, 2946]"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloor_and_bathurst_list = geographical_dist(list_popular_liked_ratio,bloor_and_bathurst,0, 10)\n",
    "bloor_and_bathurst_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2637, 2643, 2644, 2648, 2654, 2662, 2649, 2632, 2608, 2624]"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queen_and_spadina_list = geographical_dist(list_popular_liked_ratio,queen_and_spadina,0, 10)\n",
    "queen_and_spadina_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list 1-3 demo for user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_num_id</th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>business_stars</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>2063</td>\n",
       "      <td>Pastel Creperies &amp; Dessert House</td>\n",
       "      <td>[{'alias': 'creperies', 'title': 'Creperies'},...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35939</th>\n",
       "      <td>2354</td>\n",
       "      <td>Cafe Bene</td>\n",
       "      <td>[{'alias': 'desserts', 'title': 'Desserts'}, {...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41602</th>\n",
       "      <td>2931</td>\n",
       "      <td>Hot Star Large Fried Chicken</td>\n",
       "      <td>[{'alias': 'chickenshop', 'title': 'Chicken Sh...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52211</th>\n",
       "      <td>2584</td>\n",
       "      <td>Sansotei Ramen</td>\n",
       "      <td>[{'alias': 'ramen', 'title': 'Ramen'}, {'alias...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64065</th>\n",
       "      <td>2255</td>\n",
       "      <td>Jung Soo Nae Restaurant</td>\n",
       "      <td>[{'alias': 'korean', 'title': 'Korean'}]</td>\n",
       "      <td>3.5</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83975</th>\n",
       "      <td>2682</td>\n",
       "      <td>Burrito Place</td>\n",
       "      <td>[{'alias': 'mexican', 'title': 'Mexican'}]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113360</th>\n",
       "      <td>2807</td>\n",
       "      <td>Daisy Nails and Spa</td>\n",
       "      <td>[{'alias': 'othersalons', 'title': 'Nail Salon...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139434</th>\n",
       "      <td>2655</td>\n",
       "      <td>Sang-Ji Fried Bao</td>\n",
       "      <td>[{'alias': 'noodles', 'title': 'Noodles'}, {'a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188960</th>\n",
       "      <td>2182</td>\n",
       "      <td>Go Topoki</td>\n",
       "      <td>[{'alias': 'korean', 'title': 'Korean'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192364</th>\n",
       "      <td>2360</td>\n",
       "      <td>Chicken in the Kitchen</td>\n",
       "      <td>[{'alias': 'chickenshop', 'title': 'Chicken Sh...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        business_num_id                              name  \\\n",
       "1802               2063  Pastel Creperies & Dessert House   \n",
       "35939              2354                         Cafe Bene   \n",
       "41602              2931      Hot Star Large Fried Chicken   \n",
       "52211              2584                    Sansotei Ramen   \n",
       "64065              2255           Jung Soo Nae Restaurant   \n",
       "83975              2682                     Burrito Place   \n",
       "113360             2807               Daisy Nails and Spa   \n",
       "139434             2655                 Sang-Ji Fried Bao   \n",
       "188960             2182                         Go Topoki   \n",
       "192364             2360            Chicken in the Kitchen   \n",
       "\n",
       "                                               categories  business_stars  \\\n",
       "1802    [{'alias': 'creperies', 'title': 'Creperies'},...             4.0   \n",
       "35939   [{'alias': 'desserts', 'title': 'Desserts'}, {...             3.5   \n",
       "41602   [{'alias': 'chickenshop', 'title': 'Chicken Sh...             3.5   \n",
       "52211   [{'alias': 'ramen', 'title': 'Ramen'}, {'alias...             4.0   \n",
       "64065            [{'alias': 'korean', 'title': 'Korean'}]             3.5   \n",
       "83975          [{'alias': 'mexican', 'title': 'Mexican'}]             3.0   \n",
       "113360  [{'alias': 'othersalons', 'title': 'Nail Salon...             4.0   \n",
       "139434  [{'alias': 'noodles', 'title': 'Noodles'}, {'a...             4.0   \n",
       "188960           [{'alias': 'korean', 'title': 'Korean'}]             4.0   \n",
       "192364  [{'alias': 'chickenshop', 'title': 'Chicken Sh...             3.5   \n",
       "\n",
       "        review_count  \n",
       "1802             316  \n",
       "35939             84  \n",
       "41602             52  \n",
       "52211             94  \n",
       "64065             46  \n",
       "83975             61  \n",
       "113360            92  \n",
       "139434            93  \n",
       "188960            90  \n",
       "192364             9  "
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_location_yonge_and_finch=df_location[df_location[\"business_num_id\"].isin(yonge_and_finch_list)].drop_duplicates(subset = 'business_num_id', keep = 'first')[[\"business_num_id\",\"name\",\"categories\",\"business_stars\",\"review_count_y\"]].rename(columns={'review_count_y': 'review_count'})\n",
    "df_location_yonge_and_finch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_num_id</th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>business_stars</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>2963</td>\n",
       "      <td>BMV Books</td>\n",
       "      <td>[{'alias': 'bookstores', 'title': 'Bookstores'...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99945</th>\n",
       "      <td>2571</td>\n",
       "      <td>Hodo Kwaja</td>\n",
       "      <td>[{'alias': 'bakeries', 'title': 'Bakeries'}, {...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105714</th>\n",
       "      <td>2946</td>\n",
       "      <td>Jin Dal Lae</td>\n",
       "      <td>[{'alias': 'korean', 'title': 'Korean'}]</td>\n",
       "      <td>4.5</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122426</th>\n",
       "      <td>2567</td>\n",
       "      <td>Fuzz Wax Bar</td>\n",
       "      <td>[{'alias': 'waxing', 'title': 'Waxing'}, {'ali...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126487</th>\n",
       "      <td>2910</td>\n",
       "      <td>PAT Central Market</td>\n",
       "      <td>[{'alias': 'grocery', 'title': 'Grocery'}, {'a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133705</th>\n",
       "      <td>2490</td>\n",
       "      <td>Sora Beauty Salon</td>\n",
       "      <td>[{'alias': 'hair', 'title': 'Hair Salons'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134665</th>\n",
       "      <td>2731</td>\n",
       "      <td>KINTON RAMEN</td>\n",
       "      <td>[{'alias': 'ramen', 'title': 'Ramen'}]</td>\n",
       "      <td>3.5</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154335</th>\n",
       "      <td>2482</td>\n",
       "      <td>Freezone JaYuGongGan Karaoke</td>\n",
       "      <td>[{'alias': 'karaoke', 'title': 'Karaoke'}]</td>\n",
       "      <td>3.5</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181795</th>\n",
       "      <td>2678</td>\n",
       "      <td>El Pocho Antojitos Bar</td>\n",
       "      <td>[{'alias': 'bars', 'title': 'Bars'}, {'alias':...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195825</th>\n",
       "      <td>2497</td>\n",
       "      <td>El Furniture Warehouse</td>\n",
       "      <td>[{'alias': 'tradamerican', 'title': 'American ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        business_num_id                          name  \\\n",
       "6915               2963                     BMV Books   \n",
       "99945              2571                    Hodo Kwaja   \n",
       "105714             2946                   Jin Dal Lae   \n",
       "122426             2567                  Fuzz Wax Bar   \n",
       "126487             2910            PAT Central Market   \n",
       "133705             2490             Sora Beauty Salon   \n",
       "134665             2731                  KINTON RAMEN   \n",
       "154335             2482  Freezone JaYuGongGan Karaoke   \n",
       "181795             2678        El Pocho Antojitos Bar   \n",
       "195825             2497        El Furniture Warehouse   \n",
       "\n",
       "                                               categories  business_stars  \\\n",
       "6915    [{'alias': 'bookstores', 'title': 'Bookstores'...             4.0   \n",
       "99945   [{'alias': 'bakeries', 'title': 'Bakeries'}, {...             4.5   \n",
       "105714           [{'alias': 'korean', 'title': 'Korean'}]             4.5   \n",
       "122426  [{'alias': 'waxing', 'title': 'Waxing'}, {'ali...             4.0   \n",
       "126487  [{'alias': 'grocery', 'title': 'Grocery'}, {'a...             4.0   \n",
       "133705        [{'alias': 'hair', 'title': 'Hair Salons'}]             4.0   \n",
       "134665             [{'alias': 'ramen', 'title': 'Ramen'}]             3.5   \n",
       "154335         [{'alias': 'karaoke', 'title': 'Karaoke'}]             3.5   \n",
       "181795  [{'alias': 'bars', 'title': 'Bars'}, {'alias':...             4.0   \n",
       "195825  [{'alias': 'tradamerican', 'title': 'American ...             3.0   \n",
       "\n",
       "        review_count  \n",
       "6915             140  \n",
       "99945            188  \n",
       "105714            42  \n",
       "122426            40  \n",
       "126487            83  \n",
       "133705            59  \n",
       "134665           111  \n",
       "154335            27  \n",
       "181795            47  \n",
       "195825           292  "
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_location_bloor_and_bathurst_list=df_location[df_location[\"business_num_id\"].isin(bloor_and_bathurst_list)].drop_duplicates(subset = 'business_num_id', keep = 'first')[[\"business_num_id\",\"name\",\"categories\",\"business_stars\",\"review_count_y\"]].rename(columns={'review_count_y': 'review_count'})\n",
    "df_location_bloor_and_bathurst_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_num_id</th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>business_stars</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50478</th>\n",
       "      <td>2644</td>\n",
       "      <td>LCBO</td>\n",
       "      <td>[{'alias': 'beer_and_wine', 'title': 'Beer, Wi...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71107</th>\n",
       "      <td>2643</td>\n",
       "      <td>Gabby's Grill and Bar</td>\n",
       "      <td>[{'alias': 'newcanadian', 'title': 'Canadian (...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74469</th>\n",
       "      <td>2637</td>\n",
       "      <td>Carver</td>\n",
       "      <td>[{'alias': 'sandwiches', 'title': 'Sandwiches'...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99038</th>\n",
       "      <td>2654</td>\n",
       "      <td>R&amp;D Restaurant</td>\n",
       "      <td>[{'alias': 'asianfusion', 'title': 'Asian Fusi...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114661</th>\n",
       "      <td>2624</td>\n",
       "      <td>La Palette</td>\n",
       "      <td>[{'alias': 'french', 'title': 'French'}, {'ali...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130424</th>\n",
       "      <td>2608</td>\n",
       "      <td>The Porch Toronto</td>\n",
       "      <td>[{'alias': 'tradamerican', 'title': 'American ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177237</th>\n",
       "      <td>2632</td>\n",
       "      <td>The Fifth &amp; Terrace</td>\n",
       "      <td>[{'alias': 'french', 'title': 'French'}, {'ali...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181310</th>\n",
       "      <td>2662</td>\n",
       "      <td>Juicy Dumpling</td>\n",
       "      <td>[{'alias': 'chinese', 'title': 'Chinese'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187596</th>\n",
       "      <td>2648</td>\n",
       "      <td>b.good</td>\n",
       "      <td>[{'alias': 'salad', 'title': 'Salad'}, {'alias...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189428</th>\n",
       "      <td>2649</td>\n",
       "      <td>Hong Kong Bistro Cafe</td>\n",
       "      <td>[{'alias': 'chinese', 'title': 'Chinese'}, {'a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        business_num_id                   name  \\\n",
       "50478              2644                   LCBO   \n",
       "71107              2643  Gabby's Grill and Bar   \n",
       "74469              2637                 Carver   \n",
       "99038              2654         R&D Restaurant   \n",
       "114661             2624             La Palette   \n",
       "130424             2608      The Porch Toronto   \n",
       "177237             2632    The Fifth & Terrace   \n",
       "181310             2662         Juicy Dumpling   \n",
       "187596             2648                 b.good   \n",
       "189428             2649  Hong Kong Bistro Cafe   \n",
       "\n",
       "                                               categories  business_stars  \\\n",
       "50478   [{'alias': 'beer_and_wine', 'title': 'Beer, Wi...             4.0   \n",
       "71107   [{'alias': 'newcanadian', 'title': 'Canadian (...             2.5   \n",
       "74469   [{'alias': 'sandwiches', 'title': 'Sandwiches'...             3.0   \n",
       "99038   [{'alias': 'asianfusion', 'title': 'Asian Fusi...             3.5   \n",
       "114661  [{'alias': 'french', 'title': 'French'}, {'ali...             4.0   \n",
       "130424  [{'alias': 'tradamerican', 'title': 'American ...             3.0   \n",
       "177237  [{'alias': 'french', 'title': 'French'}, {'ali...             4.0   \n",
       "181310         [{'alias': 'chinese', 'title': 'Chinese'}]             4.0   \n",
       "187596  [{'alias': 'salad', 'title': 'Salad'}, {'alias...             3.5   \n",
       "189428  [{'alias': 'chinese', 'title': 'Chinese'}, {'a...             4.0   \n",
       "\n",
       "        review_count  \n",
       "50478             31  \n",
       "71107            120  \n",
       "74469             45  \n",
       "99038            216  \n",
       "114661           215  \n",
       "130424           125  \n",
       "177237            55  \n",
       "181310            73  \n",
       "187596            36  \n",
       "189428            87  "
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_location_queen_and_spadina_list=df_location[df_location[\"business_num_id\"].isin(queen_and_spadina_list)].drop_duplicates(subset = 'business_num_id', keep = 'first')[[\"business_num_id\",\"name\",\"categories\",\"business_stars\",\"review_count_y\"]].rename(columns={'review_count_y': 'review_count'})\n",
    "df_location_queen_and_spadina_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce the list of restaurants close to the set location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial setting, set for user Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three locations for user recommendation\n",
    "intersection = [bloor_and_yonge,dundas_and_yonge,spadina_and_dundas]\n",
    "businessIndexRange = 3\n",
    "prediction_matrix = {}\n",
    "\n",
    "\n",
    "# manually enter this number\n",
    "userIndex = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First list\n",
    "Final_UI_predict_Explicit = geographical_dist(UI_predict_Explicit,intersection[0],userIndex,businessIndexRange)\n",
    "\n",
    "#Second list\n",
    "Final_UI_predict_Implicit = geographical_dist(UI_predict_Implicit,intersection[0],userIndex,businessIndexRange)\n",
    "\n",
    "#Third list\n",
    "Final_UI_predict_IECombined = geographical_dist(UI_predict_IECombined,intersection[0],userIndex,businessIndexRange)\n",
    "\n",
    "#Fourth lsit\n",
    "Final_UI_predict_popular = geographical_dist(popular_list,intersection[0],userIndex,businessIndexRange)\n",
    "\n",
    "#Five list\n",
    "Final_UI_predict_max = geographical_dist(UI_predict_max,intersection[0],userIndex,businessIndexRange)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1243, 1238, 1541]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_UI_predict_popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2913.0, 621.0]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_IU_predict_Explicit = geographical_dist(IU_predict_Explicit,intersection[0],0,businessIndexRange)\n",
    "Final_IU_predict_Explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1243, 1238, 1541]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_UI_predict_popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = [UI_predict_Implicit, popular_list, UI_predict_IECombined, IU_predict_Explicit, Item_predict_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [[] for i in range(len(metric))]\n",
    "for i in range(len(intersection)):\n",
    "    for j in range(len(metric)):\n",
    "        temp = geographical_dist(metric[j],intersection[i],userIndex,businessIndexRange)\n",
    "        res[j] += temp\n",
    "#     #First list\n",
    "#     temp_UI_predict_Implicit = geographical_dist(UI_predict_Implicit,intersection[i],userIndex,businessIndexRange)\n",
    "#     list_UI_predict_Implicit += temp_UI_predict_Implicit\n",
    "    \n",
    "#     #Second list\n",
    "#     temp_UI_predict_popular = geographical_dist(popular_list,intersection[i],userIndex,businessIndexRange)\n",
    "#     list_UI_predict_popular += temp_UI_predict_popular\n",
    "    \n",
    "#     #Third list\n",
    "#     temp_UI_predict_IECombined = geographical_dist(UI_predict_IECombined,intersection[i],userIndex,businessIndexRange)\n",
    "#     list_UI_predict_IECombined += temp_UI_predict_IECombined\n",
    "    \n",
    "#     #Fourth list\n",
    "#     temp_IU_predict_Explicit = geographical_dist(IU_predict_Explicit,intersection[i],userIndex,businessIndexRange)\n",
    "#     list_IU_predict_Explicit += temp_IU_predict_Explicit\n",
    "    \n",
    "#     temp_UI_predict_Explicit = geographical_dist(UI_predict_Explicit,intersection[i],userIndex,businessIndexRange)\n",
    "#     list_UI_predict_Explicit += temp_UI_predict_Explicit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2560.0, 621.0, 3326.0, 2741.0, 2713.0, 3615.0, 2726.0, 2517.0, 3155.0],\n",
       " [1243, 1238, 2730, 1787, 1790, 41, 2926, 1480, 1536],\n",
       " [2560.0, 621.0, 3025.0, 2741.0, 1790.0, 2713.0, 964.0, 2517.0, 2726.0],\n",
       " [2913.0, 621.0, 881.0, 3010.0, 3214.0, 1887.0, 1246.0, 2193.0],\n",
       " [14.0, 3036.0, 3615.0, 1866.0, 1812.0, 628.0]]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_final = []\n",
    "for element in range(len(res)):\n",
    "    dic = {}\n",
    "    for i in range(len(res[element])):\n",
    "        if isinstance(metric[element], list):\n",
    "            dic[metric[element].index(res[element][i])] = res[element][i]\n",
    "        else:\n",
    "            dic[metric[element][userIndex].tolist().index(res[element][i])] = res[element][i]\n",
    "    temp = []\n",
    "    for j in sorted(dic.keys()):\n",
    "        temp.append(dic[j])\n",
    "    res_final.append(temp[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2741.0, 2713.0, 2726.0],\n",
       " [1787, 2926, 1480],\n",
       " [964.0, 2741.0, 2517.0],\n",
       " [1887.0, 1246.0, 2193.0],\n",
       " [14.0, 1866.0, 3615.0]]"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match restaurant information according to business_num_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructResDictionary(userIndex, busIndexRange, UI_prediction):\n",
    "    dictionaryToConstruct = {}\n",
    "    #Loop through the number of businesses \n",
    "    for busIndex in range(busIndexRange):\n",
    "        businessSeries = df[df[\"business_num_id\"] == UI_prediction[busIndex]].iloc[0]\n",
    "        busName = businessSeries['name']\n",
    "        address_generator = (str(w) for w in yaml.safe_load(businessSeries.location)['display_address'])\n",
    "        busLocation = ', '.join(address_generator)\n",
    "        bus_Price = businessSeries.price\n",
    "        busStars = businessSeries.business_stars\n",
    "        busReviewCount = businessSeries.review_count_y \n",
    "        category_generator = (str(s) for s in [cateDict['title'] for cateDict in yaml.safe_load(businessSeries.categories)])\n",
    "        #busCategories = [cateDict['title'] for cateDict in yaml.safe_load(businessSeries.categories)]\n",
    "        busCategories = ', '.join(category_generator)\n",
    "        #Now add the restaurant to the dictionary\n",
    "        dictionaryToConstruct[busName] = {'Address': busLocation,\\\n",
    "                                'Price': bus_Price,\\\n",
    "                                 'Star': busStars, \\\n",
    "                                 'Review Count': busReviewCount, \\\n",
    "                                 'Category': busCategories}\n",
    "    return dictionaryToConstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to recommend for user 0 for now \n",
    "\n",
    "\n",
    "dict1_ExplicitRecommend = {}\n",
    "dict2_ImplicitRecommend = {}\n",
    "dict3_EICombineRecommend = {}\n",
    "dict4_PopularityRecommend = {}\n",
    "dict5_maxRecommend = {}\n",
    "#User predict [user index] [item index]\n",
    "#Add a for loop for the top recommended items\n",
    "\n",
    "dict1_ExplicitRecommend = constructResDictionary(userIndex, businessIndexRange, UI_predict_Explicit)\n",
    "dict2_ImplicitRecommend = constructResDictionary(userIndex, businessIndexRange, UI_predict_Implicit)\n",
    "dict3_EICombineRecommend = constructResDictionary(userIndex, businessIndexRange, UI_predict_IECombined)\n",
    "dict4_PopularityRecommend = constructResDictionary(userIndex, businessIndexRange, UI_predict_popular)\n",
    "dict5_maxRecommend = constructResDictionary(userIndex, businessIndexRange, UI_predict_max)\n",
    "\n",
    "#Say we are recommending 5 restaurants for now\n",
    "# for busIndex in range(5):\n",
    "#     businessSeries = df[df[\"business_num_id\"] == UI_predict_Explicit[0][busIndex]].iloc[0]\n",
    "#     busName = businessSeries['name']\n",
    "#     address_generator = (str(w) for w in yaml.safe_load(businessSeries.location)['display_address'])\n",
    "#     busLocation = ', '.join(address_generator)\n",
    "#     bus_Price = businessSeries.price\n",
    "#     busStars = businessSeries.business_stars\n",
    "#     busReviewCount = businessSeries.review_count_y \n",
    "#     category_generator = (str(s) for s in [cateDict['title'] for cateDict in yaml.safe_load(businessSeries.categories)])\n",
    "#     #busCategories = [cateDict['title'] for cateDict in yaml.safe_load(businessSeries.categories)]\n",
    "#     busCategories = ', '.join(category_generator)\n",
    "#     #Now add the restaurant to the dictionary\n",
    "#     dict1stAlgo[busName] = {'Address': busLocation,\\\n",
    "#                             'Price': bus_Price,\\\n",
    "#                              'Star': busStars, \\\n",
    "#                              'Review Count': busReviewCount, \\\n",
    "#                              'Category': busCategories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiListbox(Frame):\n",
    "    \n",
    "    def __init__(self, master, lists):\n",
    "        Frame.__init__(self, master)\n",
    "        self.lists = []\n",
    "        \n",
    "        #Loop through the lists\n",
    "        for l,widthW in lists:\n",
    "            \n",
    "            frame = Frame(self); frame.pack(side=LEFT, expand=YES, fill=BOTH)\n",
    "            \n",
    "            Label(frame, text=l, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "            \n",
    "            lb = Listbox(frame, width=widthW, borderwidth=0, selectborderwidth=0,\n",
    "                 relief=FLAT, exportselection=FALSE)\n",
    "            lb.pack(expand=YES, fill=BOTH)\n",
    "            \n",
    "            self.lists.append(lb)\n",
    "            lb.bind('<B1-Motion>', lambda e, s=self: s._select(e.y))\n",
    "            lb.bind('<Button-1>', lambda e, s=self: s._select(e.y))\n",
    "            \n",
    "            lb.bind('<Leave>', lambda e: 'break')\n",
    "            \n",
    "            lb.bind('<B2-Motion>', lambda e, s=self: s._b2motion(e.x, e.y))\n",
    "            lb.bind('<Button-2>', lambda e, s=self: s._button2(e.x, e.y))\n",
    "            \n",
    "        frame = Frame(self); frame.pack(side=LEFT, fill=Y)\n",
    "        \n",
    "        Label(frame, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "        \n",
    "        #Setting a scrollbar \n",
    "#         sb = Scrollbar(frame, orient=VERTICAL, command=self._scroll)\n",
    "        \n",
    "#         sb.pack(expand=YES, fill=Y)\n",
    "        \n",
    "#         self.lists[0]['yscrollcommand']=sb.set\n",
    "\n",
    "    def _select(self, y):\n",
    "        row = self.lists[0].nearest(y)\n",
    "        self.selection_clear(0, END)\n",
    "        self.selection_set(row)\n",
    "        return 'break'\n",
    "\n",
    "    def _button2(self, x, y):\n",
    "        for l in self.lists: l.scan_mark(x, y)\n",
    "        return 'break'\n",
    "\n",
    "    def _b2motion(self, x, y):\n",
    "        for l in self.lists: l.scan_dragto(x, y)\n",
    "        return 'break'\n",
    "\n",
    "    def _scroll(self, *args):\n",
    "        for l in self.lists:\n",
    "            apply(l.yview, args)\n",
    "\n",
    "    def curselection(self):\n",
    "        return self.lists[0].curselection()\n",
    "\n",
    "    def delete(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.delete(first, last)\n",
    "\n",
    "#     def get(self, first, last=None):\n",
    "#         result = []\n",
    "#         for l in self.lists:\n",
    "#             result.append(l.get(first,last))\n",
    "#         if last: return apply(map, [None] + result)\n",
    "#         return result\n",
    "\n",
    "    def index(self, index):\n",
    "        self.lists[0].index(index)\n",
    "\n",
    "    def insert(self, index, *elements):\n",
    "        #Loop through the elements \n",
    "        for element in elements:\n",
    "            i = 0\n",
    "            for l in self.lists:\n",
    "                l.insert(index, element[i])\n",
    "                i = i + 1\n",
    "\n",
    "    def size(self):\n",
    "        return self.lists[0].size()\n",
    "\n",
    "    def see(self, index):\n",
    "        for l in self.lists:\n",
    "            l.see(index)\n",
    "\n",
    "    def selection_anchor(self, index):\n",
    "        for l in self.lists:\n",
    "            l.selection_anchor(index)\n",
    "\n",
    "    def selection_clear(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.selection_clear(first, last)\n",
    "\n",
    "    def selection_includes(self, index):\n",
    "        return self.lists[0].selection_includes(index)\n",
    "\n",
    "    def selection_set(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.selection_set(first, last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #Initiate a frame or master? \n",
    "    tk = Tk()\n",
    "    Label(tk, text='List of recommended restaurants').pack()\n",
    "    \n",
    "    #Creating the multi-listbox object, pass in a tuple of tuple (lists)\n",
    "    mlb = MultiListbox(tk, (('Explicit User-Ratings', 20), ('Implicit User-Ratings', 20), \\\n",
    "                            ('Explicit Implicit Combined', 20), ('Popularity List', 20), ('Taking Max', 20)))\n",
    "    \n",
    "    for index in range(len(dict1_ExplicitRecommend)):\n",
    "        #First restaurant information to display \n",
    "        #Following the restaurants' name \n",
    "        restList1 = list(dict1_ExplicitRecommend.keys())[index]\n",
    "        restList2 = list(dict2_ImplicitRecommend.keys())[index]\n",
    "        restList3 = list(dict3_EICombineRecommend.keys())[index]\n",
    "        restList4 = list(dict4_PopularityRecommend.keys())[index]\n",
    "        restList5 = list(dict5_maxRecommend.keys())[index]\n",
    "        \n",
    "        mlb.insert(END, (' ', ' ', ' ', ' ', ' '))\n",
    "        \n",
    "        #Inserting the restaurant names\n",
    "        mlb.insert(END, ('%d: %s' % (index + 1, restList1),'%d: %s' % (index + 1, restList2),\n",
    "                         '%d: %s' % (index + 1, restList3),'%d: %s' % (index + 1, restList4),\n",
    "                         '%d: %s' % (index + 1, restList5)))\n",
    "        \n",
    "        #Looping through each attribute keys - resinfo\n",
    "        for resinfo in dict1_ExplicitRecommend.get(restList1).keys():\n",
    "            restList1Info = resinfo + ':' + str(dict1_ExplicitRecommend.get(restList1).get(resinfo,''))\n",
    "            restList2Info = resinfo + ':' + str(dict2_ImplicitRecommend.get(restList2).get(resinfo,''))\n",
    "            restList3Info = resinfo + ':' + str(dict3_EICombineRecommend.get(restList3).get(resinfo,''))\n",
    "            restList4Info = resinfo + ':' + str(dict4_PopularityRecommend.get(restList4).get(resinfo,''))\n",
    "            restList5Info = resinfo + ':' + str(dict5_maxRecommend.get(restList5).get(resinfo,''))\n",
    "            \n",
    "            mlb.insert(END, (restList1Info, restList2Info, restList3Info, restList4Info, restList5Info))\n",
    "        \n",
    "        mlb.insert(END, ('----------------', '----------------', '----------------', '----------------', '----------------'))\n",
    "    \n",
    "        \n",
    "    mlb.pack(expand=YES,fill=BOTH)\n",
    "    tk.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "similarity1 = train(rtrain_implicit)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "user_item_prediction_score1 = predict(rtrain_implicit, 90, similarity1, item_similarity_en= False)\n",
    "user_item_predict1 = prediction(user_item_prediction_score1, 50, rtrain_implicit)\n",
    "user_item_res1 = evaluate(user_item_predict1, rvalid_implicit)\n",
    "user_item_res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rvalid_implicit.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine implicit and explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "similarity1 = train(rtrain_implicit)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "user_item_prediction_score1 = predict(rtrain, 90, similarity1, item_similarity_en= False)\n",
    "user_item_predict1 = prediction(user_item_prediction_score1, 50, rtrain)\n",
    "user_item_res1 = evaluate(user_item_predict1, rvalid)\n",
    "user_item_res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. With ratings that subtracts user average rating, pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #UU similarity, using cosine similarity\n",
    "# similarity2 = train(rtrain_userAvg)\n",
    "# #get a user-item matrix  UI prediction\n",
    "# #Predict using UI matrix with ratings in it \n",
    "# user_item_prediction_score2 = predict(rtrain_userAvg, 100, similarity2, item_similarity_en= False)\n",
    "# user_item_predict2 = prediction(user_item_prediction_score2, 50, rtrain_userAvg)\n",
    "# user_item_res2 = evaluate(user_item_predict2, rvalid_userAvg)\n",
    "# user_item_res2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. With raw ratings, cosinesimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UU similarity\n",
    "similarity3 = train(rtrain)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "user_item_prediction_score3 = predict(rtrain, 90, similarity3, item_similarity_en= False)\n",
    "user_item_predict3 = prediction(user_item_prediction_score3, 50, rtrain)\n",
    "#Check user item prediction score\n",
    "user_item_res3 = evaluate(user_item_predict3, rvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Base KNN using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "IK_MATRIX = X_cleaned_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3996/3996 [00:03<00:00, 945.72it/s]\n",
      "100%|| 6099/6099 [00:01<00:00, 4045.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# IK_MATRIX = X_cleaned_sparse\n",
    "I_I_similarity = train(IK_MATRIX)\n",
    "item_based_prediction_score4 = predict(rtrain, 10, I_I_similarity, item_similarity_en= True)\n",
    "#for each restuarant top50 users \n",
    "item_based_predict4 = prediction(item_based_prediction_score4, 50, rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1126.,   24.,   14., ..., 3184., 3679., 3311.],\n",
       "       [2497., 3575., 2527., ...,  766., 2970., 3245.],\n",
       "       [2173., 3368.,   88., ...,  482.,  212., 1533.],\n",
       "       ...,\n",
       "       [3813.,  430., 2724., ..., 2576., 3562., 3269.],\n",
       "       [2411., 2258., 1697., ...,  533., 2727., 2231.],\n",
       "       [3393.,  245., 3751., ..., 2646., 2645., 2644.]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_based_predict4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6099/6099 [00:00<00:00, 7054.56it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 7082.02it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 6810.92it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 6913.74it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 6842.57it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 9082.67it/s]\n"
     ]
    }
   ],
   "source": [
    "item_based_res_TFIDF = evaluate(item_based_predict4, rvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP@5': (0.0147, 0.0018),\n",
       " 'MAP@10': (0.0131, 0.0013),\n",
       " 'MAP@15': (0.0122, 0.0011),\n",
       " 'MAP@20': (0.0115, 0.0009),\n",
       " 'MAP@50': (0.0094, 0.0006)}"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_based_res_TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Base KNN using TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<91381x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5019585 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 91381/91381 [00:11<00:00, 8095.33it/s]\n",
      "100%|| 3996/3996 [00:03<00:00, 1014.62it/s]\n",
      "100%|| 6099/6099 [00:01<00:00, 4013.77it/s]\n"
     ]
    }
   ],
   "source": [
    "#3987 items? 5000 key phrase\n",
    "I_K_matrix = get_I_K(df_train, X, shape = (rating_matrix.shape[1], 5000))\n",
    "I_I_similarity = train(I_K_matrix)\n",
    "item_based_prediction_score = predict(rtrain, 10, I_I_similarity, item_similarity_en= True)\n",
    "#for each restuarant top50 users \n",
    "item_based_predict = prediction(item_based_prediction_score, 50, rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6099/6099 [00:00<00:00, 6994.88it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 6947.13it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 6852.69it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 6809.13it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 6779.62it/s]\n",
      "100%|| 6099/6099 [00:00<00:00, 8955.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@5': (0.0172, 0.0019),\n",
       " 'MAP@10': (0.0154, 0.0014),\n",
       " 'MAP@15': (0.0143, 0.0012),\n",
       " 'MAP@20': (0.0134, 0.001),\n",
       " 'MAP@50': (0.0108, 0.0007)}"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_based_res = evaluate(item_based_predict, rvalid)\n",
    "item_based_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item based KNN with IC Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_C_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_I_C = train(I_C_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_item_prediction_score_I_C = predict(e\n",
    "                                         , 90, similarity_I_C, item_similarity_en= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each restuarant top50 users \n",
    "item_based_predict_I_C = prediction(user_item_prediction_score_I_C, 50, rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_based_res_I_C = evaluate(item_based_predict_I_C, rvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item_based_res_I_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User based KNN with explicit UC Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_C_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain_implicit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_C_matrix = rtrain_implicit*I_C_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_UC = train(U_C_matrix)\n",
    "user_item_prediction_score_UC = predict(rtrain, 90, similarity_UC, item_similarity_en= False)\n",
    "user_item_predict_UC = prediction(user_item_prediction_score_UC, 90,rtrain)\n",
    "user_item_res_UC = evaluate(user_item_predict_UC, rvalid)\n",
    "user_item_res_UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_item_res_UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User based KNN with implicit UC Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_C_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImplicitMatrix(sparseMatrix, threashold=0):\n",
    "    temp_matrix = sparse.csr_matrix(sparseMatrix.shape)\n",
    "    temp_matrix[(sparseMatrix > threashold).nonzero()] = 1\n",
    "    return temp_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_C_matrix_implicit = getImplicitMatrix(U_C_matrix,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_C_matrix_implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_UC_implicit = train(U_C_matrix_implicit)\n",
    "user_item_prediction_score_UC_implicit = predict(rtrain, 90, similarity_UC_implicit, item_similarity_en= False)\n",
    "user_item_predict_UC_implicit = prediction(user_item_prediction_score_UC_implicit, 90,rtrain)\n",
    "user_item_res_UC_implicit = evaluate(user_item_predict_UC_implicit, rvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res_UC_implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try combining different similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a UI matrix if it's not item_similarity based or else IU\n",
    "def predictUU(matrix_train, k, similarity1, similarity2=None, similarity3=None, similarity4=None, weight1=None, weight2=None, weight3=None, weight4=None, chooseWeigthMethod = None, item_similarity_en = False):\n",
    "    prediction_scores = []\n",
    "    \n",
    "    #inverse to IU matrix\n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    #for each user or item, depends UI or IU \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "    #for user_index in tqdm(range(1)):\n",
    "        \n",
    "        numberSimilarMatrix = 0\n",
    "        # Get user u's prediction scores for all items \n",
    "        #Get prediction/similarity score for each user 1*num or user or num of items\n",
    "        vector_u1 = similarity1[user_index]\n",
    "        numberSimilarMatrix += 1\n",
    "        \n",
    "        if similarity2 is not None:\n",
    "            vector_u2 = similarity2[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u2 = [0]*len(vector_u1)\n",
    "            \n",
    "        if similarity3 is not None:\n",
    "            vector_u3 = similarity3[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u3 = [0]*len(vector_u1)\n",
    "            \n",
    "        if similarity4 is not None:\n",
    "            vector_u4 = similarity4[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u4 = [0]*len(vector_u1)\n",
    "        \n",
    "        vector_u = vector_u1.copy()\n",
    "            \n",
    "        #If we are choosing the max, min, or avg or similarity scores\n",
    "        if chooseWeigthMethod is not None:\n",
    "            #loop through the user index \n",
    "            for item_index in tqdm(range(matrix_train.shape[0])):\n",
    "\n",
    "                if chooseWeigthMethod == 'max':\n",
    "                    vector_u[item_index] = max(vector_u1[item_index], vector_u2[item_index], vector_u3[item_index],vector_u4[item_index])\n",
    "                elif chooseWeigthMethod == 'min':\n",
    "                    vector_u[item_index] = min(vector_u1[item_index], vector_u2[item_index], vector_u3[item_index],vector_u4[item_index])\n",
    "                elif chooseWeigthMethod == 'average':\n",
    "                    vector_u[item_index] = (vector_u1[item_index]+vector_u2[item_index]+vector_u3[item_index]+vector_u4[item_index])/(numberSimilarMatrix)\n",
    "        \n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        \n",
    "        # Get neighbors similarity weights and ratings\n",
    "        #similar_users_weights = similarity1[user_index][similar_users]\n",
    "        similar_users_weights = vector_u[similar_users]\n",
    "        \n",
    "        #similar_users_weights_sum = np.sum(similar_users_weights)\n",
    "        #print(similar_users_weights.shape)\n",
    "        #shape: num of res * k\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "              \n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "        #print(prediction_scores_u)\n",
    "        \n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "        \n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    return res\n",
    "    #return vector_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "similarity1 = train(rtrain)\n",
    "#similarity2 = train(rtrain_userAvg)\n",
    "similarity2 = None\n",
    "#similarity3 =train(userVisitMatrix)\n",
    "similarity3 = None\n",
    "similarity4 = train(rtrain_implicit)\n",
    "#similarity4 = None\n",
    "weight1 = None\n",
    "weight2=None\n",
    "weight3=None\n",
    "#vectorU = predictUU(rtrain_userAvg, 90, similarity1, similarity2, similarity3, weight1, weight2, weight3, item_similarity_en= False)\n",
    "user_item_prediction_score1 = predictUU(rtrain, 90, similarity1, similarity2, similarity3, similarity4, weight1, weight2, weight3, chooseWeigthMethod = 'max', item_similarity_en= False)\n",
    "user_item_predict1 = prediction(user_item_prediction_score1, 50, rtrain)\n",
    "user_item_res1 = evaluate(user_item_predict1, rvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1   #1,4, max rtrain,rvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1   #1,2,3,4 max rtrain,rvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1  #1,2,3,4, average  rtrain, rvalid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1  #1,4 average rtrain_implicit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1  #1,4 average, rtrain_userAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1  #1,4 average rtrain, rvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_item_res1    #1,4 average with userVisitMatrix, rvalid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1 #average 1,4 similarity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1 #average 1,2,4 similarity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_item_res1    #average 1-2 similarity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1   #average 3 1-3similarity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1  #average 4, 1-4similarity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1   #min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1 #max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1  #rtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res1 #rtrain_userAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_prediction_score1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[3764   36  896 5156 5737  618 2345   11 1706 1554  726  627 5821 3348\n",
    "  243 1571 1247 5209  203 4856 3754  544 5417 3849 2411 1763  274 5211\n",
    "   91 2558 4208 4063 4712 3520  457 2900  387 2455 1178 3024 1342 6014\n",
    " 2823 5848 4691 5099 5109 5545 3987 6069 5794 5572   77 3180 3266 5039\n",
    " 5904 2325 3628  493 5735   27 4797 4813 3128 5337 4845 3907 3544 2128\n",
    "  320  134 1985 3470 5944 3188 2438 4850 2630 3912 1245 1674 5986 2360\n",
    " 5094 3670 5902 1491 3997 3856]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[3026 4208 5069 5507 1571 3529  544 3090 2538 4797 3527 1558 6079 5736\n",
    "   13 5209 5363 1866 3640 5684 1062 5213 5757 5417 5986 1466 5902 3109\n",
    " 4096  464  493  629 3912 3814 1268 3552 2438 5109 1207 4504 5634 4870\n",
    " 5229  320 4354  785 3167 2416 3128 4540 5457 5420 1241 1674 4850 5140\n",
    "   15  457  710 1245  942 5147 5164 4314 4346 3648 3180 3103 3189 1357\n",
    " 4917 3987 5571 2906  210 2489  901 2293 5737  879 2558  564 4310 2119\n",
    " 5521 5059 2376 3856 1967 4094]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number in rtrain_userAvg:\n",
    "    print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain_userAvg[0][0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain_userAvg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImplicitMatrix(sparseMatrix, threashold=0):\n",
    "    temp_matrix = sparse.csr_matrix(sparseMatrix.shape)\n",
    "    temp_matrix[(sparseMatrix > threashold).nonzero()] = 1\n",
    "    return temp_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "userVisitMatrix = getImplicitMatrix(rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UU similarity\n",
    "similarity3 = train(userVisitMatrix)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "user_item_prediction_score3 = predict(userVisitMatrix, 90, similarity3, item_similarity_en= False)\n",
    "user_item_predict3 = prediction(user_item_prediction_score3, 50, userVisitMatrix)\n",
    "#Check user item prediction score\n",
    "user_item_res3 = evaluate(user_item_predict3, rvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_index in tqdm(range(2)):\n",
    "    print(user_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userVisitMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(5,8,9,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.mean([1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([5,10,2], [10,5,9], [2,5,13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
