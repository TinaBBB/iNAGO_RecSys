{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Downloading https://files.pythonhosted.org/packages/80/93/d384479da0ead712bdaf697a8399c13a9a89bd856ada5a27d462fb45e47b/geopy-1.20.0-py2.py3-none-any.whl (100kB)\n",
      "Collecting geographiclib<2,>=1.49\n",
      "  Downloading https://files.pythonhosted.org/packages/8b/62/26ec95a98ba64299163199e95ad1b0e34ad3f4e176e221c40245f211e425/geographiclib-1.50-py3-none-any.whl\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-1.50 geopy-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.sparse import csr_matrix, load_npz, save_npz\n",
    "from tqdm import tqdm\n",
    "#from scipy import integrate\n",
    "import statistics as stats\n",
    "#import seaborn as sns\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import yaml\n",
    "import scipy.sparse as sparse\n",
    "import sys\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy import distance\n",
    "from geopy import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewJson = \"..\\\\data\\\\Export_CleanedReview.json\"\n",
    "#reviewJsonWithClosedRes = \"..\\\\data\\\\Export_CleanedReviewWithClosedRes.json\"\n",
    "#reviewJsonToronto = \"..\\\\data\\\\Export_TorontoData.json\"\n",
    "reviewJsonToronto = \"..\\\\data\\\\Cleaned_Toronto_Reviews.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Select top frenquent user and top frequenty restaurants that had at least 1 review >= 4 stars (Kickking out users that gave all  reviews <=3 and restaurants that never got start >= 4 stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yelp_df(path = 'data/', filename = 'Export_CleanedReview.json', sampling=False, top_user_num=7000, top_item_num=5000):\n",
    "    \"\"\"\n",
    "    Get the pandas dataframe\n",
    "    Sampling only the top users/items by density \n",
    "    Implicit representation applies\n",
    "    \"\"\"\n",
    "    with open(filename,'r') as f:\n",
    "        data = f.readlines()\n",
    "        data = list(map(json.loads, data))\n",
    "    \n",
    "    data = data[0]\n",
    "    #Get all the data from the dggeata file\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df.rename(columns={'stars': 'review_stars', 'text': 'review_text', 'cool': 'review_cool',\n",
    "                       'funny': 'review_funny', 'useful': 'review_useful'},\n",
    "              inplace=True)\n",
    "\n",
    "    df['business_num_id'] = df.business_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.business_id.nunique()))\n",
    "    df['business_num_id'] = df['business_num_id'].astype('int')\n",
    "\n",
    "    df['user_num_id'] = df.user_id.astype('category').\\\n",
    "    cat.rename_categories(range(0, df.user_id.nunique()))\n",
    "    df['user_num_id'] = df['user_num_id'].astype('int')\n",
    "\n",
    "    df['timestamp'] = df['date'].apply(date_to_timestamp)\n",
    "\n",
    "    if sampling:\n",
    "        df = filter_yelp_df(df, top_user_num=top_user_num, top_item_num=top_item_num)\n",
    "        # Refresh num id\n",
    "        df['business_num_id'] = df.business_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.business_id.nunique()))\n",
    "        df['business_num_id'] = df['business_num_id'].astype('int')\n",
    "        \n",
    "        df['user_num_id'] = df.user_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.user_id.nunique()))\n",
    "        df['user_num_id'] = df['user_num_id'].astype('int')\n",
    "#     drop_list = ['date','review_id','review_funny','review_cool','review_useful']\n",
    "#     df = df.drop(drop_list, axis=1)\n",
    "\n",
    "    df = df.reset_index(drop = True)\n",
    "\n",
    "    return df \n",
    "\n",
    "def filter_yelp_df(df, top_user_num=7000, top_item_num=5000):\n",
    "    #Getting the reviews where starts are above 3\n",
    "    df_implicit = df[df['review_stars']>3]\n",
    "    frequent_user_id = df_implicit['user_num_id'].value_counts().head(top_user_num).index.values\n",
    "    frequent_item_id = df_implicit['business_num_id'].value_counts().head(top_item_num).index.values\n",
    "    return df.loc[(df['user_num_id'].isin(frequent_user_id)) & (df['business_num_id'].isin(frequent_item_id))]\n",
    "\n",
    "def date_to_timestamp(date):\n",
    "    dt = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    return time.mktime(dt.timetuple())\n",
    "\n",
    "def df_to_sparse(df, row_name='userId', col_name='movieId', value_name='rating',\n",
    "                 shape=None):\n",
    "    rows = df[row_name]\n",
    "    cols = df[col_name]\n",
    "    if value_name is not None:\n",
    "        values = df[value_name]\n",
    "    else:\n",
    "        values = [1]*len(rows)\n",
    "\n",
    "    return csr_matrix((values, (rows, cols)), shape=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rating-UI and timestamp-UI matrix from original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_timestamp_matrix(df, sampling=False, top_user_num=7000, top_item_num=5000):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #make the df implicit with top frenquent users and \n",
    "    #no need to sample anymore if df was sampled before \n",
    "    if sampling:\n",
    "        df = filter_yelp_df(df, top_user_num=top_user_num, top_item_num=top_item_num)\n",
    "\n",
    "    rating_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "                                 col_name='business_num_id',\n",
    "                                 value_name='review_stars',\n",
    "                                 shape=None)\n",
    "    \n",
    "    #Have same dimension and data entries with rating_matrix, except that the review stars are - user avg\n",
    "#     ratingWuserAvg_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "#                                  col_name='business_num_id',\n",
    "#                                  value_name='reviewStars_userAvg',\n",
    "#                                  shape=None)\n",
    "    \n",
    "    timestamp_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "                                    col_name='business_num_id',\n",
    "                                    value_name='timestamp',\n",
    "                                    shape=None)\n",
    "    \n",
    "    \n",
    "    IC_matrix, IC_dictionary = get_I_C(df)\n",
    "#     ratingWuserAvg_matrix\n",
    "    return rating_matrix, timestamp_matrix, IC_matrix, IC_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_I_C(df):\n",
    "    lst = df.categories.values.tolist()\n",
    "    cat = []\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] is None:\n",
    "            print(i)\n",
    "        cat.extend(lst[i].split(', '))\n",
    "        \n",
    "    unique_cat = set(cat)\n",
    "    #     set categories id\n",
    "    df_cat = pd.DataFrame(list(unique_cat),columns=[\"Categories\"])\n",
    "    df_cat['cat_id'] = df_cat.Categories.astype('category').cat.rename_categories(range(0, df_cat.Categories.nunique()))\n",
    "    dict_cat = df_cat.set_index('Categories')['cat_id'].to_dict()\n",
    "    \n",
    "    df_I_C = pd.DataFrame(columns=['business_num_id', 'cat_id'])\n",
    "    \n",
    "    for i in range((df['business_num_id'].unique().shape)[0]):\n",
    "        df_temp = df[df['business_num_id'] == i].iloc[:1]\n",
    "        temp_lst = df_temp['categories'].to_list()[0].split(\",\")\n",
    "        for j in range(len(temp_lst)):\n",
    "            df_I_C = df_I_C.append({'business_num_id' : i  , 'cat_id' : dict_cat[temp_lst[j].strip()]} , ignore_index=True)\n",
    "    \n",
    "    IC_Matrix = df_to_sparse(df_I_C, row_name='business_num_id',\n",
    "                                 col_name='cat_id',\n",
    "                                 value_name=None,\n",
    "                                 shape=None)    \n",
    "    return IC_Matrix, dict_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct IC, IP, IR, ID... for Critiquing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IP_matrix_dictionary(df,item_sim):\n",
    "    # get an initial item price dataframe(without onehot encoding)\n",
    "    # drop duplicates\n",
    "    df_temp = df[['business_num_id', 'price']].drop_duplicates()\n",
    "    # with nontype with string \"NaN\"\n",
    "    df_temp.fillna(value = \"NaN\", inplace = True)\n",
    "    \n",
    "    for i in tqdm(range(df_temp.shape[0])):\n",
    "    # find all the items with no price\n",
    "        if df_temp[df_temp['business_num_id'] == i]['price'].values[0] == \"NaN\":\n",
    "            # get the index of the second large number in the similarity matrix\n",
    "            temp_l = list(item_sim[i])\n",
    "            index = [temp_l.index(x) for x in sorted(temp_l, reverse=True)[:2]][1]\n",
    "            # get the dollar sign of the similar item\n",
    "            dollar_of_sim_item = df_temp[df_temp['business_num_id'] == index]['price'].values[0]\n",
    "            # replace the Nan\n",
    "            df_temp.loc[df_temp['business_num_id'] == i,\"price\"] = dollar_of_sim_item\n",
    "\n",
    "    # assign single dollar sign($) to the ones still with no price tag(since there is no items that are similar to this item)\n",
    "    df_temp.loc[df_temp[\"price\"] == \"NaN\",\"price\"] = \"$\"\n",
    "\n",
    "    # One hot encoding\n",
    "    #note that the last column is price__$$$$\n",
    "    #cat_columns = [\"price\"]\n",
    "    #df_processed = pd.get_dummies(df_temp, prefix_sep=\"_\",\n",
    "    #                          columns=cat_columns)\n",
    "    \n",
    "    df_processed = df_temp.copy()\n",
    "    df_processed['Price'] = df_processed.apply (lambda row: len(row.price), axis=1)\n",
    "    \n",
    "    #drop the $ column\n",
    "    df_processed = df_processed.drop('price', 1)\n",
    "    \n",
    "    #Adding additional column of price label, range 1-4\n",
    "    #df_preprocessed['Price_label'] = df.apply (lambda row: label_price(row), axis=1)\n",
    "    df_processed.set_index(\"business_num_id\", drop=True, inplace=True)\n",
    "    I_P_dictionary = df_processed.to_dict()['Price']\n",
    "    df_processed.reset_index(level=0, inplace=True)\n",
    "    \n",
    "    return df_processed, I_P_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IS_dictionary(df):\n",
    "    df_IS = df[['business_num_id', 'business_stars']].drop_duplicates()\n",
    "    df_IS.set_index(\"business_num_id\", drop=True, inplace=True)\n",
    "    IS_dictionary = df_IS.to_dict()['business_stars']\n",
    "    df_IS.reset_index(level=0, inplace=True)\n",
    "    \n",
    "    return IS_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input a list of prediction matrix\n",
    "def get_ID_dictionary(df,prediction_matrix,intersection):\n",
    "    ID_dictionary = dict()\n",
    "    length = len(prediction_matrix)\n",
    "    \n",
    "    for j in tqdm(range(length)):\n",
    "        #Save the coordinates of the business id to a dictionary \n",
    "        coordinateDict = yaml.safe_load(df[df[\"business_num_id\"] == prediction_matrix[j]].iloc[0].coordinates)\n",
    "    \n",
    "        #Load the business latitude and longitude\n",
    "        test_point = Point(coordinateDict['latitude'],coordinateDict['longitude'])\n",
    "\n",
    "        #Get the distance with the test point, unit in km \n",
    "        result = round(distance.distance(intersection,test_point).kilometers,1)\n",
    "\n",
    "        ID_dictionary[prediction_matrix[j]] = result\n",
    "        \n",
    "    return ID_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time ordered split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_ordered_splitModified(rating_matrix, ratingWuserAvg_matrix, timestamp_matrix, ratio=[0.5, 0.2, 0.3],\n",
    "                       implicit=True, remove_empty=False, threshold=3,\n",
    "                       sampling=False, sampling_ratio=0.1, trainSampling=1):\n",
    "    \"\"\"\n",
    "    Split the data to train,valid,test by time\n",
    "    ratio:  train:valid:test\n",
    "    threshold: for implicit representation\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if implicit:\n",
    "        temp_rating_matrix = sparse.csr_matrix(rating_matrix.shape)\n",
    "        temp_rating_matrix[(rating_matrix > threshold).nonzero()] = 1\n",
    "        rating_matrix = temp_rating_matrix\n",
    "        timestamp_matrix = timestamp_matrix.multiply(rating_matrix)\n",
    "        #ratingWuserAvg_matrix = ratingWuserAvg_matrix.multiply(rating_matrix)\n",
    "\n",
    "    nonzero_index = None\n",
    "\n",
    "    #Default false, not removing empty columns and rows\n",
    "    #Should not have this case, since users should have at least 1 record of 4,5 \n",
    "    #And restuarant should have at least 1 record of 4,5 \n",
    "    if remove_empty:\n",
    "        # Remove empty columns. record original item index\n",
    "        nonzero_index = np.unique(rating_matrix.nonzero()[1])\n",
    "        rating_matrix = rating_matrix[:, nonzero_index]\n",
    "        timestamp_matrix = timestamp_matrix[:, nonzero_index]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[:, nonzero_index]\n",
    "\n",
    "        # Remove empty rows. record original user index\n",
    "        nonzero_rows = np.unique(rating_matrix.nonzero()[0])\n",
    "        rating_matrix = rating_matrix[nonzero_rows]\n",
    "        timestamp_matrix = timestamp_matrix[nonzero_rows]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[nonzero_rows]\n",
    "\n",
    "    user_num, item_num = rating_matrix.shape\n",
    "\n",
    "    rtrain = []\n",
    "    rtrain_userAvg = []\n",
    "    rtime = []\n",
    "    rvalid = []\n",
    "    rvalid_userAvg = []\n",
    "    rtest = []\n",
    "    rtest_userAvg = []\n",
    "    # Get the index list corresponding to item for train,valid,test\n",
    "    item_idx_train = []\n",
    "    item_idx_valid = []\n",
    "    item_idx_test = []\n",
    "    \n",
    "    for i in tqdm(range(user_num)):\n",
    "        #Get the non_zero indexs, restuarants where the user visited/liked if implicit \n",
    "        item_indexes = rating_matrix[i].nonzero()[1]        \n",
    "        #Get the data for the user\n",
    "        data = rating_matrix[i].data      \n",
    "        #Get time stamp value \n",
    "        timestamp = timestamp_matrix[i].data \n",
    "        #Get review stars with user avg data \n",
    "        if implicit == False:\n",
    "            dataWuserAvg = ratingWuserAvg_matrix[i].data\n",
    "\n",
    "            \n",
    "        #Non zero reviews for this user\n",
    "        num_nonzeros = len(item_indexes)\n",
    "        \n",
    "        #If the user has at least one review\n",
    "        if num_nonzeros >= 1:\n",
    "            num_test = int(num_nonzeros * ratio[2])\n",
    "            num_valid = int(num_nonzeros * (ratio[1] + ratio[2]))\n",
    "            valid_offset = num_nonzeros - num_valid\n",
    "            \n",
    "            # Adding this for sampling for training set\n",
    "            valid_offsetSample = int(valid_offset*trainSampling)\n",
    "            test_offset = num_nonzeros - num_test\n",
    "            \n",
    "            #Sort the timestamp for each review for the user\n",
    "            argsort = np.argsort(timestamp)\n",
    "            \n",
    "            #Sort the reviews for the user according to the time stamp \n",
    "            data = data[argsort]\n",
    "            \n",
    "            #Sort the review with user avg accoridng to the time stamp\n",
    "            if implicit == False:\n",
    "                dataWuserAvg = dataWuserAvg[argsort]\n",
    "            \n",
    "            #Non-zero review index sort according to time\n",
    "            item_indexes = item_indexes[argsort]\n",
    "            \n",
    "            #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "            #if take from old to new\n",
    "            #rtrain.append([data[:valid_offsetSample], np.full(valid_offsetSample, i), item_indexes[:valid_offsetSample]])\n",
    "            #if take from new to old\n",
    "            rtrain.append([data[valid_offset-valid_offsetSample:valid_offset], np.full(valid_offsetSample, i), item_indexes[valid_offset-valid_offsetSample:valid_offset]])\n",
    "            rvalid.append([data[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                           item_indexes[valid_offset:test_offset]])\n",
    "            rtest.append([data[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "            \n",
    "            if implicit == False:\n",
    "                #Now for the rating matrix that considers user average rating\n",
    "                #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "                #from old to new\n",
    "                #rtrain_userAvg.append([dataWuserAvg[:valid_offsetSample], np.full(valid_offsetSample, i), item_indexes[:valid_offsetSample]])\n",
    "                #take nearest\n",
    "                rtrain_userAvg.append([dataWuserAvg[valid_offset-valid_offsetSample:valid_offset], np.full(valid_offsetSample, i), item_indexes[valid_offset-valid_offsetSample:valid_offset]])                \n",
    "                    \n",
    "                rvalid_userAvg.append([dataWuserAvg[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                               item_indexes[valid_offset:test_offset]])\n",
    "                \n",
    "                rtest_userAvg.append([dataWuserAvg[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "                \n",
    "            item_idx_train.append(item_indexes[:valid_offsetSample])\n",
    "            item_idx_valid.append(item_indexes[:test_offset])\n",
    "            item_idx_test.append(item_indexes[test_offset:])\n",
    "            \n",
    "        else:\n",
    "            item_idx_train.append([])\n",
    "    \n",
    "    rtrain = np.array(rtrain)\n",
    "    rvalid = np.array(rvalid)\n",
    "    rtest = np.array(rtest)\n",
    "   \n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = np.array(rtrain_userAvg)\n",
    "        rvalid_userAvg = np.array(rvalid_userAvg)\n",
    "        rtest_userAvg = np.array(rtest_userAvg)\n",
    "\n",
    "    #take non-zeros values, row index, and column (non-zero) index and store into sparse matrix\n",
    "    rtrain = sparse.csr_matrix((np.hstack(rtrain[:, 0]), (np.hstack(rtrain[:, 1]), np.hstack(rtrain[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rvalid = sparse.csr_matrix((np.hstack(rvalid[:, 0]), (np.hstack(rvalid[:, 1]), np.hstack(rvalid[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rtest = sparse.csr_matrix((np.hstack(rtest[:, 0]), (np.hstack(rtest[:, 1]), np.hstack(rtest[:, 2]))),\n",
    "                              shape=rating_matrix.shape, dtype=np.float32)\n",
    "    \n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = sparse.csr_matrix((np.hstack(rtrain_userAvg[:, 0]), (np.hstack(rtrain_userAvg[:, 1]), np.hstack(rtrain_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rvalid_userAvg = sparse.csr_matrix((np.hstack(rvalid_userAvg[:, 0]), (np.hstack(rvalid_userAvg[:, 1]), np.hstack(rvalid_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rtest_userAvg = sparse.csr_matrix((np.hstack(rtest_userAvg[:, 0]), (np.hstack(rtest_userAvg[:, 1]), np.hstack(rtest_userAvg[:, 2]))),\n",
    "                                  shape=rating_matrix.shape, dtype=np.float32)\n",
    "\n",
    "    return rtrain, rvalid, rtest,rtrain_userAvg, rvalid_userAvg, rtest_userAvg, nonzero_index, timestamp_matrix, item_idx_train, item_idx_valid, item_idx_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_three_popularity_matrix(df_original,rtrain):\n",
    "    # get the list of popular items by ranking the number of reviews\n",
    "    numUsers = rtrain.shape[0]\n",
    "    numItems = rtrain.shape[1]\n",
    "    \n",
    "    dff_popular = df_original.copy()\n",
    "    dff_popular = dff_popular.sort_values(by=[\"review_count_y\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "    popular_list_num_of_reviews = dff_popular[\"business_num_id\"].tolist()\n",
    "    \n",
    "    # get the list of popular items by ranking average rating score\n",
    "    dff_popular_rating = df_original.copy()\n",
    "    dff_popular_rating = dff_popular_rating.sort_values(by=[\"business_stars\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "    popular_list_avg_stars = dff_popular_rating[\"business_num_id\"].tolist()\n",
    "    \n",
    "    lst_temp = []\n",
    "    for item in tqdm(range(numItems)):\n",
    "        numOfUsersRated = len(rtrain.toarray()[:, item].nonzero()[0])\n",
    "        if numOfUsersRated <= 50:\n",
    "            lst_temp.append(item)\n",
    "    popular_list_avg_stars = [x for x in popular_list_avg_stars if x not in lst_temp]\n",
    "    \n",
    "    # get the popularity items by using the percentage liked method(number of liked items / total items)\n",
    "    predictionMatrix = np.zeros((numUsers , numItems))\n",
    "\n",
    "    # Define function for converting 1-5 rating to 0/1 (like / don't like)\n",
    "    vf = np.vectorize(lambda x: 1 if x >= 4 else 0)\n",
    "    rtrain_array = rtrain.toarray()\n",
    "    # For every item calculate the number of people liked (4-5) divided by the number of people that rated\n",
    "    itemPopularity = np.zeros((numItems))\n",
    "    for item in range(numItems):\n",
    "        numOfUsersRated = len(rtrain_array[:, item].nonzero()[0])\n",
    "        numOfUsersLiked = len(vf(rtrain_array[:, item]).nonzero()[0])\n",
    "#         if numOfUsersRated == 0:\n",
    "        # set a threshold to filter out restaurants with very few reviews\n",
    "        if numOfUsersRated <= 30:\n",
    "            itemPopularity[item] = 0\n",
    "        else:\n",
    "            itemPopularity[item] = numOfUsersLiked/numOfUsersRated\n",
    "    popular_list_liked_ratio = itemPopularity.argsort()\n",
    "    \n",
    "    return np.asarray(popular_list_num_of_reviews),np.asarray(popular_list_avg_stars),popular_list_liked_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get df for training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Item idex matrix stores the reivews starts\n",
    "#This function returns a list of index for the reviews included in training set \n",
    "def get_corpus_idx_list(df, item_idx_matrix):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    df: total dataframe\n",
    "    item_idx_matrix: train index list got from time_split \n",
    "    Output: row index in original dataframe for training data by time split\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    #For all the users: 5791\n",
    "    for i in tqdm(range(len(item_idx_matrix))):\n",
    "        \n",
    "        #find row index where user_num_id is i\n",
    "        a = df.index[df['user_num_id'] == i].tolist()\n",
    "        \n",
    "        #loop through the busienss id that the user i reviewed for in offvalid set \n",
    "        for item_idx in  item_idx_matrix[i]:\n",
    "            \n",
    "            #get the row index for reviews for business that the user liked in the train set\n",
    "            b = df.index[df['business_num_id'] == item_idx].tolist()\n",
    "            \n",
    "            #Find the index for which this user liked, one user only rate a business once\n",
    "            idx_to_add = list(set(a).intersection(b))\n",
    "            \n",
    "            if idx_to_add not in lst:\n",
    "                lst.extend(idx_to_add)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess using Term Frequency - CounterVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shenti10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shenti10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Stemming and Lemmatisation\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Get corpus and CountVector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "new_words = ['not_the']\n",
    "stop_words = stop_words.union(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#Should 'because' added?\n",
    "def preprocess(df, reset_list = [',','.','?',';','however','but']):\n",
    "    corpus = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        text = df['review_text'][i]\n",
    "        change_flg = 0\n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        ##Convert to list from string, loop through the review text\n",
    "        text = text.split()\n",
    "        \n",
    "        #any sentence that encounters a not, the folloing words will become not phrase until hit the sentence end\n",
    "        for j in range(len(text)):\n",
    "            #Make the not_ hack\n",
    "            if text[j] == 'not':\n",
    "                change_flg = 1\n",
    "#                 print 'changes is made after ', i\n",
    "                continue\n",
    "            #if was 1 was round and not hit a 'not' in this round\n",
    "            if change_flg == 1 and any(reset in text[j] for reset in reset_list):\n",
    "                text[j] = 'not_' + text[j]\n",
    "                change_flg = 0\n",
    "#                 print 'reset at ', i\n",
    "            if change_flg == 1:\n",
    "                text[j] = 'not_' + text[j]\n",
    "        \n",
    "        #Convert back to string\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "        #Remove punctuations\n",
    "#       text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        \n",
    "        #remove tags\n",
    "        text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "        \n",
    "        # remove special characters and digits\n",
    "        text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "        \n",
    "        ##Convert to list from string\n",
    "        text = text.split()\n",
    "        \n",
    "        ##Stemming\n",
    "        ps=PorterStemmer()\n",
    "        \n",
    "        #Lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        text = [lem.lemmatize(word) for word in text if not word in  \n",
    "                stop_words] \n",
    "        text = \" \".join(text)\n",
    "        corpus.append(text)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def train(matrix_train):\n",
    "    similarity = cosine_similarity(X=matrix_train, Y=None, dense_output=True)\n",
    "    return similarity\n",
    "\n",
    "def get_I_K(df, X, row_name = 'business_num_id', binary = True, shape = (121994,6000)):\n",
    "    \"\"\"\n",
    "    get the item-keyphrase matrix\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    cols = []\n",
    "    vals = []\n",
    "    #For each review history\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        #Get the array of frequencies for document/review i \n",
    "        arr = X[i].toarray() \n",
    "        nonzero_element = arr.nonzero()[1]  # Get nonzero element in each line, keyphrase that appears index \n",
    "        length_of_nonzero = len(nonzero_element) #number of important keyphrase that appears\n",
    "        \n",
    "        # df[row_name][i] is the item idex\n",
    "        #Get a list row index that indicates the document/review\n",
    "        rows.extend(np.array([df[row_name][i]]*length_of_nonzero)) ## Item index\n",
    "        #print(rows)\n",
    "        \n",
    "        #Get a list of column index indicating the key phrase that appears in i document/review\n",
    "        cols.extend(nonzero_element) ## Keyword Index\n",
    "        if binary:\n",
    "            #Create a bunch of 1s\n",
    "            vals.extend(np.array([1]*length_of_nonzero))\n",
    "        else:\n",
    "            #If not binary \n",
    "            vals.extend(arr[arr.nonzero()])    \n",
    "    return csr_matrix((vals, (rows, cols)), shape=shape)\n",
    "\n",
    "\n",
    "#Get a UI matrix if it's not item_similarity based or else IU\n",
    "def predict(matrix_train, k, similarity, item_similarity_en = False):\n",
    "    prediction_scores = []\n",
    "    \n",
    "    #inverse to IU matrix\n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    #for each user or item, depends UI or IU \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "        # Get user u's prediction scores for all items\n",
    "        #Get prediction/similarity score for each user 1*num or user or num of items\n",
    "        vector_u = similarity[user_index]\n",
    "\n",
    "        # Get closest K neighbors excluding user u self\n",
    "        #Decending accoding to similarity score, select top k\n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        \n",
    "        # Get neighbors similarity weights and ratings\n",
    "        similar_users_weights = similarity[user_index][similar_users]\n",
    "        \n",
    "        #similar_users_weights_sum = np.sum(similar_users_weights)\n",
    "        #print(similar_users_weights.shape)\n",
    "        #shape: num of res * k\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "              \n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "        #print(prediction_scores_u)\n",
    "        \n",
    "        \n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "        \n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    return res\n",
    "\n",
    "\n",
    "#Preidction score is UI or IU?\n",
    "def prediction(prediction_score, topK, matrix_Train):\n",
    "\n",
    "    prediction = []\n",
    "\n",
    "    #for each user\n",
    "    for user_index in tqdm(range(matrix_Train.shape[0])):\n",
    "        \n",
    "        #take the prediction scores for user 1 * num res\n",
    "        vector_u = prediction_score[user_index]\n",
    "        \n",
    "        #The restuarant the user rated\n",
    "        vector_train = matrix_Train[user_index]\n",
    "        \n",
    "        if len(vector_train.nonzero()[0]) > 0:\n",
    "            vector_predict = sub_routine(vector_u, vector_train, topK=topK)\n",
    "        else:\n",
    "            vector_predict = np.zeros(topK, dtype=np.float32)\n",
    "\n",
    "        prediction.append(vector_predict)\n",
    "\n",
    "    return np.vstack(prediction)\n",
    "\n",
    "def prediction_modified(prediction_score, matrix_Train, user_id, topK = 50):\n",
    "    prediction = []\n",
    "\n",
    "    #for each user\n",
    "    for user_index in tqdm(range(matrix_Train.shape[0])):\n",
    "        \n",
    "        #take the prediction scores for user 1 * num res\n",
    "        vector_u = prediction_score[user_index]\n",
    "        \n",
    "        #The restuarant the user rated\n",
    "        vector_train = matrix_Train[user_index]\n",
    "        \n",
    "        if len(vector_train.nonzero()[0]) > 0:\n",
    "            vector_predict = sub_routine_modified(vector_u, vector_train)\n",
    "        else:\n",
    "            vector_predict = np.zeros(topK, dtype=np.float32)\n",
    "\n",
    "        prediction.append(vector_predict)\n",
    "\n",
    "    return prediction[user_id]\n",
    "\n",
    "#topK: the number of restuarants we are suggesting \n",
    "#if vector_train has number, then the user has visited\n",
    "def sub_routine(vector_u, vector_train, topK=500):\n",
    "\n",
    "    #index where non-zero\n",
    "    train_index = vector_train.nonzero()[1]\n",
    "    \n",
    "    vector_u = vector_u\n",
    "    \n",
    "    #get topk + num rated res prediction score descending, top index \n",
    "    candidate_index = np.argpartition(-vector_u, topK+len(train_index))[:topK+len(train_index)]\n",
    "    \n",
    "    #sort top prediction score index in range topK+len(train_index) into vector_u`\n",
    "    vector_u = candidate_index[vector_u[candidate_index].argsort()[::-1]]\n",
    "    \n",
    "    #deleted the rated res from the topk+train_index prediction score vector for user u \n",
    "    #Delete the user rated res index from the topk+numRated index\n",
    "    vector_u = np.delete(vector_u, np.isin(vector_u, train_index).nonzero()[0])\n",
    "\n",
    "    #so we only include the top K prediction score here\n",
    "    return vector_u[:topK]\n",
    "\n",
    "def sub_routine_modified(vector_u, vector_train):\n",
    "\n",
    "    #index where non-zero\n",
    "    train_index = vector_train.nonzero()[1]\n",
    "    \n",
    "    vector_u = vector_u\n",
    "    \n",
    "    #get topk + num rated res prediction score descending, top index \n",
    "    candidate_index = np.argpartition(-vector_u, -1)\n",
    "    \n",
    "    #sort top prediction score index in range topK+len(train_index) into vector_u`\n",
    "    vector_u = candidate_index[vector_u[candidate_index].argsort()[::-1]]\n",
    "    \n",
    "    #deleted the rated res from the topk+train_index prediction score vector for user u \n",
    "    #Delete the user rated res index from the topk+numRated index\n",
    "    vector_u = np.delete(vector_u, np.isin(vector_u, train_index).nonzero()[0])\n",
    "\n",
    "    #so we only include the top K prediction score here\n",
    "    return vector_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recallk(vector_true_dense, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "\n",
    "def precisionk(vector_predict, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_predict)\n",
    "\n",
    "\n",
    "def average_precisionk(vector_predict, hits, **unused):\n",
    "    precisions = np.cumsum(hits, dtype=np.float32)/range(1, len(vector_predict)+1)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "def r_precision(vector_true_dense, vector_predict, **unused):\n",
    "    vector_predict_short = vector_predict[:len(vector_true_dense)]\n",
    "    hits = len(np.isin(vector_predict_short, vector_true_dense).nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "\n",
    "def _dcg_support(size):\n",
    "    arr = np.arange(1, size+1)+1\n",
    "    return 1./np.log2(arr)\n",
    "\n",
    "\n",
    "def ndcg(vector_true_dense, vector_predict, hits):\n",
    "    idcg = np.sum(_dcg_support(len(vector_true_dense)))\n",
    "    dcg_base = _dcg_support(len(vector_predict))\n",
    "    dcg_base[np.logical_not(hits)] = 0\n",
    "    dcg = np.sum(dcg_base)\n",
    "    return dcg/idcg\n",
    "\n",
    "\n",
    "def click(hits, **unused):\n",
    "    first_hit = next((i for i, x in enumerate(hits) if x), None)\n",
    "    if first_hit is None:\n",
    "        return 5\n",
    "    else:\n",
    "        return first_hit/10\n",
    "\n",
    "\n",
    "def evaluate(matrix_Predict, matrix_Test, metric_names =['R-Precision', 'NDCG', 'Precision', 'Recall', 'MAP'], atK = [5, 10, 15, 20, 50], analytical=False):\n",
    "    \"\"\"\n",
    "    :param matrix_U: Latent representations of users, for LRecs it is RQ, for ALSs it is U\n",
    "    :param matrix_V: Latent representations of items, for LRecs it is Q, for ALSs it is V\n",
    "    :param matrix_Train: Rating matrix for training, features.\n",
    "    :param matrix_Test: Rating matrix for evaluation, true labels.\n",
    "    :param k: Top K retrieval\n",
    "    :param metric_names: Evaluation metrics\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global_metrics = {\n",
    "        #\"R-Precision\": r_precision,\n",
    "        #\"NDCG\": ndcg,\n",
    "        #\"Clicks\": click\n",
    "    }\n",
    "\n",
    "    local_metrics = {\n",
    "        #\"Precision\": precisionk,\n",
    "        #\"Recall\": recallk,\n",
    "        \"MAP\": average_precisionk\n",
    "    }\n",
    "\n",
    "    output = dict()\n",
    "\n",
    "    num_users = matrix_Predict.shape[0]\n",
    "\n",
    "    for k in atK:\n",
    "\n",
    "        local_metric_names = list(set(metric_names).intersection(local_metrics.keys()))\n",
    "        results = {name: [] for name in local_metric_names}\n",
    "        topK_Predict = matrix_Predict[:, :k]\n",
    "\n",
    "        for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "            vector_predict = topK_Predict[user_index]\n",
    "            if len(vector_predict.nonzero()[0]) > 0:\n",
    "                vector_true = matrix_Test[user_index]\n",
    "                vector_true_dense = vector_true.nonzero()[1]\n",
    "                hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "                if vector_true_dense.size > 0:\n",
    "                    for name in local_metric_names:\n",
    "                        results[name].append(local_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                                 vector_predict=vector_predict,\n",
    "                                                                 hits=hits))\n",
    "        results_summary = dict()\n",
    "        if analytical:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = round(results[name],4)\n",
    "        else:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = (round((np.average(results[name])),4),\n",
    "                                                              round((1.96*np.std(results[name])/np.sqrt(num_users)),4))\n",
    "        output.update(results_summary)\n",
    "\n",
    "    global_metric_names = list(set(metric_names).intersection(global_metrics.keys()))\n",
    "    results = {name: [] for name in global_metric_names}\n",
    "\n",
    "    topK_Predict = matrix_Predict[:]\n",
    "\n",
    "    for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "        vector_predict = topK_Predict[user_index]\n",
    "\n",
    "        if len(vector_predict.nonzero()[0]) > 0:\n",
    "            vector_true = matrix_Test[user_index]\n",
    "            vector_true_dense = vector_true.nonzero()[1]\n",
    "            hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "            # if user_index == 1:\n",
    "            #     import ipdb;\n",
    "            #     ipdb.set_trace()\n",
    "\n",
    "            if vector_true_dense.size > 0:\n",
    "                for name in global_metric_names:\n",
    "                    results[name].append(global_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                              vector_predict=vector_predict,\n",
    "                                                              hits=hits))\n",
    "    results_summary = dict()\n",
    "    if analytical:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = round(results[name],4)\n",
    "    else:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = (round(np.average(results[name]),4), round((1.96*np.std(results[name])/np.sqrt(num_users)),4))\n",
    "    output.update(results_summary)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass in a vector of predicted restaurant for this specific user\n",
    "#We specify the index of the recommended restaurant we want to display \n",
    "def displayRestaurantInfo(RecommendItem_Index, user_item_predictionMatrix,df,ID_dictionary):\n",
    "    #RecommendItem_Index: 0,1,2,3.. changes as user critiques the restaurant itself\n",
    "    print('------------------------------------------------------')\n",
    "    recommend_item = user_item_predictionMatrix[RecommendItem_Index]\n",
    "    print('Business_num_id: ', recommend_item)\n",
    "    print('Restaurant name:', df[df['business_num_id'] == recommend_item].name.unique()[0],\\\n",
    "      '\\nCuisine Type: ', df[df['business_num_id'] == recommend_item].categories.unique()[0],\\\n",
    "      '\\nPrice:', df[df['business_num_id'] == recommend_item].price.unique()[0],\\\n",
    "      '\\nRating:', df[df['business_num_id'] == recommend_item].business_stars.unique()[0],\\\n",
    "      '\\nDistance:', ID_dictionary[recommend_item], 'km'\n",
    "     )\n",
    "    print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get original dataframe out of the review datastet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_yelp_df(path ='', filename=reviewJsonToronto, sampling= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205968"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# listCity = []\n",
    "# for location in df.location.unique():\n",
    "#     city = yaml.safe_load(location)['city']\n",
    "#     if city not in listCity:\n",
    "#         listCity.append(city)\n",
    "# listCity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0_x</th>\n",
       "      <th>Unnamed: 0_y</th>\n",
       "      <th>Updated</th>\n",
       "      <th>Year</th>\n",
       "      <th>alias</th>\n",
       "      <th>business_id</th>\n",
       "      <th>business_stars</th>\n",
       "      <th>...</th>\n",
       "      <th>review_text</th>\n",
       "      <th>transactions</th>\n",
       "      <th>ufc</th>\n",
       "      <th>url</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_loc</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>business_num_id</th>\n",
       "      <th>user_num_id</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>21653</td>\n",
       "      <td>False</td>\n",
       "      <td>2016</td>\n",
       "      <td>happy-lemon-markham</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>I ordered the lemon mango slush and the lemon ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>https://www.yelp.com/biz/happy-lemon-markham?a...</td>\n",
       "      <td>zsJFjhBQEFQ6gJ7BsNM_Ug</td>\n",
       "      <td>Toronto, Canada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2682</td>\n",
       "      <td>6984</td>\n",
       "      <td>1.471925e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>21653</td>\n",
       "      <td>False</td>\n",
       "      <td>2016</td>\n",
       "      <td>happy-lemon-markham</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>Came here on a Sunday afternoon, it wasn't bus...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>https://www.yelp.com/biz/happy-lemon-markham?a...</td>\n",
       "      <td>P7YuMh74-I2cDq7oU8frww</td>\n",
       "      <td>York Regional Municipality, Canada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2682</td>\n",
       "      <td>2845</td>\n",
       "      <td>1.475381e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day  Month  Unnamed: 0.1  Unnamed: 0_x  Unnamed: 0_y  Updated  Year  \\\n",
       "0   23      8             6             6         21653    False  2016   \n",
       "1    2     10             7             7         21653    False  2016   \n",
       "\n",
       "                 alias             business_id  business_stars  ...  \\\n",
       "0  happy-lemon-markham  Xo1LNzhnwE-ilqsM3ybs9Q             3.5  ...   \n",
       "1  happy-lemon-markham  Xo1LNzhnwE-ilqsM3ybs9Q             3.5  ...   \n",
       "\n",
       "                                         review_text transactions        ufc  \\\n",
       "0  I ordered the lemon mango slush and the lemon ...           []  [1, 1, 1]   \n",
       "1  Came here on a Sunday afternoon, it wasn't bus...           []  [1, 0, 0]   \n",
       "\n",
       "                                                 url                 user_id  \\\n",
       "0  https://www.yelp.com/biz/happy-lemon-markham?a...  zsJFjhBQEFQ6gJ7BsNM_Ug   \n",
       "1  https://www.yelp.com/biz/happy-lemon-markham?a...  P7YuMh74-I2cDq7oU8frww   \n",
       "\n",
       "                             user_loc  vote_count business_num_id user_num_id  \\\n",
       "0                     Toronto, Canada         1.0            2682        6984   \n",
       "1  York Regional Municipality, Canada         1.0            2682        2845   \n",
       "\n",
       "      timestamp  \n",
       "0  1.471925e+09  \n",
       "1  1.475381e+09  \n",
       "\n",
       "[2 rows x 47 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get rating-UI matrix and timestepm-UI matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix, timestamp_matrix , I_C_matrix, IC_dictionary = get_rating_timestamp_matrix(df)\n",
    "\n",
    "# get ratingWuserAvg_matrix\n",
    "rating_array = rating_matrix.toarray()\n",
    "user_average_array = rating_array.sum(axis = 1)/np.count_nonzero(rating_array,axis = 1)\n",
    "init_UI = np.zeros(rating_array.shape)\n",
    "init_UI[rating_array.nonzero()] = 1\n",
    "\n",
    "#Creating rating with user average array array\n",
    "for i in range(user_average_array.shape[0]):\n",
    "    init_UI[i] = init_UI[i] * (user_average_array[i]-0.001) \n",
    "user_average_array = init_UI\n",
    "ratingWuserAvg_array = rating_array - user_average_array\n",
    "ratingWuserAvg_matrix=sparse.csr_matrix(ratingWuserAvg_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Chicken Wings': 47,\n",
       " 'Tabletop Games': 199,\n",
       " 'Syrian': 198,\n",
       " 'Salad': 170,\n",
       " 'Sandwiches': 172,\n",
       " 'Nicaraguan': 144,\n",
       " 'Chocolatiers & Shops': 49,\n",
       " 'Desserts': 67,\n",
       " 'Bistros': 23,\n",
       " 'Sri Lankan': 192,\n",
       " 'Chicken Shop': 46,\n",
       " 'Hungarian': 110,\n",
       " 'Mediterranean': 136,\n",
       " 'Team Building Activities': 204,\n",
       " 'Poke': 159,\n",
       " 'Venezuelan': 214,\n",
       " 'Peruvian': 155,\n",
       " 'Restaurants': 167,\n",
       " 'Dive Bars': 71,\n",
       " 'Tea Rooms': 203,\n",
       " 'Donairs': 73,\n",
       " 'Custom Cakes': 62,\n",
       " 'Sports Clubs': 191,\n",
       " 'Live/Raw Food': 131,\n",
       " 'Breweries': 28,\n",
       " 'Irish': 117,\n",
       " 'Asian Fusion': 10,\n",
       " 'Thai': 206,\n",
       " 'Arts & Entertainment': 9,\n",
       " 'Specialty Food': 188,\n",
       " 'Pool Halls': 161,\n",
       " 'Breakfast & Brunch': 27,\n",
       " 'Czech': 63,\n",
       " 'Fast Food': 80,\n",
       " 'Musical Instruments & Teachers': 143,\n",
       " 'Coffee Roasteries': 54,\n",
       " 'Cheese Shops': 44,\n",
       " 'Social Clubs': 182,\n",
       " 'Patisserie/Cake Shop': 151,\n",
       " 'Cafes': 35,\n",
       " 'Afghan': 1,\n",
       " 'Tiki Bars': 207,\n",
       " 'Cheesesteaks': 45,\n",
       " 'Food Delivery Services': 87,\n",
       " 'Art Galleries': 8,\n",
       " 'Sushi Bars': 197,\n",
       " 'Organic Stores': 147,\n",
       " 'Persian/Iranian': 153,\n",
       " 'Lebanese': 130,\n",
       " 'Tapas/Small Plates': 202,\n",
       " 'Caterers': 42,\n",
       " 'Filipino': 82,\n",
       " 'Pizza': 158,\n",
       " 'Mexican': 137,\n",
       " 'Butcher': 34,\n",
       " 'Barre Classes': 17,\n",
       " 'Juice Bars & Smoothies': 122,\n",
       " 'Comedy Clubs': 56,\n",
       " 'Tex-Mex': 205,\n",
       " 'Vietnamese': 216,\n",
       " 'Bed & Breakfast': 19,\n",
       " 'Indonesian': 114,\n",
       " 'Middle Eastern': 138,\n",
       " 'Cooking Classes': 59,\n",
       " 'Reunion': 168,\n",
       " 'Pets': 157,\n",
       " 'Mongolian': 140,\n",
       " 'Ethiopian': 77,\n",
       " 'Tours': 209,\n",
       " 'Hot Pot': 108,\n",
       " 'Pubs': 165,\n",
       " 'Bakeries': 14,\n",
       " 'Ramen': 166,\n",
       " 'Latin American': 129,\n",
       " 'Polish': 160,\n",
       " 'Russian': 169,\n",
       " 'Street Vendors': 194,\n",
       " 'Supper Clubs': 196,\n",
       " 'Laotian': 128,\n",
       " 'Shopping Centers': 178,\n",
       " 'Tapas Bars': 201,\n",
       " 'Wineries': 223,\n",
       " 'Comfort Food': 57,\n",
       " 'Cajun/Creole': 36,\n",
       " 'South African': 185,\n",
       " 'Seafood': 174,\n",
       " 'Food Court': 86,\n",
       " 'Salvadoran': 171,\n",
       " 'Seafood Markets': 175,\n",
       " 'Modern European': 139,\n",
       " 'Kids Activities': 125,\n",
       " 'Waffles': 217,\n",
       " 'Professional Sports Teams': 164,\n",
       " 'Soup': 184,\n",
       " 'Specialty Schools': 189,\n",
       " 'Gastropubs': 92,\n",
       " 'Party & Event Planning': 150,\n",
       " 'High Fidelity Audio Equipment': 103,\n",
       " 'Whiskey Bars': 219,\n",
       " 'Wine Bars': 221,\n",
       " 'Golf': 96,\n",
       " 'Bubble Tea': 31,\n",
       " 'Food Tours': 89,\n",
       " 'Bowling': 24,\n",
       " 'Cuban': 61,\n",
       " 'Buffets': 32,\n",
       " 'African': 2,\n",
       " 'Imported Food': 112,\n",
       " 'Performing Arts': 152,\n",
       " 'Community Service/Non-Profit': 58,\n",
       " 'Arabian': 5,\n",
       " 'Spanish': 187,\n",
       " 'Cinema': 51,\n",
       " 'Bangladeshi': 15,\n",
       " 'Food Trucks': 90,\n",
       " 'Scottish': 173,\n",
       " 'Wine Tours': 222,\n",
       " 'Flea Markets': 84,\n",
       " 'Ukrainian': 211,\n",
       " 'Cantonese': 40,\n",
       " 'Italian': 119,\n",
       " 'Wine & Spirits': 220,\n",
       " 'American (Traditional)': 3,\n",
       " 'Portuguese': 162,\n",
       " 'Music Venues': 142,\n",
       " 'Antiques': 4,\n",
       " 'Turkish': 210,\n",
       " 'Wedding Planning': 218,\n",
       " 'Health Markets': 102,\n",
       " 'Brazilian': 26,\n",
       " 'Ice Cream & Frozen Yogurt': 111,\n",
       " 'Hong Kong Style Cafe': 105,\n",
       " 'Malaysian': 134,\n",
       " 'Himalayan/Nepalese': 104,\n",
       " 'Halal': 100,\n",
       " 'Cocktail Bars': 52,\n",
       " 'Hawaiian': 101,\n",
       " 'Kosher': 127,\n",
       " 'Lounges': 132,\n",
       " 'Southern': 186,\n",
       " 'Slovakian': 180,\n",
       " 'Bars': 18,\n",
       " 'Cambodian': 37,\n",
       " 'Tobacco Shops': 208,\n",
       " 'Soul Food': 183,\n",
       " 'Australian': 11,\n",
       " 'Macarons': 133,\n",
       " 'British': 30,\n",
       " 'International': 115,\n",
       " 'Hookah Bars': 106,\n",
       " 'Internet Cafes': 116,\n",
       " 'German': 94,\n",
       " 'Colombian': 55,\n",
       " 'Personal Chefs': 154,\n",
       " 'Delicatessen': 65,\n",
       " 'Nightlife': 145,\n",
       " 'Champagne Bars': 43,\n",
       " 'Taiwanese': 200,\n",
       " 'Cideries': 50,\n",
       " 'Karaoke': 123,\n",
       " 'Egyptian': 75,\n",
       " 'Caribbean': 41,\n",
       " 'Coffee & Tea': 53,\n",
       " 'Festivals': 81,\n",
       " 'Bagels': 13,\n",
       " 'Sports Bars': 190,\n",
       " 'Cannabis Clinics': 39,\n",
       " 'Dance Clubs': 64,\n",
       " 'Venues & Event Spaces': 215,\n",
       " 'Hakka': 99,\n",
       " 'Dim Sum': 68,\n",
       " 'French': 91,\n",
       " 'Shaved Snow': 176,\n",
       " 'Vegan': 212,\n",
       " 'Korean': 126,\n",
       " 'Irish Pub': 118,\n",
       " 'Guitar Stores': 98,\n",
       " 'Brasseries': 25,\n",
       " 'Strip Clubs': 195,\n",
       " 'Argentine': 7,\n",
       " 'Dinner Theater': 70,\n",
       " 'Burgers': 33,\n",
       " 'Moroccan': 141,\n",
       " 'Delis': 66,\n",
       " 'Meat Shops': 135,\n",
       " 'Kebab': 124,\n",
       " 'Chinese': 48,\n",
       " 'Acai Bowls': 0,\n",
       " 'Donuts': 74,\n",
       " 'Do-It-Yourself Food': 72,\n",
       " 'Falafel': 78,\n",
       " 'Escape Games': 76,\n",
       " 'Jazz & Blues': 121,\n",
       " 'Austrian': 12,\n",
       " 'Beer Bar': 21,\n",
       " 'Hotels': 109,\n",
       " 'Pakistani': 148,\n",
       " 'Fish & Chips': 83,\n",
       " 'Pan Asian': 149,\n",
       " 'Barbeque': 16,\n",
       " 'Steakhouses': 193,\n",
       " 'Florists': 85,\n",
       " 'Shopping': 177,\n",
       " 'Noodles': 146,\n",
       " 'Gay Bars': 93,\n",
       " 'Beer': 20,\n",
       " 'Pet Stores': 156,\n",
       " 'Singaporean': 179,\n",
       " 'Vegetarian': 213,\n",
       " 'Indian': 113,\n",
       " 'Food Stands': 88,\n",
       " 'Canadian (New)': 38,\n",
       " 'Belgian': 22,\n",
       " 'Diners': 69,\n",
       " 'Poutineries': 163,\n",
       " 'Smokehouse': 181,\n",
       " 'Creperies': 60,\n",
       " 'Arcades': 6,\n",
       " 'Brewpubs': 29,\n",
       " 'Farmers Market': 79,\n",
       " 'Japanese': 120,\n",
       " 'Gluten-Free': 95,\n",
       " 'Greek': 97,\n",
       " 'Hot Dogs': 107}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IC_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split to get rtrain-UI matrix and valid and test.. item_index_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  del sys.path[0]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 7000/7000 [00:01<00:00, 5076.14it/s]\n"
     ]
    }
   ],
   "source": [
    "rtrain_implicit, rvalid_implicit, rtest_implicit, rtrain_userAvg_implicit, rvalid_userAvg_implicit, rtest_userAvg_implicit, nonzero_index, rtime, item_idx_matrix_train_implicit,item_idx_matrix_valid_implicit, item_idx_matrix_test_implicit = time_ordered_splitModified(rating_matrix=rating_matrix, ratingWuserAvg_matrix=ratingWuserAvg_matrix, timestamp_matrix=timestamp_matrix,\n",
    "                                                                     ratio=[0.5,0.2,0.3],\n",
    "                                                                     implicit=True,\n",
    "                                                                     remove_empty=False, threshold=3,sampling=False, \n",
    "                                                                     sampling_ratio=0.1, trainSampling=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7000/7000 [00:01<00:00, 4034.16it/s]\n"
     ]
    }
   ],
   "source": [
    "rtrain, rvalid, rtest, rtrain_userAvg, rvalid_userAvg, rtest_userAvg, nonzero_index, rtime, item_idx_matrix_train,item_idx_matrix_valid, item_idx_matrix_test = time_ordered_splitModified(rating_matrix=rating_matrix, ratingWuserAvg_matrix=ratingWuserAvg_matrix, timestamp_matrix=timestamp_matrix,\n",
    "                                                                     ratio=[0.5,0.2,0.3],\n",
    "                                                                     implicit=False,\n",
    "                                                                     remove_empty=False, threshold=3,\n",
    "                                                                     sampling=False, sampling_ratio=0.1, \n",
    "                                                                     trainSampling=0.95)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using entire dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain = rtrain + rvalid + rtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain_implicit = rtrain_implicit + rvalid_implicit + rtest_implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TD-IDF to compute corpus and X (business vs. terms) TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 205968/205968 [01:13<00:00, 2793.51it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary to store business: review text\n",
    "dict_text = {}\n",
    "for i in range(len(corpus)):\n",
    "    if df['business_num_id'][i] not in dict_text:\n",
    "        dict_text[df['business_num_id'][i]] = corpus[i]\n",
    "    else:\n",
    "        temp = dict_text[df['business_num_id'][i]]\n",
    "        temp = temp + corpus[i]\n",
    "        dict_text[df['business_num_id'][i]] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list for the review text, where the row dimension = total business ids\n",
    "list_text = []\n",
    "for key in range(0,max(list(dict_text.keys()))+1) :\n",
    "    if key not in dict_text.keys():\n",
    "        list_text.extend([\"\"])\n",
    "    else:\n",
    "        list_text.extend([dict_text[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the X vector, where dimension is #business vs #terms like IK\n",
    "vectorizer = TfidfVectorizer(max_df=0.9,stop_words=stop_words, max_features=5000, ngram_range=(1,1))\n",
    "X_cleaned = vectorizer.fit_transform(list_text).toarray()\n",
    "X_cleaned_sparse = csr_matrix(X_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check keywords\n",
    "#print(vectorizer.get_feature_names())\n",
    "#keywordList = X_cleaned_sparse[50].nonzero()[1]\n",
    "#for word in keywordList:\n",
    "#    print(vectorizer.get_feature_names()[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passing in the trained similarity matrx\n",
    "def individualKNNPrediction (similarityMatrix, predictionMatrix, kRange, validOrTestMatrix, itemBased=False):\n",
    "    \"Declaration for kRange = range(50,120,10)\"\n",
    "    #similarity = train(similarityMatrix)\n",
    "    MAP10 = {}\n",
    "    #Loop through the kvalues \n",
    "    for kValue in kRange:\n",
    "        if(itemBased==False):\n",
    "            user_item_p\n",
    "            rediction_score = predict(predictionMatrix, kValue, similarityMatrix, item_similarity_en= False)\n",
    "        else:\n",
    "            user_item_prediction_score = predict(predictionMatrix, kValue, similarityMatrix, item_similarity_en= True)\n",
    "        user_item_predict = prediction(user_item_prediction_score, 50, predictionMatrix)\n",
    "        user_item_res = evaluate(user_item_predict, validOrTestMatrix)\n",
    "        \n",
    "        \n",
    "        MAP10[kValue] = user_item_res.get('MAP@10')\n",
    "        \n",
    "    return MAP10\n",
    "\n",
    "def get_UC_Matrix(IC_Matrix,rtrain_implicit):\n",
    "    U_C_matrix_explicit = rtrain_implicit*IC_Matrix\n",
    "    U_C_matrix_implicit = getImplicitMatrix(U_C_matrix_explicit,3)\n",
    "    return U_C_matrix_explicit,U_C_matrix_implicit\n",
    "\n",
    "def getImplicitMatrix(sparseMatrix, threashold=0):\n",
    "    temp_matrix = sparse.csr_matrix(sparseMatrix.shape)\n",
    "    temp_matrix[(sparseMatrix > threashold).nonzero()] = 1\n",
    "    return temp_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passing in the trained similarity matrx\n",
    "def KNNPrediction (similarityMatrix, predictionMatrix, kValue, validOrTestMatrix, itemBased=False):\n",
    "\n",
    "    if(itemBased==False):\n",
    "        user_item_prediction_score = predict(predictionMatrix, kValue, similarityMatrix, item_similarity_en= False)\n",
    "    else:\n",
    "        user_item_prediction_score = predict(predictionMatrix, kValue, similarityMatrix, item_similarity_en= True)\n",
    "    user_item_predict = prediction(user_item_prediction_score, 50, predictionMatrix)\n",
    "    user_item_res = evaluate(user_item_predict, validOrTestMatrix)\n",
    "\n",
    "        \n",
    "    return user_item_res.get('MAP@10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDictToJson(dictionary, fileName, trainOrTest='train'):\n",
    "    json_fileName = \"{:s}.json\".format(fileName)\n",
    "    if(trainOrTest == 'train'):\n",
    "        json.dump(dictionary, open(\"crossValidation\\\\trainPerformance\\\\\"+json_fileName, 'w') )\n",
    "    else:\n",
    "        json.dump(dictionary, open(\"crossValidation\\\\testPerformance\\\\\"+json_fileName, 'w') )\n",
    "    \n",
    "\n",
    "def loadDict(fileName, trainOrTest='train'):\n",
    "    json_fileName = \"{:s}.json\".format(fileName)\n",
    "    # Read data from file:\n",
    "    if(trainOrTest == 'train'):\n",
    "        dataDict = json.load( open(\"crossValidation\\\\trainPerformance\\\\\"+json_fileName))\n",
    "    else:\n",
    "        dataDict = json.load( open(\"crossValidation\\\\testPerformance\\\\\"+json_fileName))\n",
    "    return dataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
     ]
    }
   ],
   "source": [
    "#Get UC matrices\n",
    "U_C_matrix_explicit,U_C_matrix_implicit = get_UC_Matrix(I_C_matrix,rtrain_implicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get User visit binary UI matrix\n",
    "#userVisitMatrix = getImplicitMatrix(rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 224)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_C_matrix_implicit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item based recommend & critique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intersections\n",
    "yonge_and_finch = Point(\"43.779824, -79.415665\")\n",
    "bloor_and_bathurst = Point(\"43.665194,-79.411208\")\n",
    "queen_and_spadina = Point(\"43.648772,-79.396259\")\n",
    "bloor_and_yonge = Point(\"43.670409,-79.386814\")\n",
    "dundas_and_yonge = Point(\"43.6561,-79.3802\")\n",
    "spadina_and_dundas = Point(\"43.653004,-79.398082\")\n",
    "\n",
    "#Set intersection for test case:\n",
    "intersection = spadina_and_dundas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IK TF-IDF\n",
    "IK_MATRIX = X_cleaned_sparse\n",
    "IK_similarity = train(IK_MATRIX)\n",
    "IC_similarity = train(I_C_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4996/4996 [00:03<00:00, 1425.64it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4996/4996 [00:06<00:00, 737.82it/s]\n"
     ]
    }
   ],
   "source": [
    "#Get IP dictionary\n",
    "#IP_df is the dataframe, IP_dictionary maps business_num_id with price range from 1-4\n",
    "IP_df, IP_dictionary = get_IP_matrix_dictionary(df, IK_similarity)\n",
    "IS_dictionary = get_IS_dictionary(df)\n",
    "ID_dictionary = get_ID_dictionary(df,list(set(df['business_num_id'])),intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4996, 224)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_C_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4996/4996 [00:28<00:00, 176.41it/s]\n"
     ]
    }
   ],
   "source": [
    "#IK TF-IDF\n",
    "#testPerformance['ItemKeyphrase'] = KNNPrediction(IK_similarity, rtrain, 110, rtest, itemBased=True)\n",
    "user_item_prediction_score = predict(rtrain, 110, IK_similarity, item_similarity_en= True)\n",
    "#user_item_predict = prediction(user_item_prediction_score, 50, rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_user_item_predict = prediction_modified(user_item_prediction_score, rtrain, 23)\n",
    "#all_user_item_predict[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take user num id 23 as example - Cuisine Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Take number of cuisine type\n",
    "# NumCuisine = I_C_matrix.shape[1]\n",
    "# print('Number of cuisin types: ', NumCuisine)\n",
    "\n",
    "# #Take the index of recommended restuarant\n",
    "# recommendIndex = 0\n",
    "# print('Recommending the ', recommendIndex, 'th restaurant')\n",
    "\n",
    "# # 1 * Item\n",
    "# sampleIndex = 23\n",
    "# SampleUserVector = rtrain[sampleIndex]\n",
    "\n",
    "# print('User initially rated restaurant :\\n', SampleUserVector)\n",
    "\n",
    "# #Get initial restuarant preferences\n",
    "# initialRestaurants = SampleUserVector.nonzero()[1]\n",
    "# print('initial user picked restaurants id: ', rtrain[23].nonzero()[1])\n",
    "\n",
    "# initialLiked = sparse.lil_matrix(SampleUserVector.shape)\n",
    "\n",
    "# initialLiked[(rtrain[23] >= 4.0).nonzero()] = 1\n",
    "\n",
    "# print('User initially liked restaurant:\\n', initialLiked)\n",
    "\n",
    "# initialLikedRestaurants = initialLiked.nonzero()[1]\n",
    "# print('initial user liked restaurants id:', initialLikedRestaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Adding the 1st recommended item \n",
    "# initialPreferenceAndRecommend = np.append(initialLikedRestaurants, user_item_predict[sampleIndex][recommendIndex])\n",
    "# print('Initial user restaurant preference vector: ', initialPreferenceAndRecommend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Get the initial picked restaurant cuisine type and category attribute \n",
    "# InitialCategoryId_List = []\n",
    "\n",
    "# for restuarant in initialPreferenceAndRecommend:\n",
    "#     Restaurant_name = df[df['business_num_id'] == restuarant].name.unique()\n",
    "#     Restaurant_category = df[df['business_num_id'] == restuarant].categories.unique()\n",
    "#     Restaurant_cat_id = []\n",
    "    \n",
    "#     for category in Restaurant_category:\n",
    "#         print(category)\n",
    "#         if ',' in category:\n",
    "#             Restaurant_cat_id.extend([IC_dictionary[cat] for cat in category.split(', ')])\n",
    "#         else:\n",
    "#             Restaurant_cat_id.append(IC_dictionary[category])\n",
    "        \n",
    "#     print(Restaurant_name, Restaurant_category, 'category id:', Restaurant_cat_id)\n",
    "    \n",
    "#     InitialCategoryId_List.extend(Restaurant_cat_id)\n",
    "\n",
    "# InitialCategoryId_List = sorted(list(set(InitialCategoryId_List)),reverse=False)\n",
    "# print(InitialCategoryId_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #initial category preference to numpy \n",
    "# columnVector = np.array(InitialCategoryId_List)\n",
    "# print('initial user prefernce list:', columnVector)\n",
    "\n",
    "# #Now cosntruct initial user preference binary vector \n",
    "# initPrefVector = sparse.csr_matrix((np.array([1] * columnVector.shape[0]), (np.array([0] * columnVector.shape[0]), columnVector)), \\\n",
    "#                            shape=(1, NumCuisine), dtype=np.float32)\n",
    "\n",
    "# print('Initial User Preference vector\\n', initPrefVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['business_num_id'] == 20].categories.unique()[0]\n",
    "\n",
    "#Print 1st recommended item \n",
    "#displayRestaurantInfo(recommendIndex, user_item_predict[sampleIndex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if item and categories are matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print('item category vector:\\n', I_C_matrix[4682])\n",
    "\n",
    "#print('item category in df:\\n', set(df[df['business_num_id'] == 4682].categories.values))\n",
    "\n",
    "#print('Category index in category dictionary:\\n', IC_dictionary['Tapas/Small Plates'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critique Upon Cuisine Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up criquited category \n",
    "#critiquiCategory = 'Asian Fusion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scenario 1 Criquite Upon Restaurant itself \n",
    "#Update recommended restuarant # \n",
    "#recommendIndex += 1\n",
    "#Update initialUserPreference to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scenario 2 Criquite the existing cuisine type \n",
    "#e.g. I don't want Asian Fusion \n",
    "# critiquiedIndex = IC_dictionary[critiquiCategory]\n",
    "# critiqueList = []\n",
    "# critiqueList.append(critiquiedIndex)\n",
    "# print('critiquing index at:', critiquiedIndex)\n",
    "\n",
    "# #Update initial user preference vector \n",
    "# modifiedInitPrefVect = initPrefVector.copy()\n",
    "# modifiedInitPrefVect[0,critiquiedIndex] = 0 \n",
    "# modifiedInitPrefVect.eliminate_zeros()\n",
    "# print('modified user preference \\n', modifiedInitPrefVect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recommend again \n",
    "#find the items that have this category, list of business_num_id's\n",
    "# critiquedItemsList = I_C_matrix.getcol(10).nonzero()[0]\n",
    "# print(critiquedItemsList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the items in the prediction vector for this user, set prediction score to 0 \n",
    "# user_item_predict[sampleIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Filter out critiqued items, sequence must remain the same \n",
    "#Modified_UI_predict = [x for x in user_item_predict[sampleIndex] if x not in critiquedItemsList]\n",
    "\n",
    "#Recommended\n",
    "#print('new recommend restaurant:', Modified_UI_predict[recommendIndex])\n",
    "\n",
    "#Display Recommend Restaurant Information\n",
    "#displayRestaurantInfo(recommendIndex, Modified_UI_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3 Positively Criquite Cuisine - \"I want ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #If want Hot Pot\n",
    "\n",
    "# #User input positive critique cuisine type\n",
    "# positiveCritiquiCategory = 'Chinese'\n",
    "\n",
    "# #Retrieve cuisine type index \n",
    "# positiveCritiquiedIndex = IC_dictionary[positiveCritiquiCategory]\n",
    "\n",
    "# #Modify the user preference vector \n",
    "\n",
    "# #Retrieve item matching item category \n",
    "# matchedItemList = I_C_matrix.getcol(positiveCritiquiedIndex).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to look from the entire entire prediction set \n",
    "#[x for x in Modified_UI_predict if x in I_C_matrix.getcol(positiveCritiquiedIndex).nonzero()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scenario I want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check businesses with categories\n",
    "#IC_dictionary\n",
    "#print(I_C_matrix.getcol(108))\n",
    "#df[df['business_num_id'] == 250].categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter test user index: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7000/7000 [00:04<00:00, 1565.88it/s]\n"
     ]
    }
   ],
   "source": [
    "#Initialize\n",
    "testUser_index = int(input(\"Enter test user index: \"))\n",
    "#Get user item initial prediction vector \n",
    "test_user_item_predict = prediction_modified(user_item_prediction_score, rtrain, testUser_index)\n",
    "\n",
    "#Compute user intitial preference vector \n",
    "#TODO\n",
    "\n",
    "categoryList = list(IC_dictionary.keys())\n",
    "categoryList[:15]\n",
    "\n",
    "maxDistance = max(ID_dictionary, key=ID_dictionary.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIfExhaustedList (current_user_item_predict, original_user_item_predict, critique_Res_list):\n",
    "    if len(current_user_item_predict) == 0:\n",
    "        print('Exhausted the recommendation list, fall back to initial preference')\n",
    "        current_user_item_predict = [item for item in original_user_item_predict if item not in critique_Res_list]\n",
    "        \n",
    "    return current_user_item_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateList (listTobeUpdated, listUsedToUpdate):\n",
    "    listTobeUpdated.append(listUsedToUpdate)\n",
    "    listUpdate = list(set(listTobeUpdated))\n",
    "    \n",
    "    return listUpdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4139,\n",
       " 2764,\n",
       " 883,\n",
       " 1908,\n",
       " 2202,\n",
       " 2261,\n",
       " 4201,\n",
       " 800,\n",
       " 376,\n",
       " 3298,\n",
       " 3282,\n",
       " 2766,\n",
       " 2970,\n",
       " 3277,\n",
       " 3922,\n",
       " 3442,\n",
       " 3721,\n",
       " 1681,\n",
       " 4664,\n",
       " 1176,\n",
       " 304,\n",
       " 4581,\n",
       " 1889,\n",
       " 1252,\n",
       " 4638,\n",
       " 1020,\n",
       " 539,\n",
       " 2254,\n",
       " 171,\n",
       " 3001,\n",
       " 2569,\n",
       " 4229,\n",
       " 1550,\n",
       " 4552,\n",
       " 4101,\n",
       " 1309,\n",
       " 3168,\n",
       " 32,\n",
       " 1999,\n",
       " 3387,\n",
       " 2020,\n",
       " 107,\n",
       " 1543,\n",
       " 3621,\n",
       " 2216,\n",
       " 2661,\n",
       " 637,\n",
       " 3388,\n",
       " 2278,\n",
       " 4870,\n",
       " 4767,\n",
       " 1945,\n",
       " 2549,\n",
       " 3129,\n",
       " 3059,\n",
       " 1919,\n",
       " 4434,\n",
       " 43,\n",
       " 1559,\n",
       " 3667,\n",
       " 1169,\n",
       " 1533,\n",
       " 1805,\n",
       " 4837,\n",
       " 4656,\n",
       " 1361,\n",
       " 3108,\n",
       " 4927,\n",
       " 35,\n",
       " 1027,\n",
       " 2568,\n",
       " 2612,\n",
       " 2307,\n",
       " 4645,\n",
       " 4548,\n",
       " 151,\n",
       " 4460,\n",
       " 4108,\n",
       " 3375,\n",
       " 435,\n",
       " 2226,\n",
       " 4891,\n",
       " 4076,\n",
       " 2863,\n",
       " 464,\n",
       " 1290,\n",
       " 317,\n",
       " 22,\n",
       " 712,\n",
       " 2128,\n",
       " 506,\n",
       " 1542,\n",
       " 4188,\n",
       " 290,\n",
       " 551,\n",
       " 1994,\n",
       " 266,\n",
       " 2050,\n",
       " 948,\n",
       " 4569,\n",
       " 4075,\n",
       " 4053,\n",
       " 4324,\n",
       " 1658,\n",
       " 857,\n",
       " 2657,\n",
       " 2650,\n",
       " 727,\n",
       " 1325,\n",
       " 387,\n",
       " 395,\n",
       " 997,\n",
       " 3316,\n",
       " 2088,\n",
       " 523,\n",
       " 4333,\n",
       " 609,\n",
       " 4480,\n",
       " 616,\n",
       " 2172,\n",
       " 2234,\n",
       " 970,\n",
       " 882,\n",
       " 2306,\n",
       " 1966,\n",
       " 3710,\n",
       " 900,\n",
       " 241,\n",
       " 4089,\n",
       " 2499,\n",
       " 2294,\n",
       " 4503,\n",
       " 4805,\n",
       " 3756,\n",
       " 264,\n",
       " 2848,\n",
       " 4682,\n",
       " 4178,\n",
       " 15,\n",
       " 2367,\n",
       " 2579,\n",
       " 4745,\n",
       " 861,\n",
       " 3703,\n",
       " 654,\n",
       " 2368,\n",
       " 4161,\n",
       " 179,\n",
       " 3402,\n",
       " 4366,\n",
       " 4459,\n",
       " 2962,\n",
       " 1944,\n",
       " 4064,\n",
       " 1247,\n",
       " 1443,\n",
       " 453,\n",
       " 780,\n",
       " 1492,\n",
       " 1580,\n",
       " 165,\n",
       " 3373,\n",
       " 3046,\n",
       " 1687,\n",
       " 1265,\n",
       " 996,\n",
       " 3356,\n",
       " 2992,\n",
       " 2956,\n",
       " 1756,\n",
       " 3347,\n",
       " 2781,\n",
       " 4844,\n",
       " 2452,\n",
       " 3618,\n",
       " 1983,\n",
       " 13,\n",
       " 3239,\n",
       " 3945,\n",
       " 4195,\n",
       " 4251,\n",
       " 1950,\n",
       " 2489,\n",
       " 3742,\n",
       " 3648,\n",
       " 1711,\n",
       " 2312,\n",
       " 2427,\n",
       " 265,\n",
       " 1091,\n",
       " 4116,\n",
       " 925,\n",
       " 180,\n",
       " 404,\n",
       " 427,\n",
       " 4040,\n",
       " 1131,\n",
       " 2009,\n",
       " 1689,\n",
       " 1503,\n",
       " 3350,\n",
       " 3195,\n",
       " 591,\n",
       " 3122,\n",
       " 3253,\n",
       " 1832,\n",
       " 3752,\n",
       " 3514,\n",
       " 4411,\n",
       " 2267,\n",
       " 283,\n",
       " 4008,\n",
       " 1014,\n",
       " 4523,\n",
       " 1476,\n",
       " 2891,\n",
       " 4004,\n",
       " 3272,\n",
       " 1156,\n",
       " 3915,\n",
       " 1552,\n",
       " 1316,\n",
       " 430,\n",
       " 3376,\n",
       " 2588,\n",
       " 2155,\n",
       " 1118,\n",
       " 3117,\n",
       " 1571,\n",
       " 2687,\n",
       " 1622,\n",
       " 1570,\n",
       " 4603,\n",
       " 4372,\n",
       " 961,\n",
       " 3098,\n",
       " 2526,\n",
       " 1737,\n",
       " 647,\n",
       " 4920,\n",
       " 4398,\n",
       " 4109,\n",
       " 374,\n",
       " 3040,\n",
       " 4936,\n",
       " 874,\n",
       " 111,\n",
       " 174,\n",
       " 541,\n",
       " 1526,\n",
       " 2354,\n",
       " 4362,\n",
       " 1064,\n",
       " 3694,\n",
       " 475,\n",
       " 1006,\n",
       " 4168,\n",
       " 4295,\n",
       " 4171,\n",
       " 1669,\n",
       " 771,\n",
       " 3152,\n",
       " 3073,\n",
       " 3918,\n",
       " 2112,\n",
       " 1141,\n",
       " 2026,\n",
       " 3406,\n",
       " 3292,\n",
       " 945,\n",
       " 2265,\n",
       " 3372,\n",
       " 4126,\n",
       " 168,\n",
       " 2887,\n",
       " 4410,\n",
       " 2120,\n",
       " 999,\n",
       " 4517,\n",
       " 2747,\n",
       " 2255,\n",
       " 2266,\n",
       " 1442,\n",
       " 1099,\n",
       " 642,\n",
       " 1893,\n",
       " 3173,\n",
       " 3230,\n",
       " 230,\n",
       " 817,\n",
       " 4528,\n",
       " 3628,\n",
       " 1844,\n",
       " 396,\n",
       " 2003,\n",
       " 4059,\n",
       " 4474,\n",
       " 1528,\n",
       " 3492,\n",
       " 2529,\n",
       " 130,\n",
       " 3422,\n",
       " 3379,\n",
       " 406,\n",
       " 3377,\n",
       " 4452,\n",
       " 787,\n",
       " 491,\n",
       " 3182,\n",
       " 1387,\n",
       " 4601,\n",
       " 2996,\n",
       " 2656,\n",
       " 3620,\n",
       " 3441,\n",
       " 2783,\n",
       " 1426,\n",
       " 877,\n",
       " 1054,\n",
       " 3999,\n",
       " 1875,\n",
       " 2196,\n",
       " 3796,\n",
       " 2328,\n",
       " 2036,\n",
       " 2105,\n",
       " 2192,\n",
       " 1393,\n",
       " 2084,\n",
       " 2111,\n",
       " 1544,\n",
       " 2191,\n",
       " 2109,\n",
       " 2081,\n",
       " 2082,\n",
       " 1424,\n",
       " 1574,\n",
       " 1551,\n",
       " 2100,\n",
       " 2098,\n",
       " 1558,\n",
       " 2199,\n",
       " 2205,\n",
       " 2204,\n",
       " 1407,\n",
       " 2195,\n",
       " 1557,\n",
       " 2103,\n",
       " 1553,\n",
       " 1555,\n",
       " 1415,\n",
       " 1400,\n",
       " 2090,\n",
       " 1569,\n",
       " 2197,\n",
       " 2213,\n",
       " 2145,\n",
       " 2148,\n",
       " 1489,\n",
       " 1508,\n",
       " 2137,\n",
       " 1487,\n",
       " 1471,\n",
       " 2153,\n",
       " 2157,\n",
       " 1454,\n",
       " 2162,\n",
       " 1478,\n",
       " 1474,\n",
       " 1529,\n",
       " 1433,\n",
       " 2119,\n",
       " 1435,\n",
       " 2183,\n",
       " 2123,\n",
       " 2124,\n",
       " 1524,\n",
       " 1534,\n",
       " 1538,\n",
       " 1535,\n",
       " 1532,\n",
       " 2174,\n",
       " 1446,\n",
       " 1511,\n",
       " 2132,\n",
       " 2178,\n",
       " 1436,\n",
       " 2127,\n",
       " 1786,\n",
       " 1955,\n",
       " 1798,\n",
       " 1957,\n",
       " 1958,\n",
       " 1796,\n",
       " 1794,\n",
       " 1793,\n",
       " 1818,\n",
       " 1946,\n",
       " 1948,\n",
       " 1952,\n",
       " 1804,\n",
       " 1785,\n",
       " 1752,\n",
       " 1976,\n",
       " 1747,\n",
       " 1978,\n",
       " 1742,\n",
       " 1781,\n",
       " 1780,\n",
       " 1778,\n",
       " 1974,\n",
       " 1772,\n",
       " 1763,\n",
       " 1825,\n",
       " 1915,\n",
       " 1867,\n",
       " 1865,\n",
       " 1879,\n",
       " 1890,\n",
       " 1899,\n",
       " 1903,\n",
       " 1905,\n",
       " 1863,\n",
       " 1918,\n",
       " 1843,\n",
       " 1842,\n",
       " 1841,\n",
       " 1838,\n",
       " 1846,\n",
       " 1933,\n",
       " 1934,\n",
       " 1936,\n",
       " 1828,\n",
       " 1931,\n",
       " 1856,\n",
       " 1920,\n",
       " 1922,\n",
       " 1925,\n",
       " 1927,\n",
       " 1852,\n",
       " 1850,\n",
       " 1849,\n",
       " 2046,\n",
       " 2049,\n",
       " 1618,\n",
       " 2038,\n",
       " 1651,\n",
       " 1646,\n",
       " 1645,\n",
       " 1643,\n",
       " 1640,\n",
       " 1634,\n",
       " 2068,\n",
       " 1591,\n",
       " 1590,\n",
       " 1584,\n",
       " 1583,\n",
       " 1600,\n",
       " 1612,\n",
       " 1608,\n",
       " 1604,\n",
       " 1603,\n",
       " 1652,\n",
       " 1654,\n",
       " 1996,\n",
       " 1708,\n",
       " 1733,\n",
       " 1731,\n",
       " 1694,\n",
       " 1667,\n",
       " 1664,\n",
       " 2004,\n",
       " 1691,\n",
       " 2010,\n",
       " 1676,\n",
       " 2016,\n",
       " 488,\n",
       " 480,\n",
       " 476,\n",
       " 473,\n",
       " 472,\n",
       " 469,\n",
       " 462,\n",
       " 451,\n",
       " 529,\n",
       " 526,\n",
       " 518,\n",
       " 514,\n",
       " 513,\n",
       " 505,\n",
       " 499,\n",
       " 496,\n",
       " 532,\n",
       " 398,\n",
       " 392,\n",
       " 390,\n",
       " 384,\n",
       " 382,\n",
       " 381,\n",
       " 372,\n",
       " 364,\n",
       " 399,\n",
       " 402,\n",
       " 446,\n",
       " 442,\n",
       " 408,\n",
       " 666,\n",
       " 661,\n",
       " 658,\n",
       " 648,\n",
       " 646,\n",
       " 644,\n",
       " 639,\n",
       " 631,\n",
       " 626,\n",
       " 624,\n",
       " 665,\n",
       " 621,\n",
       " 701,\n",
       " 694,\n",
       " 692,\n",
       " 689,\n",
       " 679,\n",
       " 678,\n",
       " 673,\n",
       " 671,\n",
       " 575,\n",
       " 567,\n",
       " 560,\n",
       " 553,\n",
       " 550,\n",
       " 576,\n",
       " 614,\n",
       " 613,\n",
       " 607,\n",
       " 599,\n",
       " 597,\n",
       " 355,\n",
       " 129,\n",
       " 108,\n",
       " 104,\n",
       " 101,\n",
       " 96,\n",
       " 95,\n",
       " 162,\n",
       " 147,\n",
       " 135,\n",
       " 134,\n",
       " 46,\n",
       " 44,\n",
       " 29,\n",
       " 23,\n",
       " 19,\n",
       " 18,\n",
       " 45,\n",
       " 82,\n",
       " 56,\n",
       " 49,\n",
       " 308,\n",
       " 303,\n",
       " 293,\n",
       " 292,\n",
       " 291,\n",
       " 289,\n",
       " 288,\n",
       " 284,\n",
       " 278,\n",
       " 273,\n",
       " 271,\n",
       " 269,\n",
       " 311,\n",
       " 348,\n",
       " 343,\n",
       " 341,\n",
       " 333,\n",
       " 330,\n",
       " 328,\n",
       " 326,\n",
       " 313,\n",
       " 261,\n",
       " 220,\n",
       " 215,\n",
       " 209,\n",
       " 207,\n",
       " 184,\n",
       " 221,\n",
       " 246,\n",
       " 224,\n",
       " 240,\n",
       " 238,\n",
       " 234,\n",
       " 229,\n",
       " 227,\n",
       " 1179,\n",
       " 1171,\n",
       " 1157,\n",
       " 1148,\n",
       " 1180,\n",
       " 1221,\n",
       " 1211,\n",
       " 1205,\n",
       " 1200,\n",
       " 1198,\n",
       " 1195,\n",
       " 1193,\n",
       " 1192,\n",
       " 1191,\n",
       " 1139,\n",
       " 1223,\n",
       " 1093,\n",
       " 1080,\n",
       " 1078,\n",
       " 1076,\n",
       " 1074,\n",
       " 1096,\n",
       " 1097,\n",
       " 1133,\n",
       " 1127,\n",
       " 1125,\n",
       " 1123,\n",
       " 1117,\n",
       " 1112,\n",
       " 1100,\n",
       " 1222,\n",
       " 1349,\n",
       " 1343,\n",
       " 1336,\n",
       " 1335,\n",
       " 1333,\n",
       " 1324,\n",
       " 1315,\n",
       " 1311,\n",
       " 1308,\n",
       " 1350,\n",
       " 1353,\n",
       " 1386,\n",
       " 1378,\n",
       " 1369,\n",
       " 1363,\n",
       " 1362,\n",
       " 1306,\n",
       " 1304,\n",
       " 1253,\n",
       " 1243,\n",
       " 1241,\n",
       " 1236,\n",
       " 1234,\n",
       " 1231,\n",
       " 1230,\n",
       " 1228,\n",
       " 1226,\n",
       " 1266,\n",
       " 1273,\n",
       " 1271,\n",
       " 1267,\n",
       " 1058,\n",
       " 833,\n",
       " 827,\n",
       " 805,\n",
       " 804,\n",
       " 798,\n",
       " 867,\n",
       " 862,\n",
       " 858,\n",
       " 856,\n",
       " 854,\n",
       " 852,\n",
       " 849,\n",
       " 848,\n",
       " 843,\n",
       " 748,\n",
       " 735,\n",
       " 734,\n",
       " 731,\n",
       " 726,\n",
       " 724,\n",
       " 716,\n",
       " 711,\n",
       " 749,\n",
       " 784,\n",
       " 772,\n",
       " 770,\n",
       " 757,\n",
       " 756,\n",
       " 755,\n",
       " 1003,\n",
       " 993,\n",
       " 992,\n",
       " 991,\n",
       " 989,\n",
       " 980,\n",
       " 974,\n",
       " 972,\n",
       " 1052,\n",
       " 1050,\n",
       " 1048,\n",
       " 1034,\n",
       " 1031,\n",
       " 1028,\n",
       " 1025,\n",
       " 1018,\n",
       " 1016,\n",
       " 968,\n",
       " 917,\n",
       " 913,\n",
       " 909,\n",
       " 908,\n",
       " 905,\n",
       " 897,\n",
       " 896,\n",
       " 893,\n",
       " 881,\n",
       " 944,\n",
       " 957,\n",
       " 951,\n",
       " 950,\n",
       " 931,\n",
       " 926,\n",
       " 922,\n",
       " 2219,\n",
       " 2221,\n",
       " 4077,\n",
       " 4069,\n",
       " 4087,\n",
       " 4062,\n",
       " 4061,\n",
       " 4052,\n",
       " 4050,\n",
       " 4049,\n",
       " 4047,\n",
       " 4086,\n",
       " 4113,\n",
       " 4119,\n",
       " 4127,\n",
       " 4107,\n",
       " 4100,\n",
       " 4096,\n",
       " 4092,\n",
       " 3985,\n",
       " 4037,\n",
       " 3990,\n",
       " 3991,\n",
       " 3976,\n",
       " 3970,\n",
       " 3968,\n",
       " 3997,\n",
       " 4022,\n",
       " 4035,\n",
       " 4025,\n",
       " 4023,\n",
       " 4020,\n",
       " 4018,\n",
       " 4016,\n",
       " 4005,\n",
       " 4132,\n",
       " 4248,\n",
       " 4252,\n",
       " 4254,\n",
       " 4258,\n",
       " 4262,\n",
       " 4244,\n",
       " 4243,\n",
       " 4241,\n",
       " 4235,\n",
       " 4228,\n",
       " 4224,\n",
       " 4263,\n",
       " 4289,\n",
       " 4294,\n",
       " 4300,\n",
       " 4302,\n",
       " 4280,\n",
       " 4279,\n",
       " 4278,\n",
       " 4268,\n",
       " 4218,\n",
       " 4162,\n",
       " 4148,\n",
       " 4145,\n",
       " 4136,\n",
       " 4216,\n",
       " 4214,\n",
       " 4211,\n",
       " 4207,\n",
       " 4192,\n",
       " 3738,\n",
       " 3744,\n",
       " 3750,\n",
       " 3755,\n",
       " 3734,\n",
       " 3730,\n",
       " 3757,\n",
       " 3760,\n",
       " 3795,\n",
       " 3779,\n",
       " 3782,\n",
       " 3784,\n",
       " 3789,\n",
       " 3776,\n",
       " 3769,\n",
       " 3768,\n",
       " 3764,\n",
       " 3715,\n",
       " 3714,\n",
       " 3647,\n",
       " 3651,\n",
       " 3712,\n",
       " 3663,\n",
       " 3666,\n",
       " 3680,\n",
       " 3674,\n",
       " 3903,\n",
       " 3908,\n",
       " 3916,\n",
       " 3899,\n",
       " 3891,\n",
       " 3883,\n",
       " 3881,\n",
       " 3880,\n",
       " 3924,\n",
       " 3944,\n",
       " 3946,\n",
       " 3950,\n",
       " 3957,\n",
       " 3942,\n",
       " 3936,\n",
       " 3932,\n",
       " 3926,\n",
       " 3823,\n",
       " 3832,\n",
       " 3816,\n",
       " 3813,\n",
       " 3810,\n",
       " 3806,\n",
       " 3803,\n",
       " 3801,\n",
       " 3833,\n",
       " 3871,\n",
       " 3868,\n",
       " 3860,\n",
       " 3836,\n",
       " 3837,\n",
       " 4306,\n",
       " 4777,\n",
       " 4782,\n",
       " 4786,\n",
       " 4787,\n",
       " 4765,\n",
       " 4760,\n",
       " 4791,\n",
       " 4830,\n",
       " 4820,\n",
       " 4815,\n",
       " 4818,\n",
       " 4821,\n",
       " 4792,\n",
       " 4823,\n",
       " 4796,\n",
       " 4749,\n",
       " 4748,\n",
       " 4747,\n",
       " 4702,\n",
       " 4705,\n",
       " 4691,\n",
       " 4683,\n",
       " 4673,\n",
       " 4671,\n",
       " 4670,\n",
       " 4743,\n",
       " 4742,\n",
       " 4727,\n",
       " 4726,\n",
       " 4722,\n",
       " 4714,\n",
       " 4829,\n",
       " 4832,\n",
       " 4940,\n",
       " 4942,\n",
       " 4946,\n",
       " 4952,\n",
       " 4935,\n",
       " 4928,\n",
       " 4924,\n",
       " 4985,\n",
       " 4979,\n",
       " 4983,\n",
       " 4991,\n",
       " 4993,\n",
       " 4973,\n",
       " 4971,\n",
       " 4966,\n",
       " 4965,\n",
       " 4963,\n",
       " 4962,\n",
       " 4862,\n",
       " 4863,\n",
       " 4851,\n",
       " 4847,\n",
       " 4843,\n",
       " 4839,\n",
       " 4838,\n",
       " 4909,\n",
       " 4902,\n",
       " 4876,\n",
       " 4890,\n",
       " 4879,\n",
       " 4665,\n",
       " 4419,\n",
       " 4432,\n",
       " 4415,\n",
       " 4413,\n",
       " 4403,\n",
       " 4402,\n",
       " 4401,\n",
       " 4471,\n",
       " 4472,\n",
       " 4476,\n",
       " 4457,\n",
       " 4455,\n",
       " 4454,\n",
       " 4453,\n",
       " 4447,\n",
       " 4395,\n",
       " 4393,\n",
       " 4339,\n",
       " 4332,\n",
       " 4338,\n",
       " 4342,\n",
       " 4329,\n",
       " 4318,\n",
       " 4383,\n",
       " 4380,\n",
       " 4376,\n",
       " 4365,\n",
       " 4359,\n",
       " 4479,\n",
       " 4599,\n",
       " 4611,\n",
       " 4596,\n",
       " 4588,\n",
       " 4622,\n",
       " 4623,\n",
       " 4657,\n",
       " 4654,\n",
       " 4652,\n",
       " 4640,\n",
       " 4635,\n",
       " 4631,\n",
       " 4630,\n",
       " 4579,\n",
       " 4509,\n",
       " 4510,\n",
       " 4514,\n",
       " 4522,\n",
       " 4505,\n",
       " 4504,\n",
       " 4500,\n",
       " 4494,\n",
       " 4490,\n",
       " 4530,\n",
       " 4573,\n",
       " 4570,\n",
       " 4568,\n",
       " 4557,\n",
       " 4545,\n",
       " 4544,\n",
       " 4537,\n",
       " 3622,\n",
       " 2697,\n",
       " 2700,\n",
       " 2710,\n",
       " 2696,\n",
       " 2680,\n",
       " 2715,\n",
       " 2741,\n",
       " 2742,\n",
       " 2748,\n",
       " 2729,\n",
       " 2721,\n",
       " 2606,\n",
       " 2613,\n",
       " 2616,\n",
       " 2582,\n",
       " 2578,\n",
       " 2646,\n",
       " 2654,\n",
       " 2638,\n",
       " 2637,\n",
       " 2628,\n",
       " 2627,\n",
       " 2625,\n",
       " 2753,\n",
       " 2855,\n",
       " 2857,\n",
       " 2865,\n",
       " 2866,\n",
       " 2868,\n",
       " 2853,\n",
       " 2852,\n",
       " 2849,\n",
       " ...]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_user_item_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Recommendation\n",
      "Enter Stop any time to exist loop\n",
      "Recommending...\n",
      "------------------------------------------------------\n",
      "Business_num_id:  4501\n",
      "Restaurant name: Lai Wah Heen \n",
      "Cuisine Type:  Dim Sum \n",
      "Price: $$$ \n",
      "Rating: 3.5 \n",
      "Distance: 1.0 km\n",
      "------------------------------------------------------\n",
      "\n",
      "????????????????\n",
      "You Like? ('yes', 'no') no\n",
      "What feature to critique: (name, cuisine, price, rating, distance)cuisine\n",
      "Positive or negative: positive\n",
      "Critique value: (Chicken WingsTabletop GamesSyrian...)chinese\n",
      "????????????????\n",
      "\n",
      "matching list: [  4   9  21  36 111 114 125 132 151 153]\n",
      "Recommending...\n",
      "------------------------------------------------------\n",
      "Business_num_id:  111\n",
      "Restaurant name: Wah Too \n",
      "Cuisine Type:  Seafood, Chinese \n",
      "Price: $$ \n",
      "Rating: 3.5 \n",
      "Distance: 0.9 km\n",
      "------------------------------------------------------\n",
      "\n",
      "????????????????\n",
      "You Like? ('yes', 'no') no\n",
      "What feature to critique: (name, cuisine, price, rating, distance)distance\n",
      "Positive or negative: positive\n",
      "Critique value: (closer, further, or distance in range 0.5 ~169in every 0.5km)closer\n",
      "????????????????\n",
      "\n",
      "Critiquing distance >= 0.9\n",
      "Recommending...\n",
      "------------------------------------------------------\n",
      "Business_num_id:  151\n",
      "Restaurant name: Origination \n",
      "Cuisine Type:  Chinese \n",
      "Price: $$ \n",
      "Rating: 3.5 \n",
      "Distance: 0.2 km\n",
      "------------------------------------------------------\n",
      "\n",
      "????????????????\n",
      "You Like? ('yes', 'no') yes\n",
      "BYE :)\n"
     ]
    }
   ],
   "source": [
    "#Initialize variable \n",
    "\n",
    "#Need to initialize to have only restaurants within 2.0km recommended, restaurants distance above 2.0 will be critiqued\n",
    "critique_Distance = 2.0\n",
    "\n",
    "#list of restaurant don't wanted, initialized as restaurants distance >2.0km\n",
    "critique_Res_list = [key for (key, value) in ID_dictionary.items() if value > critique_Distance] \n",
    "critique_Price_list = []\n",
    "ciritiqued_Rating_list = []\n",
    "#list of categories don't wanted - Accumulated \n",
    "critique_Cat_list = []\n",
    "\n",
    "#Used to fall back, when user inputs are conflicted \n",
    "critiqued_Res_AccrdName_list= []\n",
    "critiqued_Res_AccrdCuisine_list = []\n",
    "critiqued_Res_AccrdPrice_list = []\n",
    "critiqued_Res_AccrdStar_list = []\n",
    "critiqued_Res_AccrdDistance_list = [key for (key, value) in ID_dictionary.items() if value > critique_Distance] \n",
    "\n",
    "#Categories that explicitly wanted, ONLY 1 FOR NOW\n",
    "wanted_Category_index = None\n",
    "#wanted_Price_list = []\n",
    "#Recommendation info\n",
    "recommendIndex = 0  \n",
    "outPutString = ''\n",
    "\n",
    "#Initial user item vector for critiquing process \n",
    "current_user_item_predict = [item for item in test_user_item_predict if item not in critique_Res_list]\n",
    "\n",
    "print('Initial Recommendation')\n",
    "print('Enter Stop any time to exist loop')\n",
    "#displayRestaurantInfo(recommendIndex, test_user_item_predict)\n",
    "\n",
    "while True:\n",
    "    satisfied = 'None'\n",
    "    feature = 'None'\n",
    "    positiveOrNegative = 'None'\n",
    "    critiqueValue = 'None'\n",
    "    \n",
    "    print('Recommending...')\n",
    "    displayRestaurantInfo(recommendIndex, current_user_item_predict,df,ID_dictionary)\n",
    "    \n",
    "    #Current recommendation cuisine type \n",
    "    currentCuisineType = df[df['business_num_id'] == current_user_item_predict[recommendIndex]]\\\n",
    "                                                        .categories.unique()[0].split(', ')\n",
    "    currentPriceLabel = IP_dictionary[current_user_item_predict[recommendIndex]]\n",
    "    currentRating = IS_dictionary[current_user_item_predict[recommendIndex]]\n",
    "    currentDistance = ID_dictionary[current_user_item_predict[recommendIndex]]\n",
    "    \n",
    "    #First testing cuisine type\n",
    "    print('\\n????????????????')\n",
    "    \n",
    "    while satisfied.lower() not in ['yes', 'no', 'stop']: \n",
    "        satisfied = input(\"You Like? ('yes', 'no') \").strip().lower()\n",
    "    \n",
    "    if satisfied == 'stop' or satisfied == 'yes':\n",
    "        print('BYE :)')\n",
    "        break\n",
    "        \n",
    "    #When satisfied is NO, take in feature\n",
    "    while feature.lower() not in ['name', 'cuisine', 'price', 'distance', 'rating', 'stop']: \n",
    "        feature = input(\"What feature to critique: (name, cuisine, price, rating, distance)\")\n",
    "    \n",
    "    if feature == 'stop':\n",
    "        break\n",
    "    \n",
    "    #Take in Positive or nagative \n",
    "    if feature != 'name':\n",
    "        while positiveOrNegative.lower() not in ['positive', 'negative', 'stop']:\n",
    "            positiveOrNegative = input(\"Positive or negative: \")\n",
    "    \n",
    "    if positiveOrNegative == 'stop':\n",
    "        break\n",
    "    \n",
    "    #Only ask for critique value when not critiquing restaurant name, or not negatively critiquing price \n",
    "    if feature != 'name' and not(feature == 'price' and positiveOrNegative == 'negative')\\\n",
    "    and not(feature == 'rating' and positiveOrNegative == 'negative')\\\n",
    "    and not(feature == 'distance' and positiveOrNegative == 'negative'): \n",
    "        #The valid values to be critiuqed that can pass in \n",
    "        validCritiqueValueList = []\n",
    "        #negatively critique current cuisine type, should only enter current cruisine type\n",
    "        if 'cuisine' in feature and 'negative' in positiveOrNegative:\n",
    "            validCritiqueValueList = [cat.strip().lower() for cat in currentCuisineType] + ['stop']\n",
    "            outPutString = '(' + currentCuisineType[0] +')'\n",
    "        elif 'cuisine' in feature and 'positive' in positiveOrNegative:\n",
    "            validCritiqueValueList = [cat.strip().lower() for cat in categoryList] \n",
    "            categories = list(set(IC_dictionary.keys())) + ['stop']\n",
    "            outPutString = '(' + categories[0] +categories[1] +categories[2] +'...)'\n",
    "        #Can only enter cheapter or more expensive\n",
    "        elif 'price' in feature:\n",
    "            validCritiqueValueList = ['cheaper', 'more expensive','stop']\n",
    "            outPutString = '(cheaper, more expensive)'\n",
    "        elif 'rating' in feature:\n",
    "            validCritiqueValueList = [star/10 for star in range(0,51,1)] + ['stop']\n",
    "            outPutString = '(0 ~ 5.0 with 0.1 increment)'\n",
    "        #positively critique distance\n",
    "        elif 'distance' in feature:\n",
    "            validCritiqueDis = [str(i/10) for i in range(0,int((maxDistance+0.5)*10),5)][1:] \n",
    "            validCritiqueValueList = ['closer', 'further'] + validCritiqueDis + ['stop']\n",
    "            outPutString = '(closer, further, or distance in range 0.5 ~' + str(maxDistance) + 'in every 0.5km)' \n",
    "        #Prompt to ask critique value\n",
    "        while critiqueValue not in validCritiqueValueList:\n",
    "            critiqueValue = input(\"Critique value: \" + outPutString).strip().lower()\n",
    "        \n",
    "        if critiqueValue == 'stop':\n",
    "            break\n",
    "            \n",
    "    print('????????????????\\n')\n",
    "    \n",
    "    #Need to check if the critiqued value is listed for the recommended item \n",
    "    \n",
    "    #If user starts critiquing, the current showend restaurant will be in critiqued list \n",
    "    critique_Res_list.append(current_user_item_predict[recommendIndex])\n",
    "    critique_Res_list = list(set(critique_Res_list))\n",
    "    \n",
    "    \n",
    "    #Scenario 1 - Critique Restaurant name \n",
    "    if 'name' in feature:\n",
    "        \n",
    "        print(\"Saving critiqued item: \", current_user_item_predict[recommendIndex])\n",
    "        \n",
    "        #Save critiqued restaurant to list \n",
    "        #critique_Res_list.append(current_user_item_predict[recommendIndex])\n",
    "        #critique_Res_list = list(set(critique_Res_list))\n",
    "        #critique_Res_list = updateList(critique_Res_list, current_user_item_predict[recommendIndex])\n",
    "        \n",
    "        #update critiqued restaurant list according to name\n",
    "        #critiqued_Res_AccrdName_list.append(current_user_item_predict[recommendIndex])\n",
    "        critiqued_Res_AccrdName_list = updateList(critiqued_Res_AccrdName_list, current_user_item_predict[recommendIndex])\n",
    "        \n",
    "        current_user_item_predict = current_user_item_predict[recommendIndex+1 :]\n",
    "        #Handling all items critiqued case\n",
    "        \"\"\"BUT I'M NOT COUNTING THE FACTOR THAT THE USER HAD LIKED CATEGORIES\"\"\"\n",
    "        current_user_item_predict = checkIfExhaustedList(current_user_item_predict, test_user_item_predict, critique_Res_list)\n",
    "            \n",
    "    \n",
    "    #Scenario 2 - Negatively critique restaurant features \n",
    "    if ('cuisine' in feature.lower()) and ('negative' in positiveOrNegative.lower()):\n",
    "        \n",
    "        #Find the correct category name the user want to critique in current recommended item categories\n",
    "        critiqueValue = [cuisine for cuisine in currentCuisineType if critiqueValue.strip().lower() in cuisine.lower()][0]\n",
    "        \n",
    "        critiquied_Cat_Index = IC_dictionary[critiqueValue]\n",
    "        print('Saving negatively critiqued cuisine type:', critiqueValue, ', cuisine index: ', critiquied_Cat_Index)\n",
    "        critique_Cat_list.append(critiquied_Cat_Index)\n",
    "        \n",
    "        #Handling extreme case - critique categories previously requested \n",
    "        if critiquied_Cat_Index == wanted_Category_index:\n",
    "            #Reset wanted category index\n",
    "            wanted_Category_index = None\n",
    "            print('You are critiquing a cuisine type you previously requested\\n Fall back to initial preference')\n",
    "            current_user_item_predict = [item for item in test_user_item_predict if item not in critique_Res_list]\n",
    "        \n",
    "        \n",
    "        #Find list of restaurants to filter out \n",
    "        critiquedItemsList = I_C_matrix.getcol(critiquied_Cat_Index).nonzero()[0]\n",
    "        \n",
    "        critiqued_Res_AccrdCuisine_list.append(critiquedItemsList)\n",
    "        critiqued_Res_AccrdCuisine_list =list(set(critique_Res_list))\n",
    "        \n",
    "        #Updating the critiqued items to list\n",
    "        print('Saving critiqued items at index: ', critiquedItemsList[:5], '...')\n",
    "        critique_Res_list.extend(list(critiquedItemsList))\n",
    "        critique_Res_list = list(set(critique_Res_list))\n",
    "        \n",
    "        #Filter out critiqued items, sequence must remain the same \n",
    "        #Update current valid set \n",
    "        current_user_item_predict = [item for item in current_user_item_predict if item not in critique_Res_list]\n",
    "\n",
    "        #Handle case where run out of items! - Fall Back!\n",
    "        current_user_item_predict = checkIfExhaustedList(current_user_item_predict, test_user_item_predict, critique_Res_list)\n",
    "        \n",
    "        \n",
    "    \n",
    "    #Scenario 3 - Positively critique restaurant cuisine type \n",
    "    if 'cuisine' in feature and 'positive' in positiveOrNegative:\n",
    "\n",
    "        #Find the correct category within all the categories list - assuming exact word typed in \n",
    "        \"\"\"Add ERROR HANDLING HERE\"\"\"\n",
    "        positiveCritiquiCategory = [cuisine for cuisine in categoryList if critiqueValue.strip().lower() in cuisine.lower()][0]\n",
    "        \n",
    "        #Retrieve cuisine type index \n",
    "        positiveCritiquiedIndex = IC_dictionary[positiveCritiquiCategory]\n",
    "        \n",
    "        #Get the preferred category index \n",
    "        wanted_Category_index = positiveCritiquiedIndex\n",
    "        \n",
    "        #Check if user have previously critiqued - remove it\n",
    "        if positiveCritiquiedIndex in critique_Cat_list:\n",
    "            critique_Cat_list.remove(positiveCritiquiedIndex)\n",
    "            \n",
    "            #UPDATE CRITIQUE RESTAURANT LIST \n",
    "            \"\"\"TODO\"\"\"\n",
    "        \n",
    "        #Retrieve items matching item category \n",
    "        matchedResList = I_C_matrix.getcol(positiveCritiquiedIndex).nonzero()[0]\n",
    "        print('matching list:', matchedResList[:10])\n",
    "        \n",
    "        #Update current valid set, make sure not in critiqued restaurant set \n",
    "        current_user_item_predict = [item for item in matchedResList if item not in critique_Res_list]\n",
    "        \n",
    "        \n",
    "    #Scenario 4 - Negative critique restuarant price e.g. \"I don't want expensive restaurants\" OR positive \"I want cheaper\"\n",
    "    #Does not pass in anything ASSUMING ONLY GOING DOWN\n",
    "    #Scenario 5 - Positively critique restaurant price, \"I want fine dining\", wanting more expensive restaurant \n",
    "    if 'price' in feature:\n",
    "        \n",
    "        #Update critiqued price list\n",
    "        if 'positive' in positiveOrNegative and critiqueValue == 'more expensive':\n",
    "            critique_Price_list.extend([price for price in range(1,currentPriceLabel+1,1)])\n",
    "        #Negative or positive, cheaper     \n",
    "        else:     \n",
    "            critique_Price_list.extend([price for price in range(currentPriceLabel,5,1)])\n",
    "            \n",
    "        #Deduplicate\n",
    "        critique_Price_list = list(set(critique_Price_list))\n",
    "        \n",
    "        #Check if critiqued all price range, if exhausted, fall back \n",
    "        if len(critique_Price_list) == 4:\n",
    "            print('You have exhaused the price range option, showing the most', critiqueValue, 'option')\n",
    "            #Clear the previous critiqued restuarants based on price out of critiqued restuarant list \n",
    "            #POTENTIAL ISSUE HERE, MAY BE ERASING SOME RESTAURANTS CRITIQUED IN OTHER STEPS ... NEED TO RECHECK\n",
    "            critique_Res_list = [item for item in test_user_item_predict if\\\n",
    "                                item in list(set(critiqued_Res_AccrdName_list+ critiqued_Res_AccrdCuisine_list+\\\n",
    "                                                 critiqued_Res_AccrdStar_list + critiqued_Res_AccrdDistance_list))]\n",
    "            \n",
    "            #Clear out the restuarants critiquied by price \n",
    "            critiqued_Res_AccrdPrice_list = []\n",
    "            \n",
    "            #Restore default critiquing price list \n",
    "            if critiqueValue == 'more expensive':\n",
    "                critique_Price_list = [1,2,3]\n",
    "            else:\n",
    "                critique_Price_list = [2,3,4]  \n",
    "        \n",
    "        print('Critiqing price at range:', ['$'*label for label in critique_Price_list])\n",
    "        \n",
    "        #Find the list of restaurants to critique \n",
    "        listCritiqueRestaurant = [key  for (key, value) in IP_dictionary.items() if value in critique_Price_list]\n",
    "        \n",
    "        #Record the list of restaurants critiqued so far based on price \n",
    "        critiqued_Res_AccrdPrice_list.extend(listCritiqueRestaurant)\n",
    "        critiqued_Res_AccrdPrice_list = list(set(critiqued_Res_AccrdPrice_list))\n",
    "        \n",
    "        #Update the critiqued restaurant list\n",
    "        critique_Res_list.extend(listCritiqueRestaurant)\n",
    "        critique_Res_list = list(set(critique_Res_list))\n",
    "        \n",
    "        #Updating the critiqued items to list\n",
    "        print('Saving critiqued items at index: ', listCritiqueRestaurant[:5], '...')\n",
    "        \n",
    "        #Filter out critiqued items, sequence must remain the same \n",
    "        #Update current valid set \n",
    "        current_user_item_predict = [item for item in current_user_item_predict if item not in critique_Res_list]\n",
    "        \n",
    "        #Check if exhaused list\n",
    "        current_user_item_predict = checkIfExhaustedList(current_user_item_predict, test_user_item_predict, critique_Res_list)\n",
    "        \n",
    "        #Recommended\n",
    "        print('Re-recommending...')\n",
    "        \n",
    "    #Scenario 6 negatively critique rating & positively critique rating: both goes up \n",
    "    #\"I don't want ratings this low?\" \"I don't want restuarants with rating below XXX\" \"I want restaurants with rating above XXX\"\n",
    "    #I will critique the ratings below the current rating or specific rating \n",
    "    if 'rating' in feature:\n",
    "        #Critiquing the restaurants that has ratings equal and below this restaurant \n",
    "        if 'negative' in positiveOrNegative:\n",
    "            ciritiqued_Rating_list.extend([rating/10 for rating in range(0,int(currentRating*10+1),5)])\n",
    "        else:\n",
    "            ciritiqued_Rating_list.extend([rating/10 for rating in range(0,int(critiqueValue*10+1),5)])\n",
    "        \n",
    "        ciritiqued_Rating_list = list(set(ciritiqued_Rating_list))\n",
    "            \n",
    "        #If critiqued all price\n",
    "        if ciritiqued_Rating_list == [price/10 for price in range(0,int(5.0 *10 +1),5)]:\n",
    "            print('exhausted list, no better restaurants, recommending he finest restaurants')\n",
    "            #reset critiqued restaurant list \n",
    "            critique_Res_list = [item for item in test_user_item_predict if\\\n",
    "                                item in list(set(critiqued_Res_AccrdName_list+critiqued_Res_AccrdCuisine_list+\\\n",
    "                                                critiqued_Res_AccrdPrice_list+critiqued_Res_AccrdDistance_list))] \n",
    "\n",
    "            #Clear out the restuarants critiquied by rating \n",
    "            critiqued_Res_AccrdStar_list = []\n",
    "\n",
    "            ciritiqued_Rating_list = [rating/10 for rating in range(0,int(currentRating*10+1),5)]\n",
    "\n",
    "        print('critiquing restaurants at rating rangeing at:',ciritiqued_Rating_list)\n",
    "\n",
    "        #Get the list of restaurants to critique\n",
    "        listCritiqueRes = [key  for (key, value) in IS_dictionary.items() if value in ciritiqued_Rating_list]\n",
    "\n",
    "        #Updating the critiqued items to list\n",
    "        print('Saving critiqued items at index: ', listCritiqueRes[:5], '...')\n",
    "\n",
    "        #Update the critiqued restaurant list\n",
    "        critique_Res_list.extend(listCritiqueRes)\n",
    "        critique_Res_list = list(set(critique_Res_list))\n",
    "\n",
    "        #Record those as well\n",
    "        critiqued_Res_AccrdStar_list.extend(listCritiqueRes)\n",
    "        critiqued_Res_AccrdStar_list = list(set(critiqued_Res_AccrdStar_list))\n",
    "\n",
    "        #Update current valid set \n",
    "        current_user_item_predict = [item for item in current_user_item_predict if item not in critique_Res_list]\n",
    "        \n",
    "        \n",
    "    #Scenario #7 Critique on Distance, goes up or down, only say positive critiques\n",
    "    #Want closer, further, or specific value \n",
    "    if 'distance' in feature:\n",
    "        #If want further distance restaurants\n",
    "        if 'positive' in positiveOrNegative and 'further' in critiqueValue:\n",
    "            print('Critiquing distance <=', currentDistance)\n",
    "\n",
    "            #New requirements for restaurants to be critiqued\n",
    "            listCritiquedRestaurant = [key for (key, value) in ID_dictionary.items() if value <= currentDistance]\n",
    "        \n",
    "        #Closer distance\n",
    "        else:\n",
    "            if (critiqueValue != 'None') and not 'closer' in critiqueValue:\n",
    "                currentDistance = int(critiqueValue)\n",
    "                \n",
    "            print('Critiquing distance >=', currentDistance)\n",
    "            \n",
    "            #New requirements for restaurants to be critiqued\n",
    "            listCritiquedRestaurant = [key for (key, value) in ID_dictionary.items() if value >= currentDistance]\n",
    "        \n",
    "\n",
    "        #Update critique restaurant list, only keep the ones that are critiqued under other features\n",
    "        critique_Res_list = [item for item in test_user_item_predict if\\\n",
    "                            (item in list(set(listCritiquedRestaurant+critiqued_Res_AccrdPrice_list+\\\n",
    "                                              critiqued_Res_AccrdStar_list+critiqued_Res_AccrdName_list+\\\n",
    "                                                critiqued_Res_AccrdCuisine_list)))] \n",
    "        \n",
    "        #Update list of restaurants critiqued by the restuarnt distance, entire replacement\n",
    "        critiqued_Res_AccrdDistance_list= listCritiquedRestaurant\n",
    "        \n",
    "        #Update initial valid restuarnt list\n",
    "        current_user_item_predict = [item for item in  current_user_item_predict if item not in critique_Res_list]\n",
    "        \n",
    "        #Handle case where critiqued everything, there's no restuarnt that satisfy all the critiques \n",
    "        if(len(current_user_item_predict) == 0):\n",
    "            print('Exhausted sytem')\n",
    "            \n",
    "            new_Distance= input('There\\'s no restaurant that matches your preference within this area, please input a new larger distance')\n",
    "\n",
    "            listCritiquedRestaurant = [key for (key, value) in ID_dictionary.items() if value > new_Distance]\n",
    "            \n",
    "            #Restore original critiqued restaurants under distance feature \n",
    "            critique_Res_list = [item for item in test_user_item_predict if\\\n",
    "                                (item in list(set(listCritiquedRestaurant+critiqued_Res_AccrdPrice_list\\\n",
    "                                                  +critiqued_Res_AccrdStar_list+critiqued_Res_AccrdName_list+\\\n",
    "                                                    critiqued_Res_AccrdCuisine_list)))] \n",
    "            \n",
    "            #New critiquing restuarants\n",
    "            critiqued_Res_AccrdDistance_list= listCritiquedRestaurant\n",
    "        \n",
    "            #Update initial valid restuarnt list\n",
    "            current_user_item_predict = [item for item in  current_user_item_predict if item not in critique_Res_list]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chinese'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positiveCritiquiCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = listCritiquedRestaurant+critiqued_Res_AccrdPrice_list+ critiqued_Res_AccrdStar_list+critiqued_Res_AccrdName_list + critiqued_Res_AccrdCuisine_list\n",
    "a = 4139\n",
    "4139 not in critiqued_Res_AccrdDistance_list or 4139 in list(set(merge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of reviews popularity list, redundent with the output of the next method\n",
    "dff_popular = df.copy()\n",
    "dff_popular = dff_popular.sort_values(by=[\"review_count_y\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "#Get the list of restaurants accoridng to their popularity level\n",
    "popular_list = dff_popular[\"business_num_id\"].tolist()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrainValide = rtrain + rvalid\n",
    "numUsers = rtrainValide.shape[0]\n",
    "# transfer to a matrix(list * number of users)\n",
    "matrix_popular_list_num_of_reviews = np.tile(popular_list,(numUsers,1))\n",
    "matrix_popular_list_num_of_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "popularity_res = evaluate(matrix_popular_list_num_of_reviews, rtest)\n",
    "popularity_res['MAP@10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain = rtrain + rvalid \n",
    "rtrain_implicit = rtrain_implicit + rvalid_implicit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain_implicit_similarity_trainValid = train(rtrain_implicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IK_MATRIX_trainValid = X_cleaned_sparse_trainValid\n",
    "IK_similarity_trainValid = train(IK_MATRIX_trainValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPerformance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testPerformance['Implicit_UserReview'] = KNNPrediction(rtrain_implicit_similarity_trainValid, rtrain_implicit, 140, rtest_implicit, itemBased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testPerformance['Implicit_Explicit_Combined_UserReview'] = KNNPrediction(rtrain_implicit_similarity_trainValid, rtrain, 100, rtest, itemBased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testPerformance['ItemKeyphrase'] = KNNPrediction(IK_similarity_trainValid, rtrain, 110, rtest, itemBased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testPerformance['ItemCategory'] = KNNPrediction(IC_similarity, rtrain, 130, rtest, itemBased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPerformance['Popularity_reviewNumber'] = popularity_res['MAP@10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testPerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPerformance.items()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "listPrediction = testPerformance.items()\n",
    "\n",
    "x1, y1 = zip(*listPrediction) # unpack a list of pairs into two tuples\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "err = []\n",
    "for item in x1:\n",
    "    x.append(item)\n",
    "    \n",
    "for item in y1:\n",
    "    y.append(item[0])\n",
    "    err.append(item[1])\n",
    "    \n",
    "plt.figure(figsize=(13,12))\n",
    "\n",
    "plt.scatter(x,y)\n",
    "\n",
    "plt.errorbar(x,y,yerr=err, linestyle=\"None\", fmt='o')\n",
    "\n",
    "for x2, y2 in zip(x, y): \n",
    "    plt.text(x2, y2, str(y2))\n",
    "      \n",
    "plt.title('Test Performance of algorithms')\n",
    "plt.legend(bbox_to_anchor=(1, 1),\n",
    "           bbox_transform=plt.gcf().transFigure)\n",
    "plt.xlabel('Algorithms')\n",
    "plt.ylabel('MAP@10')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
