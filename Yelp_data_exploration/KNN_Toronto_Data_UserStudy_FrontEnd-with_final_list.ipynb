{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This files combines the basic structures and logic of the implementation of user study, sequence is not correct, the final version will be in another file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Downloading https://files.pythonhosted.org/packages/80/93/d384479da0ead712bdaf697a8399c13a9a89bd856ada5a27d462fb45e47b/geopy-1.20.0-py2.py3-none-any.whl (100kB)\n",
      "Collecting geographiclib<2,>=1.49 (from geopy)\n",
      "  Downloading https://files.pythonhosted.org/packages/8b/62/26ec95a98ba64299163199e95ad1b0e34ad3f4e176e221c40245f211e425/geographiclib-1.50-py3-none-any.whl\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-1.50 geopy-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.sparse import csr_matrix, load_npz, save_npz\n",
    "from tqdm import tqdm\n",
    "import statistics as stats\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import yaml\n",
    "import scipy.sparse as sparse\n",
    "import yaml\n",
    "from tkinter import *\n",
    "import tkinter\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy import distance\n",
    "from geopy import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewJson = \"..\\\\data\\\\Export_CleanedReview.json\"\n",
    "#reviewJsonWithClosedRes = \"..\\\\data\\\\Export_CleanedReviewWithClosedRes.json\"\n",
    "reviewJsonToronto = \"..\\\\data\\\\Export_TorontoData.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Select top frenquent user and top frequenty restaurants that had at least 1 review >= 4 stars (Kickking out users that gave all  reviews <=3 and restaurants that never got start >= 4 stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yelp_df(path = 'data/', filename = 'Export_CleanedReview.json', sampling=False, top_user_num=6100, top_item_num=4000):\n",
    "    \"\"\"\n",
    "    Get the pandas dataframe\n",
    "    Sampling only the top users/items by density \n",
    "    Implicit representation applies\n",
    "    \"\"\"\n",
    "    with open(filename,'r') as f:\n",
    "        data = f.readlines()\n",
    "        data = list(map(json.loads, data))\n",
    "    \n",
    "    data = data[0]\n",
    "    #Get all the data from the dggeata file\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df.rename(columns={'stars': 'review_stars', 'text': 'review_text', 'cool': 'review_cool',\n",
    "                       'funny': 'review_funny', 'useful': 'review_useful'},\n",
    "              inplace=True)\n",
    "\n",
    "    df['business_num_id'] = df.business_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.business_id.nunique()))\n",
    "    df['business_num_id'] = df['business_num_id'].astype('int')\n",
    "\n",
    "    df['user_num_id'] = df.user_id.astype('category').\\\n",
    "    cat.rename_categories(range(0, df.user_id.nunique()))\n",
    "    df['user_num_id'] = df['user_num_id'].astype('int')\n",
    "\n",
    "    df['timestamp'] = df['date'].apply(date_to_timestamp)\n",
    "\n",
    "    if sampling:\n",
    "        df = filter_yelp_df(df, top_user_num=top_user_num, top_item_num=top_item_num)\n",
    "        # Refresh num id\n",
    "        df['business_num_id'] = df.business_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.business_id.nunique()))\n",
    "        df['business_num_id'] = df['business_num_id'].astype('int')\n",
    "        \n",
    "        df['user_num_id'] = df.user_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.user_id.nunique()))\n",
    "        df['user_num_id'] = df['user_num_id'].astype('int')\n",
    "#     drop_list = ['date','review_id','review_funny','review_cool','review_useful']\n",
    "#     df = df.drop(drop_list, axis=1)\n",
    "\n",
    "    df = df.reset_index(drop = True)\n",
    "\n",
    "    return df \n",
    "\n",
    "def filter_yelp_df(df, top_user_num=6100, top_item_num=4000):\n",
    "    #Getting the reviews where starts are above 3\n",
    "    df_implicit = df[df['review_stars']>3]\n",
    "    frequent_user_id = df_implicit['user_num_id'].value_counts().head(top_user_num).index.values\n",
    "    frequent_item_id = df_implicit['business_num_id'].value_counts().head(top_item_num).index.values\n",
    "    return df.loc[(df['user_num_id'].isin(frequent_user_id)) & (df['business_num_id'].isin(frequent_item_id))]\n",
    "\n",
    "def date_to_timestamp(date):\n",
    "    dt = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    return time.mktime(dt.timetuple())\n",
    "\n",
    "def df_to_sparse(df, row_name='userId', col_name='movieId', value_name='rating',\n",
    "                 shape=None):\n",
    "    rows = df[row_name]\n",
    "    cols = df[col_name]\n",
    "    if value_name is not None:\n",
    "        values = df[value_name]\n",
    "    else:\n",
    "        values = [1]*len(rows)\n",
    "\n",
    "    return csr_matrix((values, (rows, cols)), shape=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rating-UI and timestamp-UI matrix from original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_timestamp_matrix(df, sampling=False, top_user_num=6100, top_item_num=4000):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #make the df implicit with top frenquent users and \n",
    "    #no need to sample anymore if df was sampled before \n",
    "    if sampling:\n",
    "        df = filter_yelp_df(df, top_user_num=top_user_num, top_item_num=top_item_num)\n",
    "\n",
    "    rating_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "                                 col_name='business_num_id',\n",
    "                                 value_name='review_stars',\n",
    "                                 shape=None)\n",
    "    \n",
    "    #Have same dimension and data entries with rating_matrix, except that the review stars are - user avg\n",
    "#     ratingWuserAvg_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "#                                  col_name='business_num_id',\n",
    "#                                  value_name='reviewStars_userAvg',\n",
    "#                                  shape=None)\n",
    "    \n",
    "    timestamp_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "                                    col_name='business_num_id',\n",
    "                                    value_name='timestamp',\n",
    "                                    shape=None)\n",
    "    \n",
    "    \n",
    "    IC_matrix = get_I_C(df)\n",
    "#     ratingWuserAvg_matrix\n",
    "    return rating_matrix, timestamp_matrix, IC_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_I_C(df):\n",
    "    lst = df.categories.values.tolist()\n",
    "    cat = []\n",
    "    for i in range(len(lst)):\n",
    "        cat.extend(lst[i].split(', '))\n",
    "    unique_cat = set(cat)\n",
    "    #     set categories id\n",
    "    df_cat = pd.DataFrame(list(unique_cat),columns=[\"Categories\"])\n",
    "    df_cat['cat_id'] = df_cat.Categories.astype('category').cat.rename_categories(range(0, df_cat.Categories.nunique()))\n",
    "    dict_cat = df_cat.set_index('Categories')['cat_id'].to_dict()\n",
    "    \n",
    "    df_I_C = pd.DataFrame(columns=['business_num_id', 'cat_id'])\n",
    "    \n",
    "    for i in range((df['business_num_id'].unique().shape)[0]):\n",
    "        df_temp = df[df['business_num_id'] == i].iloc[:1]\n",
    "        temp_lst = df_temp['categories'].to_list()[0].split(\",\")\n",
    "        for j in range(len(temp_lst)):\n",
    "            df_I_C = df_I_C.append({'business_num_id' : i  , 'cat_id' : dict_cat[temp_lst[j].strip()]} , ignore_index=True)\n",
    "    \n",
    "    IC_Matrix = df_to_sparse(df_I_C, row_name='business_num_id',\n",
    "                                 col_name='cat_id',\n",
    "                                 value_name=None,\n",
    "                                 shape=None)    \n",
    "    return IC_Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time ordered split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_ordered_split(rating_matrix, ratingWuserAvg_matrix, timestamp_matrix, ratio=[0.5, 0.2, 0.3],\n",
    "                       implicit=True, remove_empty=False, threshold=3,\n",
    "                       sampling=False, sampling_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Split the data to train,valid,test by time\n",
    "    ratio:  train:valid:test\n",
    "    threshold: for implicit representation\n",
    "    \"\"\"\n",
    "    if implicit:\n",
    "        temp_rating_matrix = sparse.csr_matrix(rating_matrix.shape)\n",
    "        temp_rating_matrix[(rating_matrix > threshold).nonzero()] = 1\n",
    "        rating_matrix = temp_rating_matrix\n",
    "        timestamp_matrix = timestamp_matrix.multiply(rating_matrix)\n",
    "        #ratingWuserAvg_matrix = ratingWuserAvg_matrix.multiply(rating_matrix)\n",
    "\n",
    "    nonzero_index = None\n",
    "\n",
    "    #Default false, not removing empty columns and rows\n",
    "    #Should not have this case, since users should have at least 1 record of 4,5 \n",
    "    #And restuarant should have at least 1 record of 4,5 \n",
    "    if remove_empty:\n",
    "        # Remove empty columns. record original item index\n",
    "        nonzero_index = np.unique(rating_matrix.nonzero()[1])\n",
    "        rating_matrix = rating_matrix[:, nonzero_index]\n",
    "        timestamp_matrix = timestamp_matrix[:, nonzero_index]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[:, nonzero_index]\n",
    "\n",
    "        # Remove empty rows. record original user index\n",
    "        nonzero_rows = np.unique(rating_matrix.nonzero()[0])\n",
    "        rating_matrix = rating_matrix[nonzero_rows]\n",
    "        timestamp_matrix = timestamp_matrix[nonzero_rows]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[nonzero_rows]\n",
    "\n",
    "    user_num, item_num = rating_matrix.shape\n",
    "\n",
    "    rtrain = []\n",
    "    rtrain_userAvg = []\n",
    "    rtime = []\n",
    "    rvalid = []\n",
    "    rvalid_userAvg = []\n",
    "    rtest = []\n",
    "    rtest_userAvg = []\n",
    "    # Get the index list corresponding to item for train,valid,test\n",
    "    item_idx_train = []\n",
    "    item_idx_valid = []\n",
    "    item_idx_test = []\n",
    "    \n",
    "    for i in tqdm(range(user_num)):\n",
    "        #Get the non_zero indexs, restuarants where the user visited/liked if implicit \n",
    "        item_indexes = rating_matrix[i].nonzero()[1]\n",
    "        \n",
    "        #Get the data for the user\n",
    "        data = rating_matrix[i].data\n",
    "        \n",
    "        #Get time stamp value \n",
    "        timestamp = timestamp_matrix[i].data\n",
    "        \n",
    "        #Get review stars with user avg data \n",
    "        if implicit == False:\n",
    "            dataWuserAvg = ratingWuserAvg_matrix[i].data\n",
    "        \n",
    "        #Non zero reviews for this user\n",
    "        num_nonzeros = len(item_indexes)\n",
    "        \n",
    "        #If the user has at least one review\n",
    "        if num_nonzeros >= 1:\n",
    "            #Get number of test and valid data \n",
    "            #train is 30%\n",
    "            num_test = int(num_nonzeros * ratio[2])\n",
    "            #validate is 50%\n",
    "            num_valid = int(num_nonzeros * (ratio[1] + ratio[2]))\n",
    "\n",
    "            valid_offset = num_nonzeros - num_valid\n",
    "            test_offset = num_nonzeros - num_test\n",
    "\n",
    "            #Sort the timestamp for each review for the user\n",
    "            argsort = np.argsort(timestamp)\n",
    "            \n",
    "            #Sort the reviews for the user according to the time stamp \n",
    "            data = data[argsort]\n",
    "            \n",
    "            #Sort the review with user avg accoridng to the time stamp\n",
    "            if implicit == False:\n",
    "                dataWuserAvg = dataWuserAvg[argsort]\n",
    "            \n",
    "            #Non-zero review index sort according to time\n",
    "            item_indexes = item_indexes[argsort]\n",
    "            \n",
    "            #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "            rtrain.append([data[:valid_offset], np.full(valid_offset, i), item_indexes[:valid_offset]])\n",
    "            \n",
    "            if implicit == False:\n",
    "                #Changing valid set to binary\n",
    "                count=valid_offset\n",
    "                for eachData in data[valid_offset:test_offset]:\n",
    "                    #if rating-avgRating > 0 then like\n",
    "                    if eachData >= 4:\n",
    "                        data[count] = 1\n",
    "                    else:\n",
    "                        data[count] = 0\n",
    "                    count += 1\n",
    "                \n",
    "            #50%-70%\n",
    "            rvalid.append([data[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                           item_indexes[valid_offset:test_offset]])\n",
    "            #remaining 30%\n",
    "            rtest.append([data[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "            \n",
    "            if implicit == False:\n",
    "                #Now for the rating matrix that considers user average rating\n",
    "                #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "                rtrain_userAvg.append([dataWuserAvg[:valid_offset], np.full(valid_offset, i), item_indexes[:valid_offset]])\n",
    "                #50%-70%\n",
    "\n",
    "                #Changing valid set to binary\n",
    "                count=valid_offset\n",
    "                for eachData in dataWuserAvg[valid_offset:test_offset]:\n",
    "                    #if rating-avgRating > 0 then like\n",
    "                    if eachData > 0:\n",
    "                        dataWuserAvg[count] = 1\n",
    "                    else:\n",
    "                        dataWuserAvg[count] = 0\n",
    "                    count += 1\n",
    "\n",
    "                rvalid_userAvg.append([dataWuserAvg[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                               item_indexes[valid_offset:test_offset]])\n",
    "\n",
    "                #Change test set to binary even we don't use it\n",
    "                countTest = test_offset\n",
    "                for eachData in dataWuserAvg[test_offset:]:\n",
    "                    #if rating-avgRating > 0 then like\n",
    "                    if eachData > 0:\n",
    "                        dataWuserAvg[count] = 1\n",
    "                    else:\n",
    "                        dataWuserAvg[count] = 0\n",
    "                    count += 1\n",
    "\n",
    "\n",
    "                #remaining 30%\n",
    "                rtest_userAvg.append([dataWuserAvg[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "                \n",
    "            item_idx_train.append(item_indexes[:valid_offset])\n",
    "            \n",
    "#             item_idx_valid.append(item_indexes[valid_offset:test_offset])\n",
    "#             item_idx_test.append(item_indexes[test_offset:])\n",
    "        else:\n",
    "            item_idx_train.append([])\n",
    "#             item_idx_valid.append([])\n",
    "#             item_idx_test.append([])\n",
    "    \n",
    "    rtrain = np.array(rtrain)\n",
    "    rvalid = np.array(rvalid)\n",
    "    rtest = np.array(rtest)\n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = np.array(rtrain_userAvg)\n",
    "        rvalid_userAvg = np.array(rvalid_userAvg)\n",
    "        rtest_userAvg = np.array(rtest_userAvg)\n",
    "\n",
    "    #take non-zeros values, row index, and column (non-zero) index and store into sparse matrix\n",
    "    rtrain = sparse.csr_matrix((np.hstack(rtrain[:, 0]), (np.hstack(rtrain[:, 1]), np.hstack(rtrain[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rvalid = sparse.csr_matrix((np.hstack(rvalid[:, 0]), (np.hstack(rvalid[:, 1]), np.hstack(rvalid[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rtest = sparse.csr_matrix((np.hstack(rtest[:, 0]), (np.hstack(rtest[:, 1]), np.hstack(rtest[:, 2]))),\n",
    "                              shape=rating_matrix.shape, dtype=np.float32)\n",
    "    \n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = sparse.csr_matrix((np.hstack(rtrain_userAvg[:, 0]), (np.hstack(rtrain_userAvg[:, 1]), np.hstack(rtrain_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rvalid_userAvg = sparse.csr_matrix((np.hstack(rvalid_userAvg[:, 0]), (np.hstack(rvalid_userAvg[:, 1]), np.hstack(rvalid_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rtest_userAvg = sparse.csr_matrix((np.hstack(rtest_userAvg[:, 0]), (np.hstack(rtest_userAvg[:, 1]), np.hstack(rtest_userAvg[:, 2]))),\n",
    "                                  shape=rating_matrix.shape, dtype=np.float32)\n",
    "\n",
    "    return rtrain, rvalid, rtest,rtrain_userAvg, rvalid_userAvg, rtest_userAvg, nonzero_index, timestamp_matrix, item_idx_train, item_idx_valid, item_idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_ordered_splitModified(rating_matrix, ratingWuserAvg_matrix, timestamp_matrix, ratio=[0.5, 0.2, 0.3],\n",
    "                       implicit=True, remove_empty=False, threshold=3,\n",
    "                       sampling=False, sampling_ratio=0.1, trainSampling=1):\n",
    "    \"\"\"\n",
    "    Split the data to train,valid,test by time\n",
    "    ratio:  train:valid:test\n",
    "    threshold: for implicit representation\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if implicit:\n",
    "        temp_rating_matrix = sparse.csr_matrix(rating_matrix.shape)\n",
    "        temp_rating_matrix[(rating_matrix > threshold).nonzero()] = 1\n",
    "        rating_matrix = temp_rating_matrix\n",
    "        timestamp_matrix = timestamp_matrix.multiply(rating_matrix)\n",
    "        #ratingWuserAvg_matrix = ratingWuserAvg_matrix.multiply(rating_matrix)\n",
    "\n",
    "    nonzero_index = None\n",
    "\n",
    "    #Default false, not removing empty columns and rows\n",
    "    #Should not have this case, since users should have at least 1 record of 4,5 \n",
    "    #And restuarant should have at least 1 record of 4,5 \n",
    "    if remove_empty:\n",
    "        # Remove empty columns. record original item index\n",
    "        nonzero_index = np.unique(rating_matrix.nonzero()[1])\n",
    "        rating_matrix = rating_matrix[:, nonzero_index]\n",
    "        timestamp_matrix = timestamp_matrix[:, nonzero_index]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[:, nonzero_index]\n",
    "\n",
    "        # Remove empty rows. record original user index\n",
    "        nonzero_rows = np.unique(rating_matrix.nonzero()[0])\n",
    "        rating_matrix = rating_matrix[nonzero_rows]\n",
    "        timestamp_matrix = timestamp_matrix[nonzero_rows]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[nonzero_rows]\n",
    "\n",
    "    user_num, item_num = rating_matrix.shape\n",
    "\n",
    "    rtrain = []\n",
    "    rtrain_userAvg = []\n",
    "    rtime = []\n",
    "    rvalid = []\n",
    "    rvalid_userAvg = []\n",
    "    rtest = []\n",
    "    rtest_userAvg = []\n",
    "    # Get the index list corresponding to item for train,valid,test\n",
    "    item_idx_train = []\n",
    "    item_idx_valid = []\n",
    "    item_idx_test = []\n",
    "    \n",
    "    for i in tqdm(range(user_num)):\n",
    "        #Get the non_zero indexs, restuarants where the user visited/liked if implicit \n",
    "        item_indexes = rating_matrix[i].nonzero()[1]        \n",
    "        #Get the data for the user\n",
    "        data = rating_matrix[i].data      \n",
    "        #Get time stamp value \n",
    "        timestamp = timestamp_matrix[i].data \n",
    "        #Get review stars with user avg data \n",
    "        if implicit == False:\n",
    "            dataWuserAvg = ratingWuserAvg_matrix[i].data\n",
    "\n",
    "            \n",
    "        #Non zero reviews for this user\n",
    "        num_nonzeros = len(item_indexes)\n",
    "        \n",
    "        #If the user has at least one review\n",
    "        if num_nonzeros >= 1:\n",
    "            num_test = int(num_nonzeros * ratio[2])\n",
    "            num_valid = int(num_nonzeros * (ratio[1] + ratio[2]))\n",
    "            valid_offset = num_nonzeros - num_valid\n",
    "            \n",
    "            # Adding this for sampling for training set\n",
    "            valid_offsetSample = int(valid_offset*trainSampling)\n",
    "            test_offset = num_nonzeros - num_test\n",
    "            \n",
    "            #Sort the timestamp for each review for the user\n",
    "            argsort = np.argsort(timestamp)\n",
    "            \n",
    "            #Sort the reviews for the user according to the time stamp \n",
    "            data = data[argsort]\n",
    "            \n",
    "            #Sort the review with user avg accoridng to the time stamp\n",
    "            if implicit == False:\n",
    "                dataWuserAvg = dataWuserAvg[argsort]\n",
    "            \n",
    "            #Non-zero review index sort according to time\n",
    "            item_indexes = item_indexes[argsort]\n",
    "            \n",
    "            #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "            #if take from old to new\n",
    "            #rtrain.append([data[:valid_offsetSample], np.full(valid_offsetSample, i), item_indexes[:valid_offsetSample]])\n",
    "            #if take from new to old\n",
    "            rtrain.append([data[valid_offset-valid_offsetSample:valid_offset], np.full(valid_offsetSample, i), item_indexes[valid_offset-valid_offsetSample:valid_offset]])\n",
    "            rvalid.append([data[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                           item_indexes[valid_offset:test_offset]])\n",
    "            rtest.append([data[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "            \n",
    "            if implicit == False:\n",
    "                #Now for the rating matrix that considers user average rating\n",
    "                #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "                #from old to new\n",
    "                #rtrain_userAvg.append([dataWuserAvg[:valid_offsetSample], np.full(valid_offsetSample, i), item_indexes[:valid_offsetSample]])\n",
    "                #take nearest\n",
    "                rtrain_userAvg.append([dataWuserAvg[valid_offset-valid_offsetSample:valid_offset], np.full(valid_offsetSample, i), item_indexes[valid_offset-valid_offsetSample:valid_offset]])                \n",
    "                    \n",
    "                rvalid_userAvg.append([dataWuserAvg[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                               item_indexes[valid_offset:test_offset]])\n",
    "                \n",
    "                rtest_userAvg.append([dataWuserAvg[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "                \n",
    "            item_idx_train.append(item_indexes[:valid_offsetSample])\n",
    "            \n",
    "        else:\n",
    "            item_idx_train.append([])\n",
    "    \n",
    "    rtrain = np.array(rtrain)\n",
    "    rvalid = np.array(rvalid)\n",
    "    rtest = np.array(rtest)\n",
    "   \n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = np.array(rtrain_userAvg)\n",
    "        rvalid_userAvg = np.array(rvalid_userAvg)\n",
    "        rtest_userAvg = np.array(rtest_userAvg)\n",
    "\n",
    "    #take non-zeros values, row index, and column (non-zero) index and store into sparse matrix\n",
    "    rtrain = sparse.csr_matrix((np.hstack(rtrain[:, 0]), (np.hstack(rtrain[:, 1]), np.hstack(rtrain[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rvalid = sparse.csr_matrix((np.hstack(rvalid[:, 0]), (np.hstack(rvalid[:, 1]), np.hstack(rvalid[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rtest = sparse.csr_matrix((np.hstack(rtest[:, 0]), (np.hstack(rtest[:, 1]), np.hstack(rtest[:, 2]))),\n",
    "                              shape=rating_matrix.shape, dtype=np.float32)\n",
    "    \n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = sparse.csr_matrix((np.hstack(rtrain_userAvg[:, 0]), (np.hstack(rtrain_userAvg[:, 1]), np.hstack(rtrain_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rvalid_userAvg = sparse.csr_matrix((np.hstack(rvalid_userAvg[:, 0]), (np.hstack(rvalid_userAvg[:, 1]), np.hstack(rvalid_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rtest_userAvg = sparse.csr_matrix((np.hstack(rtest_userAvg[:, 0]), (np.hstack(rtest_userAvg[:, 1]), np.hstack(rtest_userAvg[:, 2]))),\n",
    "                                  shape=rating_matrix.shape, dtype=np.float32)\n",
    "\n",
    "    return rtrain, rvalid, rtest,rtrain_userAvg, rvalid_userAvg, rtest_userAvg, nonzero_index, timestamp_matrix, item_idx_train, item_idx_valid, item_idx_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get df for training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Item idex matrix stores the reivews starts\n",
    "#This function returns a list of index for the reviews included in training set \n",
    "def get_corpus_idx_list(df, item_idx_matrix):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    df: total dataframe\n",
    "    item_idx_matrix: train index list got from time_split \n",
    "    Output: row index in original dataframe for training data by time split\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    #For all the users: 5791\n",
    "    for i in tqdm(range(len(item_idx_matrix))):\n",
    "        \n",
    "        #find row index where user_num_id is i\n",
    "        a = df.index[df['user_num_id'] == i].tolist()\n",
    "        \n",
    "        #loop through the busienss id that the user i reviewed for in offvalid set \n",
    "        for item_idx in  item_idx_matrix[i]:\n",
    "            \n",
    "            #get the row index for reviews for business that the user liked in the train set\n",
    "            b = df.index[df['business_num_id'] == item_idx].tolist()\n",
    "            \n",
    "            #Find the index for which this user liked, one user only rate a business once\n",
    "            idx_to_add = list(set(a).intersection(b))\n",
    "            \n",
    "            if idx_to_add not in lst:\n",
    "                lst.extend(idx_to_add)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess using Term Frequency - CounterVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shenti10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shenti10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Stemming and Lemmatisation\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Get corpus and CountVector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "new_words = ['not_the']\n",
    "stop_words = stop_words.union(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#Should 'because' added?\n",
    "def preprocess(df, reset_list = [',','.','?',';','however','but']):\n",
    "    corpus = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        text = df['review_text'][i]\n",
    "        change_flg = 0\n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        ##Convert to list from string, loop through the review text\n",
    "        text = text.split()\n",
    "        \n",
    "        #any sentence that encounters a not, the folloing words will become not phrase until hit the sentence end\n",
    "        for j in range(len(text)):\n",
    "            #Make the not_ hack\n",
    "            if text[j] == 'not':\n",
    "                change_flg = 1\n",
    "#                 print 'changes is made after ', i\n",
    "                continue\n",
    "            #if was 1 was round and not hit a 'not' in this round\n",
    "            if change_flg == 1 and any(reset in text[j] for reset in reset_list):\n",
    "                text[j] = 'not_' + text[j]\n",
    "                change_flg = 0\n",
    "#                 print 'reset at ', i\n",
    "            if change_flg == 1:\n",
    "                text[j] = 'not_' + text[j]\n",
    "        \n",
    "        #Convert back to string\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "        #Remove punctuations\n",
    "#       text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        \n",
    "        #remove tags\n",
    "        text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "        \n",
    "        # remove special characters and digits\n",
    "        text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "        \n",
    "        ##Convert to list from string\n",
    "        text = text.split()\n",
    "        \n",
    "        ##Stemming\n",
    "        ps=PorterStemmer()\n",
    "        \n",
    "        #Lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        text = [lem.lemmatize(word) for word in text if not word in  \n",
    "                stop_words] \n",
    "        text = \" \".join(text)\n",
    "        corpus.append(text)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def train(matrix_train):\n",
    "    similarity = cosine_similarity(X=matrix_train, Y=None, dense_output=True)\n",
    "    return similarity\n",
    "\n",
    "def get_I_K(df, X, row_name = 'business_num_id', binary = True, shape = (121994,6000)):\n",
    "    \"\"\"\n",
    "    get the item-keyphrase matrix\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    cols = []\n",
    "    vals = []\n",
    "    #For each review history\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        #Get the array of frequencies for document/review i \n",
    "        arr = X[i].toarray() \n",
    "        nonzero_element = arr.nonzero()[1]  # Get nonzero element in each line, keyphrase that appears index \n",
    "        length_of_nonzero = len(nonzero_element) #number of important keyphrase that appears\n",
    "        \n",
    "        # df[row_name][i] is the item idex\n",
    "        #Get a list row index that indicates the document/review\n",
    "        rows.extend(np.array([df[row_name][i]]*length_of_nonzero)) ## Item index\n",
    "        #print(rows)\n",
    "        \n",
    "        #Get a list of column index indicating the key phrase that appears in i document/review\n",
    "        cols.extend(nonzero_element) ## Keyword Index\n",
    "        if binary:\n",
    "            #Create a bunch of 1s\n",
    "            vals.extend(np.array([1]*length_of_nonzero))\n",
    "        else:\n",
    "            #If not binary \n",
    "            vals.extend(arr[arr.nonzero()])    \n",
    "    return csr_matrix((vals, (rows, cols)), shape=shape)\n",
    "\n",
    "\n",
    "#Get a UI matrix if it's not item_similarity based or else IU\n",
    "def predict(matrix_train, k, similarity, item_similarity_en = False):\n",
    "    prediction_scores = []\n",
    "    \n",
    "    #inverse to IU matrix\n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    #for each user or item, depends UI or IU \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "        # Get user u's prediction scores for all items\n",
    "        #Get prediction/similarity score for each user 1*num or user or num of items\n",
    "        vector_u = similarity[user_index]\n",
    "\n",
    "        # Get closest K neighbors excluding user u self\n",
    "        #Decending accoding to similarity score, select top k\n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        \n",
    "        # Get neighbors similarity weights and ratings\n",
    "        similar_users_weights = similarity[user_index][similar_users]\n",
    "        \n",
    "        #similar_users_weights_sum = np.sum(similar_users_weights)\n",
    "        #print(similar_users_weights.shape)\n",
    "        #shape: num of res * k\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "              \n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "        #print(prediction_scores_u)\n",
    "        \n",
    "        \n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "        \n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    return res\n",
    "\n",
    "\n",
    "#Preidction score is UI or IU?\n",
    "def prediction(prediction_score, topK, matrix_Train):\n",
    "\n",
    "    prediction = []\n",
    "\n",
    "    #for each user\n",
    "    for user_index in tqdm(range(matrix_Train.shape[0])):\n",
    "        \n",
    "        #take the prediction scores for user 1 * num res\n",
    "        vector_u = prediction_score[user_index]\n",
    "        \n",
    "        #The restuarant the user rated\n",
    "        vector_train = matrix_Train[user_index]\n",
    "        \n",
    "        if len(vector_train.nonzero()[0]) > 0:\n",
    "            vector_predict = sub_routine(vector_u, vector_train, topK=topK)\n",
    "        else:\n",
    "            vector_predict = np.zeros(topK, dtype=np.float32)\n",
    "\n",
    "        prediction.append(vector_predict)\n",
    "\n",
    "    return np.vstack(prediction)\n",
    "\n",
    "#topK: the number of restuarants we are suggesting \n",
    "#if vector_train has number, then the user has visited\n",
    "def sub_routine(vector_u, vector_train, topK=500):\n",
    "\n",
    "    #index where non-zero\n",
    "    train_index = vector_train.nonzero()[1]\n",
    "    \n",
    "    vector_u = vector_u\n",
    "    \n",
    "    #get topk + num rated res prediction score descending, top index \n",
    "    candidate_index = np.argpartition(-vector_u, topK+len(train_index))[:topK+len(train_index)]\n",
    "    \n",
    "    #sort top prediction score index in range topK+len(train_index) into vector_u`\n",
    "    vector_u = candidate_index[vector_u[candidate_index].argsort()[::-1]]\n",
    "    \n",
    "    #deleted the rated res from the topk+train_index prediction score vector for user u \n",
    "    #Delete the user rated res index from the topk+numRated index\n",
    "    vector_u = np.delete(vector_u, np.isin(vector_u, train_index).nonzero()[0])\n",
    "\n",
    "    #so we only include the top K prediction score here\n",
    "    return vector_u[:topK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImplicitMatrix(sparseMatrix, threashold=0):\n",
    "    temp_matrix = sparse.csr_matrix(sparseMatrix.shape)\n",
    "    temp_matrix[(sparseMatrix > threashold).nonzero()] = 1\n",
    "    return temp_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictUU(matrix_train, k, chooseWeigthMethod, similarity1=None, similarity2=None, similarity3=None, similarity4=None, similarity5=None, item_similarity_en = False):\n",
    "    prediction_scores = []\n",
    "    #Convert from list to ndarray, add an axis\n",
    "    if isinstance(chooseWeigthMethod, list):\n",
    "        chooseWeigthMethod = np.array(chooseWeigthMethod)[:, np.newaxis]\n",
    "   \n",
    "    \"make sure that when passing in chooseWeightMethod, the weight must be aligned with similarity metrices, even if set to None\"\n",
    "    \"They should add to 1 as well\"\n",
    "    #inverse to IU matrix\n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    #for each user or item, depends UI or IU \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "    #for user_index in tqdm(range(10,20)):\n",
    "        \n",
    "        numberSimilarMatrix = 0\n",
    "        # Get user u's prediction scores for all items \n",
    "        #Get prediction/similarity score for each user 1*num or user or num of items\n",
    "        if similarity1 is not None:\n",
    "            vector_u1 = similarity1[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u1 = [0]*matrix_train.shape[0]\n",
    "            \n",
    "        if similarity2 is not None:\n",
    "            vector_u2 = similarity2[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u2 = [0]*len(vector_u1)\n",
    "            \n",
    "        if similarity3 is not None:\n",
    "            vector_u3 = similarity3[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u3 = [0]*len(vector_u1)\n",
    "            \n",
    "        if similarity4 is not None:\n",
    "            vector_u4 = similarity4[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u4 = [0]*len(vector_u1)\n",
    "        \n",
    "        if similarity5 is not None:\n",
    "            vector_u5 = similarity5[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u5 = [0]*len(vector_u1)\n",
    "        \n",
    "        #Temperary vector that stacks all 4 vectors together\n",
    "        tempVector = np.array([vector_u1,vector_u2,vector_u3,vector_u4, vector_u5])\n",
    "        \n",
    "        if chooseWeigthMethod is None:\n",
    "            #Get the similarity score from the first similarity matrix anyways \n",
    "            vector_u = vector_u1.copy()\n",
    "            \n",
    "        #If we are choosing the max, min, avg or similarity scores\n",
    "        if chooseWeigthMethod is not None:\n",
    "            if chooseWeigthMethod == 'max':\n",
    "                vector_u = tempVector.max(axis=0)\n",
    "            elif chooseWeigthMethod == 'min':\n",
    "                vector_u = tempVector.min(axis=0)\n",
    "            elif chooseWeigthMethod == 'average':\n",
    "                vector_u = tempVector.mean(axis=0)\n",
    "            elif isinstance(chooseWeigthMethod, np.ndarray):\n",
    "                #Validate that number of weights passed in equals number of matrices\n",
    "                #assert(len(chooseWeigthMethod) == numberSimilarMatrix)\n",
    "                #Get the new combined similarity vector \n",
    "                weighted_u = tempVector * chooseWeigthMethod\n",
    "                vector_u =np.sum(weighted_u, axis=0)\n",
    "                #print((vector_u == vector_u4).sum())\n",
    "                \n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        \n",
    "        # Get neighbors similarity weights and ratings\n",
    "        #similar_users_weights = similarity1[user_index][similar_users]\n",
    "        similar_users_weights = vector_u[similar_users]\n",
    "        \n",
    "        #similar_users_weights_sum = np.sum(similar_users_weights)\n",
    "        #print(similar_users_weights.shape)\n",
    "        #shape: num of res * k\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "              \n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "        #print(prediction_scores_u)\n",
    "        \n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "        \n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for UI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the user number that we are trying to recommend for\n",
    "#Enter the # of businesses that we want to recommend for them \n",
    "#Pass in the UI_Prediction matrix for users \n",
    "#userIndex parameter not used \n",
    "def constructResDictionary(userIndex, busIndexRange, UI_prediction):\n",
    "    #Construct the dictionary for the recommended restaurants to display \n",
    "    dictionaryToConstruct = {}\n",
    "    \n",
    "    #Loop through the number of businesses \n",
    "    for busIndex in range(busIndexRange):\n",
    "        #Get the business information for the recommended business \n",
    "        businessSeries = df[df[\"business_num_id\"] == UI_prediction[busIndex]].iloc[0]\n",
    "        #Get the business name \n",
    "        busName = businessSeries['name']\n",
    "        \n",
    "        #get the list of strings to generate the address information \n",
    "        address_generator = (str(w) for w in yaml.safe_load(businessSeries.location)['display_address'])\n",
    "        busLocation = ', '.join(address_generator)\n",
    "        bus_Price = businessSeries.price\n",
    "        busStars = businessSeries.business_stars\n",
    "        busReviewCount = businessSeries.review_count_y \n",
    "        #category_generator = (str(s) for s in [cateDict['title'] for cateDict in yaml.safe_load(businessSeries.categories)])\n",
    "        #busCategories = [cateDict['title'] for cateDict in yaml.safe_load(businessSeries.categories)]\n",
    "        #busCategories = ', '.join(category_generator)\n",
    "        busCategories = businessSeries.categories\n",
    "        #Now add the restaurant to the dictionary\n",
    "        dictionaryToConstruct[busName] = {'Address': busLocation,\\\n",
    "                                'Price': bus_Price,\\\n",
    "                                 'Star': busStars, \\\n",
    "                                 'Review Count': busReviewCount, \\\n",
    "                                 'Category': busCategories}\n",
    "    return dictionaryToConstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial method\n",
    "# def get_three_popularity_matrix(df_original,rtrain):\n",
    "#     # get the list of popular items by ranking the number of reviews\n",
    "#     dff_popular = df_original.copy()\n",
    "#     dff_popular = dff_popular.sort_values(by=[\"review_count_y\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "#     popular_list_num_of_reviews = dff_popular[\"business_num_id\"].tolist()\n",
    "    \n",
    "#     # get the list of popular items by ranking average rating score\n",
    "#     dff_popular_rating = df_original.copy()\n",
    "#     dff_popular_rating = dff_popular.sort_values(by=[\"business_stars\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "#     popular_list_avg_stars = dff_popular_rating[\"business_num_id\"].tolist()\n",
    "    \n",
    "#     # get the popularity items by using the percentage liked method(number of liked items / total items)\n",
    "#     numUsers = rtrain.shape[0]\n",
    "#     numItems = rtrain.shape[1]\n",
    "    \n",
    "#     predictionMatrix = np.zeros((numUsers , numItems))\n",
    "\n",
    "#     # Define function for converting 1-5 rating to 0/1 (like / don't like)\n",
    "#     vf = np.vectorize(lambda x: 1 if x >= 4 else 0)\n",
    "#     rtrain_array = rtrain.toarray()\n",
    "#     # For every item calculate the number of people liked (4-5) divided by the number of people that rated\n",
    "#     itemPopularity = np.zeros((numItems))\n",
    "#     for item in range(numItems):\n",
    "#         numOfUsersRated = len(rtrain_array[:, item].nonzero()[0])\n",
    "#         numOfUsersLiked = len(vf(rtrain_array[:, item]).nonzero()[0])\n",
    "# #         if numOfUsersRated == 0:\n",
    "#         # set a threshold to filter out restaurants with very few reviews\n",
    "#         if numOfUsersRated <= 100:\n",
    "#             itemPopularity[item] = 0\n",
    "#         else:\n",
    "#             itemPopularity[item] = numOfUsersLiked/numOfUsersRated\n",
    "#     popular_list_liked_ratio = itemPopularity.argsort()\n",
    "    \n",
    "#     return np.asarray(popular_list_num_of_reviews),np.asarray(popular_list_avg_stars),popular_list_liked_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_three_popularity_matrix(df_original,rtrain):\n",
    "    # get the list of popular items by ranking the number of reviews\n",
    "    numUsers = rtrain.shape[0]\n",
    "    numItems = rtrain.shape[1]\n",
    "    \n",
    "    dff_popular = df_original.copy()\n",
    "    dff_popular = dff_popular.sort_values(by=[\"review_count_y\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "    popular_list_num_of_reviews = dff_popular[\"business_num_id\"].tolist()\n",
    "    \n",
    "    # get the list of popular items by ranking average rating score\n",
    "    dff_popular_rating = df_original.copy()\n",
    "    dff_popular_rating = dff_popular_rating.sort_values(by=[\"business_stars\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "    popular_list_avg_stars = dff_popular_rating[\"business_num_id\"].tolist()\n",
    "    \n",
    "    lst_temp = []\n",
    "    for item in tqdm(range(numItems)):\n",
    "        numOfUsersRated = len(rtrain.toarray()[:, item].nonzero()[0])\n",
    "        if numOfUsersRated <= 50:\n",
    "            lst_temp.append(item)\n",
    "    popular_list_avg_stars = [x for x in popular_list_avg_stars if x not in lst_temp]\n",
    "    \n",
    "    # get the popularity items by using the percentage liked method(number of liked items / total items)\n",
    "    predictionMatrix = np.zeros((numUsers , numItems))\n",
    "\n",
    "    # Define function for converting 1-5 rating to 0/1 (like / don't like)\n",
    "    vf = np.vectorize(lambda x: 1 if x >= 4 else 0)\n",
    "    rtrain_array = rtrain.toarray()\n",
    "    # For every item calculate the number of people liked (4-5) divided by the number of people that rated\n",
    "    itemPopularity = np.zeros((numItems))\n",
    "    for item in range(numItems):\n",
    "        numOfUsersRated = len(rtrain_array[:, item].nonzero()[0])\n",
    "        numOfUsersLiked = len(vf(rtrain_array[:, item]).nonzero()[0])\n",
    "#         if numOfUsersRated == 0:\n",
    "        # set a threshold to filter out restaurants with very few reviews\n",
    "        if numOfUsersRated <= 30:\n",
    "            itemPopularity[item] = 0\n",
    "        else:\n",
    "            itemPopularity[item] = numOfUsersLiked/numOfUsersRated\n",
    "    popular_list_liked_ratio = itemPopularity.argsort()\n",
    "    \n",
    "    return np.asarray(popular_list_num_of_reviews),np.asarray(popular_list_avg_stars),popular_list_liked_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either a list or an array\n",
    "def geographical_dist(prediction_matrix,intersection,user_id, bus_indexRange):\n",
    "    lst = []\n",
    "    length = 0\n",
    "    #if the prediction matrix is a list of items for an user \n",
    "    print(type(prediction_matrix))\n",
    "    if isinstance(prediction_matrix, list):\n",
    "        print('isList')\n",
    "        length = len(prediction_matrix)\n",
    "        print(length)\n",
    "    #loop through the prediction matrix for the user, if passed in a prediction matrix \n",
    "    else:\n",
    "        length = prediction_matrix[user_id].shape[0]\n",
    "        #length = prediction_matrix.shape[0]\n",
    "    for j in range(length):\n",
    "        if isinstance(prediction_matrix, list):\n",
    "            coordinateDict = yaml.safe_load(df_location[df_location[\"business_num_id\"] == prediction_matrix[j]].iloc[0].coordinates)\n",
    "        else:    \n",
    "            coordinateDict = yaml.safe_load(df_location[df_location[\"business_num_id\"] == prediction_matrix[user_id][j]].iloc[0].coordinates)\n",
    "        test_point = Point(coordinateDict['latitude'],coordinateDict['longitude'])\n",
    "        \n",
    "        #Get the distance with the test point\n",
    "        result = distance.distance(intersection,test_point).kilometers\n",
    "        if result<=0.6:\n",
    "            #append the jth item if the condition matches\n",
    "            if isinstance(prediction_matrix, list):\n",
    "                lst.append(prediction_matrix[j])\n",
    "            else:\n",
    "                lst.append(prediction_matrix[user_id][j])\n",
    "        lst = lst[0:bus_indexRange]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiListbox_Initialize(Frame):\n",
    "    \n",
    "    def __init__(self, master, lists,list1Res,list2Res,list3Res):\n",
    "        Frame.__init__(self, master)\n",
    "        self.lists = []\n",
    "        #self.entries = []\n",
    "        #self.fields = 'Like', 'Dislike'\n",
    "        self.dropDownTextBox = 'Rank1','Score1', 'Rank2','Score2','Rank3','Score3'\n",
    "        #Var for checkbox\n",
    "        #self.var = IntVar()\n",
    "        self.var = []\n",
    "        self.listNames = []\n",
    "        self.responseDict = {}\n",
    "        #self.options = [\"Jan\", \"Feb\", \"Mar\"] #etc\n",
    "        #No 0s or else you won't choose it as liked \n",
    "        self.scores = [5, 4, 3, 2, 1]\n",
    "        #stores the list of restaurants for list 1-3\n",
    "        self.list1Name = list1Res\n",
    "        self.list2Name = list2Res\n",
    "        self.list3Name = list3Res\n",
    "        #To get response from dropdowns \n",
    "        self.rankVar = []\n",
    "        self.scoreVar = []\n",
    "        \n",
    "        #list count 0,1,2\n",
    "        listCount = 0 \n",
    "        #Loop through the lists, l is the list label and widthW is the width \n",
    "        for l,widthW in lists:\n",
    "            \n",
    "            frame = Frame(self); \n",
    "            frame.pack(side=LEFT, expand=YES, fill=BOTH)\n",
    "            \n",
    "            Label(frame, text=l, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "            \n",
    "            #store the list names \n",
    "            self.listNames.append(l)\n",
    "            \n",
    "            lb = Listbox(frame, width=widthW, borderwidth=0, selectborderwidth=0,\n",
    "                 relief=FLAT, exportselection=FALSE)\n",
    "            lb.pack(expand=YES, fill=BOTH)\n",
    "            #lb.pack(fill=X,expand=1)\n",
    "            #lb.pack\n",
    "            self.lists.append(lb)\n",
    "            \n",
    "            #lb.bind('<B1-Motion>', lambda e, s=self: s._select(e.y))\n",
    "            #lb.bind('<Button-1>', lambda e, s=self: s._select(e.y))\n",
    "            \n",
    "            #lb.bind('<Leave>', lambda e: 'break')\n",
    "            \n",
    "            #lb.bind('<B2-Motion>', lambda e, s=self: s._b2motion(e.x, e.y))\n",
    "            #lb.bind('<Button-2>', lambda e, s=self: s._button2(e.x, e.y))\n",
    "             \n",
    "            for fieldText in self.dropDownTextBox:\n",
    "\n",
    "                if('Rank' in fieldText):\n",
    "                    lab = Label(frame, text=fieldText, borderwidth=0, relief=RAISED)\n",
    "                    lab.pack(side=TOP,fill=BOTH)\n",
    "            \n",
    "                    #Get the current list number \n",
    "                    if listCount+1 == 1:\n",
    "                        currentOptions = self.list1Name\n",
    "                    elif listCount+1 == 2:\n",
    "                        currentOptions = self.list2Name\n",
    "                    elif listCount+1 == 3:\n",
    "                        currentOptions = self.list3Name\n",
    "                        \n",
    "                    variable = StringVar(master)\n",
    "                    variable.set(currentOptions[0]) # default value\n",
    "\n",
    "                    #row = Frame(self)\n",
    "                    #lab = Label(frame, width=15, text=fieldText, anchor='center')\n",
    "                    drowDwn = OptionMenu(frame, variable, *currentOptions)\n",
    "                    \n",
    "                    #Append the variable set from dropdown \n",
    "                    self.rankVar.append(variable)\n",
    "                    \n",
    "                    #lab.pack(side=LEFT,fill=X)\n",
    " \n",
    "                    drowDwn.pack(side=TOP)\n",
    "\n",
    "\n",
    "                elif('Score' in fieldText ):\n",
    "                    variable = StringVar(master)\n",
    "                    variable.set(self.scores[0]) # default value\n",
    "\n",
    "                    #row = Frame(self)\n",
    "                    lab = Label(frame, width=15, text=fieldText, anchor='center')\n",
    "                    drowDwn = OptionMenu(frame, variable, *self.scores)\n",
    "                    \n",
    "                    #Append te varaible set from dropdown \n",
    "                    self.scoreVar.append(variable)\n",
    "\n",
    "                    #lab.pack(side=RIGHT, fill=X)\n",
    "                    drowDwn.pack(side=TOP)\n",
    "            \n",
    "            listCount += 1\n",
    "            \n",
    "        #Set the submit button  \n",
    "        b1 = Button(master, text='Submit', command=self.fetch)\n",
    "        b1.pack(side=BOTTOM, padx=5, pady=5)\n",
    "\n",
    "        #pakcing the frame\n",
    "        #frame = Frame(self); frame.pack(side=LEFT, fill=Y)\n",
    "        \n",
    "        #packing the label\n",
    "        #Label(frame, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "\n",
    "    def _select(self, y):\n",
    "        row = self.lists[0].nearest(y)\n",
    "        self.selection_clear(0, END)\n",
    "        self.selection_set(row)\n",
    "        return 'break'\n",
    "\n",
    "    def _button2(self, x, y):\n",
    "        for l in self.lists: l.scan_mark(x, y)\n",
    "        return 'break'\n",
    "\n",
    "    def _b2motion(self, x, y):\n",
    "        for l in self.lists: l.scan_dragto(x, y)\n",
    "        return 'break'\n",
    "\n",
    "    def _scroll(self, *args):\n",
    "        for l in self.lists:\n",
    "            apply(l.yview, args)\n",
    "\n",
    "    def curselection(self):\n",
    "        return self.lists[0].curselection()\n",
    "\n",
    "    def delete(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.delete(first, last)\n",
    "\n",
    "#     def get(self, first, last=None):\n",
    "#         result = []\n",
    "#         for l in self.lists:\n",
    "#             result.append(l.get(first,last))\n",
    "#         if last: return apply(map, [None] + result)\n",
    "#         return result\n",
    "\n",
    "    def index(self, index):\n",
    "        self.lists[0].index(index)\n",
    "\n",
    "    def insert(self, index, *elements):\n",
    "        #Loop through the elements \n",
    "        for element in elements:\n",
    "            i = 0\n",
    "            for l in self.lists:\n",
    "                l.insert(index, element[i])\n",
    "                i = i + 1\n",
    "\n",
    "    def size(self):\n",
    "        return self.lists[0].size()\n",
    "\n",
    "    def see(self, index):\n",
    "        for l in self.lists:\n",
    "            l.see(index)\n",
    "\n",
    "    def selection_anchor(self, index):\n",
    "        for l in self.lists:\n",
    "            l.selection_anchor(index)\n",
    "\n",
    "    def selection_clear(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.selection_clear(first, last)\n",
    "\n",
    "    def selection_includes(self, index):\n",
    "        return self.lists[0].selection_includes(index)\n",
    "\n",
    "    def selection_set(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.selection_set(first, last)\n",
    "            \n",
    "    def fetch(self):\n",
    "        localDict = {}\n",
    "        localRankList = list(map((lambda var: var.get()), self.rankVar))\n",
    "        localScoreList = list(map((lambda var: var.get()), self.scoreVar))\n",
    "        \n",
    "        #lop through each selected restaurant and score, construct the dictionary \n",
    "        for count in range(len(localRankList)):\n",
    "            localDict[localRankList[count]] = int(localScoreList[count])\n",
    "            \n",
    "        print('Your selected restaurants:', localRankList)\n",
    "        print('Your scores given to the restaurants', localScoreList)\n",
    "        print('Your final choice is :', localDict)\n",
    "         \n",
    "        self.responseDict = localDict.copy()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiListbox_entries(Frame):\n",
    "    \n",
    "    def __init__(self, master, lists):\n",
    "        Frame.__init__(self, master)\n",
    "        self.lists = []\n",
    "        self.entries = []\n",
    "        self.fields = 'Like', 'Dislike'\n",
    "        #Var for checkbox\n",
    "        #self.var = IntVar()\n",
    "        self.var = []\n",
    "        self.listNames = []\n",
    "        self.responseDict = {}\n",
    "        \n",
    "        #Loop through the lists, l is the list label and widthW is the width \n",
    "        for l,widthW in lists:\n",
    "            \n",
    "            frame = Frame(self); frame.pack(side=LEFT, expand=YES, fill=BOTH)\n",
    "            \n",
    "            Label(frame, text=l, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "            \n",
    "            #store the list names \n",
    "            self.listNames.append(l)\n",
    "            \n",
    "            lb = Listbox(frame, width=widthW, borderwidth=0, selectborderwidth=0,\n",
    "                 relief=FLAT, exportselection=FALSE)\n",
    "            lb.pack(expand=YES, fill=BOTH)\n",
    "            \n",
    "            self.lists.append(lb)\n",
    "            \n",
    "            lb.bind('<B1-Motion>', lambda e, s=self: s._select(e.y))\n",
    "            lb.bind('<Button-1>', lambda e, s=self: s._select(e.y))\n",
    "            \n",
    "            lb.bind('<Leave>', lambda e: 'break')\n",
    "            \n",
    "            lb.bind('<B2-Motion>', lambda e, s=self: s._b2motion(e.x, e.y))\n",
    "            lb.bind('<Button-2>', lambda e, s=self: s._button2(e.x, e.y))\n",
    "             \n",
    "             #loop through the fields to \n",
    "            for field in self.fields:\n",
    "                #row = Frame(self)\n",
    "                lab = Label(frame, width=15, text=field, anchor='w')\n",
    "                ent = Entry(frame)\n",
    "                #row.pack(side=BOTTOM, fill=Y, padx=5, pady=5) #=BOTTOM\n",
    "                lab.pack()\n",
    "                #ent.pack(side=RIGHT, expand=YES, fill=Y)\n",
    "                ent.pack(side=TOP, fill=X)\n",
    "                self.entries.append((field, ent))\n",
    "            \n",
    "            localVar = IntVar()\n",
    "            \n",
    "            self.var.append(localVar)\n",
    "            \n",
    "            #Checkbox \n",
    "            #c = Checkbutton(frame, text=\"Liked\", variable=localVar, command=self.cb(count))\n",
    "            c = Checkbutton(frame, text=\"Liked\", variable=localVar)\n",
    "            c.pack()\n",
    "            \n",
    "        print(self.var)\n",
    "        #Set the submit button \n",
    "        #master.bind('<Button-1>', (lambda event, e=self.entries: fetch(e)))   \n",
    "        #b1 = Button(master, text='Submit', command=(lambda e=self.entries: self.fetch(e)))\n",
    "        b1 = Button(master, text='Submit', command=self.fetchDict)\n",
    "        b1.pack(side=BOTTOM, padx=5, pady=5)\n",
    "        #b2 = Button(root, text='Quit', command=root.quit)\n",
    "        #b2.pack(side=LEFT, padx=5, pady=5)\n",
    "            \n",
    "        #pakcing the frame\n",
    "        frame = Frame(self); frame.pack(side=LEFT, fill=Y)\n",
    "        \n",
    "        #packing the label\n",
    "        Label(frame, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "        #Setting a scrollbar \n",
    "#         sb = Scrollbar(frame, orient=VERTICAL, command=self._scroll)\n",
    "#         sb.pack(expand=YES, fill=Y)\n",
    "#         self.lists[0]['yscrollcommand']=sb.set\n",
    "\n",
    "    def _select(self, y):\n",
    "        row = self.lists[0].nearest(y)\n",
    "        self.selection_clear(0, END)\n",
    "        self.selection_set(row)\n",
    "        return 'break'\n",
    "    \n",
    "    #For checkbox \n",
    "    def cb(self, index):\n",
    "        print (\"variable is\", self.var[index].get())\n",
    "\n",
    "    def _button2(self, x, y):\n",
    "        for l in self.lists: l.scan_mark(x, y)\n",
    "        return 'break'\n",
    "\n",
    "    def _b2motion(self, x, y):\n",
    "        for l in self.lists: l.scan_dragto(x, y)\n",
    "        return 'break'\n",
    "\n",
    "    def _scroll(self, *args):\n",
    "        for l in self.lists:\n",
    "            apply(l.yview, args)\n",
    "\n",
    "    def curselection(self):\n",
    "        return self.lists[0].curselection()\n",
    "\n",
    "    def delete(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.delete(first, last)\n",
    "\n",
    "#     def get(self, first, last=None):\n",
    "#         result = []\n",
    "#         for l in self.lists:\n",
    "#             result.append(l.get(first,last))\n",
    "#         if last: return apply(map, [None] + result)\n",
    "#         return result\n",
    "\n",
    "    def index(self, index):\n",
    "        self.lists[0].index(index)\n",
    "\n",
    "    def insert(self, index, *elements):\n",
    "        #Loop through the elements \n",
    "        for element in elements:\n",
    "            i = 0\n",
    "            for l in self.lists:\n",
    "                l.insert(index, element[i])\n",
    "                i = i + 1\n",
    "\n",
    "    def size(self):\n",
    "        return self.lists[0].size()\n",
    "\n",
    "    def see(self, index):\n",
    "        for l in self.lists:\n",
    "            l.see(index)\n",
    "\n",
    "    def selection_anchor(self, index):\n",
    "        for l in self.lists:\n",
    "            l.selection_anchor(index)\n",
    "\n",
    "    def selection_clear(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.selection_clear(first, last)\n",
    "\n",
    "    def selection_includes(self, index):\n",
    "        return self.lists[0].selection_includes(index)\n",
    "\n",
    "    def selection_set(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.selection_set(first, last)\n",
    "            \n",
    "    def fetch(self,entries):\n",
    "        count = 1\n",
    "        \n",
    "        for entry in entries:\n",
    "            if count <= 2:\n",
    "                listCount = 1\n",
    "            elif count <= 4:\n",
    "                listCount = 2\n",
    "            elif count <= 6:\n",
    "                listCount = 3\n",
    "            elif count <= 8:\n",
    "                listCount = 4\n",
    "            elif count <= 10:\n",
    "                listCount = 5             \n",
    "            #print on odd number counts\n",
    "            if count %2 != 0:\n",
    "                print('list:', listCount)\n",
    "            #index 0 is the field \n",
    "            #index 1 is the entry data \n",
    "            field = entry[0]\n",
    "            text  = entry[1].get()\n",
    "            print('%s: \"%s\"' % (field, text))\n",
    "            \n",
    "            count =count + 1\n",
    "            \n",
    "        print(list(map((lambda var: var.get()), self.var)))\n",
    "   \n",
    "    #This function returns a dictionary of the results\n",
    "    def fetchDict(self):\n",
    "        responseDict = {}\n",
    "        count = 1\n",
    "        \n",
    "        likeResult = list(map((lambda var: var.get()), self.var))\n",
    "        \n",
    "        for entry in self.entries:\n",
    "            tempDict = {}\n",
    "            \n",
    "            field = entry[0]\n",
    "            text  = entry[1].get()\n",
    "            # like:\"text content\"\n",
    "            tempDict[field] = text\n",
    "            \n",
    "            if count <= 2:\n",
    "                listCount = 1\n",
    "            elif count <= 4:\n",
    "                listCount = 2\n",
    "            elif count <= 6:\n",
    "                listCount = 3\n",
    "            elif count <= 8:\n",
    "                listCount = 4\n",
    "            elif count <= 10:\n",
    "                listCount = 5      \n",
    "                \n",
    "            #update the dictionary with corresponding listname :  \n",
    "            try:\n",
    "                responseDict[self.listNames[listCount-1]].update(tempDict) \n",
    "            except:\n",
    "                responseDict[self.listNames[listCount-1]] = tempDict \n",
    "            \n",
    "            if count %2 != 0:\n",
    "                #print on odd number counts\n",
    "                print('list:', listCount)\n",
    "                #Get the hit indicator and update the dictionary \n",
    "                tempLikeDict = {'hit': likeResult[listCount-1]}\n",
    "                responseDict[self.listNames[listCount-1]].update(tempLikeDict)\n",
    "            \n",
    "            #Print the current the field and entry text\n",
    "            print('%s: \"%s\"' % (field, text))\n",
    "           \n",
    "            count =count + 1\n",
    "        \n",
    "        #print the hit list\n",
    "        print(list(map((lambda var: var.get()), self.var)))\n",
    "\n",
    "        self.responseDict = responseDict.copy()\n",
    "        \n",
    "        \n",
    "    #Make the entry form \n",
    "    def makeform(root, fields):\n",
    "        #For each field, like \n",
    "        for field in slef.fields:\n",
    "            row = Frame(master)\n",
    "            lab = Label(row, width=15, text=field, anchor='w')\n",
    "            ent = Entry(row)\n",
    "            row.pack(side=TOP, fill=X, padx=5, pady=5)\n",
    "            lab.pack(side=LEFT)\n",
    "            ent.pack(side=RIGHT, expand=YES, fill=X)\n",
    "            self.entries.append((field, ent))\n",
    "            \n",
    "        self.entries =  entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_vappend(a,b):\n",
    "    \"\"\" Takes in 2 csr_matrices and appends the second one to the bottom of the first one. \n",
    "    Much faster than scipy.sparse.vstack but assumes the type to be csr and overwrites\n",
    "    the first matrix instead of copying it. The data, indices, and indptr still get copied.\"\"\"\n",
    "\n",
    "    a.data = np.hstack((a.data,b.data))\n",
    "    a.indices = np.hstack((a.indices,b.indices))\n",
    "    a.indptr = np.hstack((a.indptr,(b.indptr + a.nnz)[1:]))\n",
    "    a._shape = (a.shape[0]+b.shape[0],b.shape[1])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get original dataframe out of the review datastet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_yelp_df(path ='', filename=reviewJsonToronto, sampling= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188159"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(r'..\\\\data\\\\userStudy\\\\dfWhole.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get rating-UI matrix and timestepm-UI matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix, timestamp_matrix , I_C_matrix = get_rating_timestamp_matrix(df)\n",
    "\n",
    "# get ratingWuserAvg_matrix\n",
    "rating_array = rating_matrix.toarray()\n",
    "user_average_array = rating_array.sum(axis = 1)/np.count_nonzero(rating_array,axis = 1)\n",
    "init_UI = np.zeros(rating_array.shape)\n",
    "init_UI[rating_array.nonzero()] = 1\n",
    "for i in range(user_average_array.shape[0]):\n",
    "    init_UI[i] = init_UI[i] * (user_average_array[i]-0.001) \n",
    "user_average_array = init_UI\n",
    "ratingWuserAvg_array = rating_array - user_average_array\n",
    "ratingWuserAvg_matrix=sparse.csr_matrix(ratingWuserAvg_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split to get rtrain-UI matrix and valid and test.. item_index_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  del sys.path[0]\n",
      "100%|| 6100/6100 [00:01<00:00, 4855.20it/s]\n"
     ]
    }
   ],
   "source": [
    "rtrain_implicit, rvalid_implicit, rtest_implicit, rtrain_userAvg_implicit, rvalid_userAvg_implicit, rtest_userAvg_implicit, nonzero_index, rtime, item_idx_matrix_train_implicit,item_idx_matrix_valid_implicit, item_idx_matrix_test_implicit = time_ordered_splitModified(rating_matrix=rating_matrix, ratingWuserAvg_matrix=ratingWuserAvg_matrix, timestamp_matrix=timestamp_matrix,\n",
    "                                                                     ratio=[0.5,0.2,0.3],\n",
    "                                                                     implicit=True,\n",
    "                                                                     remove_empty=False, threshold=3,sampling=False, \n",
    "                                                                     sampling_ratio=0.1, trainSampling=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6100/6100 [00:01<00:00, 3764.97it/s]\n"
     ]
    }
   ],
   "source": [
    "rtrain, rvalid, rtest, rtrain_userAvg, rvalid_userAvg, rtest_userAvg, nonzero_index, rtime, item_idx_matrix_train,item_idx_matrix_valid, item_idx_matrix_test = time_ordered_splitModified(rating_matrix=rating_matrix, ratingWuserAvg_matrix=ratingWuserAvg_matrix, timestamp_matrix=timestamp_matrix,\n",
    "                                                                     ratio=[0.5,0.2,0.3],\n",
    "                                                                     implicit=False,\n",
    "                                                                     remove_empty=False, threshold=3,\n",
    "                                                                     sampling=False, sampling_ratio=0.1, \n",
    "                                                                     trainSampling=0.95)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTENTION: I am combining rtrain+rvalid+rtest over here to use for user study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain = rtrain + rvalid + rtest \n",
    "rtrain_implicit = rtrain_implicit + rvalid_implicit + rtest_implicit\n",
    "rtrain_userAvg = rtrain_userAvg + rvalid_userAvg + rtest_userAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_numpy(matrix, path, model):\n",
    "    save_npz('{}{}'.format(path, model), matrix)\n",
    "save_numpy(rtrain, \"..\\\\data\\\\userStudy\\\\\", \"Rtrain\")\n",
    "save_numpy(rtrain_implicit, \"..\\\\data\\\\userStudy\\\\\", \"Rtrain_implicit\")\n",
    "save_numpy(rtrain_userAvg, \"..\\\\data\\\\userStudy\\\\\", \"Rtrain_userAvg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I am modifing df_train to the whole df as well, since we are using the whole data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get df shrink to df_train for rtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Get the list of row index for the training set \n",
    "# lst_train = get_corpus_idx_list(df, item_idx_matrix_train)\n",
    "\n",
    "# # Get the training dataframe from the original dataframe\n",
    "# df_train = df.loc[lst_train]\n",
    "\n",
    "# #Resetting the index of the train data\n",
    "# df_train = df_train.reset_index(drop=True)\n",
    "\n",
    "# df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3997"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_train['business_num_id'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If using term frequency only to compute corpus and X(review vs. terms) CounterVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire corpus\n",
    "#corpus = preprocess(df_train)\n",
    "# X row: df_train row, column: key words frequency \n",
    "# When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n",
    "#cv=CountVectorizer(max_df=0.9,stop_words=stop_words, max_features=5000, ngram_range=(1,1))\n",
    "#X=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If using TD-IDF to compute corpus and X (business vs. terms) TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 188159/188159 [01:05<00:00, 2859.12it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = preprocess(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary to store business: review text\n",
    "dict_text = {}\n",
    "for i in range(len(corpus)):\n",
    "    if df_train['business_num_id'][i] not in dict_text:\n",
    "        dict_text[df_train['business_num_id'][i]] = corpus[i]\n",
    "    else:\n",
    "        temp = dict_text[df_train['business_num_id'][i]]\n",
    "        temp = temp + corpus[i]\n",
    "        dict_text[df_train['business_num_id'][i]] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list for the review text, where the row dimension = total business ids\n",
    "list_text = []\n",
    "for key in range(0,max(list(dict_text.keys()))+1) :\n",
    "    if key not in dict_text.keys():\n",
    "        list_text.extend([\"\"])\n",
    "    else:\n",
    "        list_text.extend([dict_text[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the X vector, where dimension is #business vs #terms\n",
    "vectorizer = TfidfVectorizer(max_df=0.9,stop_words=stop_words, max_features=5000, ngram_range=(1,1))\n",
    "X_cleaned = vectorizer.fit_transform(list_text).toarray()\n",
    "X_cleaned_sparse = csr_matrix(X_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store initial similarity matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "UU_similarity_explicit = train(rtrain)\n",
    "UU_similarity_implicit = train(rtrain_implicit)\n",
    "II_similarity_usingUI = train(rtrain.T)\n",
    "#Get II from IK\n",
    "IK_MATRIX = X_cleaned_sparse\n",
    "II_similarity_usingIK = train(IK_MATRIX)\n",
    "II_similarity_usingIC = train(I_C_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Store similarity matrices\n",
    "# np.save(\"..\\\\data\\\\userStudy\\\\UU_explicit.npy\",UU_similarity_explicit)\n",
    "# np.save(\"..\\\\data\\\\userStudy\\\\UU_implicit.npy\",UU_similarity_implicit)\n",
    "# np.save(\"..\\\\data\\\\userStudy\\\\II_usingIU.npy\",II_similarity_usingUI)\n",
    "# np.save(\"..\\\\data\\\\userStudy\\\\II_usingIK.npy\", II_similarity_usingIK)\n",
    "# np.save(\"..\\\\data\\\\userStudy\\\\II_usingIC.npy\",II_similarity_usingIC)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial User Input Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Popularity Metrics\n",
    "#### get the popular items in three ways\n",
    "1. avg stars\n",
    "2. number of reviews\n",
    "3. percentage liked\n",
    "\n",
    "The small analysis and the map are in the Analyse_3_ways_of_popularities.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of reviews popularity list, redundent with the output of the next method\n",
    "dff_popular = df.copy()\n",
    "dff_popular = dff_popular.sort_values(by=[\"review_count_y\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "#Get the list of restaurants accoridng to their popularity level\n",
    "popular_list = dff_popular[\"business_num_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3998/3998 [02:09<00:00, 30.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# get number of users and number of items\n",
    "numUsers = rtrain.shape[0]\n",
    "numItems = rtrain.shape[1]\n",
    "\n",
    "# get the 1d array of avg. stars, number of reviews and percentage liked ratio for the three popular metric\n",
    "popular_list_num_of_reviews,popular_list_avg_stars,popular_list_liked_ratio = get_three_popularity_matrix(df,rtrain)\n",
    "\n",
    "# transfer to a matrix(list * number of users)\n",
    "matrix_popular_list_num_of_reviews = np.tile(popular_list_num_of_reviews,(numUsers+1,1))\n",
    "#for propos, when taking in user data\n",
    "matrix_popular_list_avg_stars = np.tile(popular_list_avg_stars,(numUsers+1,1))\n",
    "#for recommendation\n",
    "matrix_popular_list_liked_ratio = np.tile(popular_list_liked_ratio,(numUsers+1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommend 3 lists for three locations by using \"popular_list_liked_ratio\" method\n",
    "\n",
    "Note that the input of the geographical_dist method can only be list or n-d array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff = df.copy()\n",
    "# dff_popular = df.copy()\n",
    "# dff_popular = dff_popular.sort_values(by=[\"review_count_y\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "# popular_list = dff_popular[\"business_num_id\"].tolist()\n",
    "# dundas_and_yonge = Point(\"43.6561,-79.3802\")\n",
    "# bay_and_queens = Point(\"43.6518,-79.3802\")\n",
    "# king_and_jarvis = Point(\"43.650577,-79.371887\")\n",
    "# bloor_and_yonge = Point(\"43.670409,-79.386814\")\n",
    "# yonge_and_eglinton = Point(\"43.7064,-79.3986\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "isList\n",
      "3998\n",
      "<class 'list'>\n",
      "isList\n",
      "3998\n",
      "<class 'list'>\n",
      "isList\n",
      "3998\n"
     ]
    }
   ],
   "source": [
    "# get a copy of df\n",
    "df_location = df.copy()\n",
    "\n",
    "# three locations for user input\n",
    "yonge_and_finch = Point(\"43.779824, -79.415665\")\n",
    "bloor_and_bathurst = Point(\"43.665194,-79.411208\")\n",
    "queen_and_spadina = Point(\"43.648772,-79.396259\")\n",
    "\n",
    "# three locations for user recommendation\n",
    "bloor_and_yonge = Point(\"43.670409,-79.386814\")\n",
    "dundas_and_yonge = Point(\"43.6561,-79.3802\")\n",
    "spadina_and_dundas = Point(\"43.653004,-79.398082\")\n",
    "\n",
    "# list of percentage liked ratio (transfer the format in order to feed in to the geographical_dist method)\n",
    "list_popular_liked_ratio = popular_list_liked_ratio.tolist()\n",
    "\n",
    "# popularity list for input does not need to input the user_id\n",
    "# put user_id as 0 here but it does not matter\n",
    "# bus_indexRange = 15, gives a list of 15 restaurants per location\n",
    "bus_indexRange = 15\n",
    "yonge_and_finch_list = geographical_dist(list_popular_liked_ratio,yonge_and_finch,0, bus_indexRange)\n",
    "bloor_and_bathurst_list = geographical_dist(list_popular_liked_ratio,bloor_and_bathurst,0, bus_indexRange)\n",
    "queen_and_spadina_list = geographical_dist(list_popular_liked_ratio,queen_and_spadina,0, bus_indexRange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of restaurants for user demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# df_location_yonge_and_finch=df_location[df_location[\"business_num_id\"].isin(yonge_and_finch_list)].drop_duplicates(subset = 'business_num_id', keep = 'first')[[\"business_num_id\",\"name\", \"price\", \"categories\",\"business_stars\",\"review_count_y\"]].rename(columns={'review_count_y': 'review_count'})\n",
    "# df_location_yonge_and_finch.head(2)\n",
    "# df_location_bloor_and_bathurst_list=df_location[df_location[\"business_num_id\"].isin(bloor_and_bathurst_list)].drop_duplicates(subset = 'business_num_id', keep = 'first')[[\"business_num_id\",\"name\",\"price\",\"categories\",\"business_stars\",\"review_count_y\"]].rename(columns={'review_count_y': 'review_count'})\n",
    "# df_location_bloor_and_bathurst_list.head(2)\n",
    "# df_location_queen_and_spadina_list=df_location[df_location[\"business_num_id\"].isin(queen_and_spadina_list)].drop_duplicates(subset = 'business_num_id', keep = 'first')[[\"business_num_id\",\"name\",\"price\",\"categories\",\"business_stars\",\"review_count_y\"]].rename(columns={'review_count_y': 'review_count'})\n",
    "# df_location_queen_and_spadina_list.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the dictionary to be used for UI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\uff0c' in position 44: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-c1bf5ad49b8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"..\\\\data\\\\userStudy\\\\bloorBathurstResDict.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdict2_bloorBathurst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"..\\\\data\\\\userStudy\\\\queenSpadinaResDict.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdict3_queenSpadina\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\uff0c' in position 44: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "#Construct the dictionary to be used for UI \n",
    "#0 is hard coded, not used for now \n",
    "dict1_yongeFinch = constructResDictionary(0, bus_indexRange, yonge_and_finch_list)\n",
    "dict2_bloorBathurst = constructResDictionary(0, bus_indexRange, bloor_and_bathurst_list)\n",
    "dict3_queenSpadina = constructResDictionary(0, bus_indexRange, queen_and_spadina_list)\n",
    "\n",
    "#These construct the restaurant name : business id dictionary for initial user setting\n",
    "RestaurantBusId = {}\n",
    "\n",
    "# yonge_and_finch_list\n",
    "# bloor_and_bathurst_list\n",
    "# queen_and_spadina_list\n",
    "restYF = list(dict1_yongeFinch.keys())\n",
    "restBB = list(dict2_bloorBathurst.keys())\n",
    "restQS = list(dict3_queenSpadina.keys())\n",
    "\n",
    "#loop through all items\n",
    "for count in range(len(yonge_and_finch_list)):\n",
    "    RestaurantBusId[restYF[count]] = yonge_and_finch_list[count]\n",
    "    RestaurantBusId[restBB[count]] = bloor_and_bathurst_list[count]\n",
    "    RestaurantBusId[restQS[count]] = queen_and_spadina_list[count]\n",
    "\n",
    "#RestaurantBusId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation \n",
    "#Get the business information for the recommended business \n",
    "# businessSeries = df[df[\"business_num_id\"] == 1904].iloc[0]\n",
    "# #Get the business name \n",
    "# businessSeries['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial user setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your selected restaurants: ['Roll', 'Basil Box', \"Puck'N Wings\", 'Regal Garden', 'El Pocho Antojitos Bar', 'Krispy Kreme', 'UNIUN', 'Maison Close 1888', 'Stelvio']\n",
      "Your scores given to the restaurants ['5', '4', '4', '5', '5', '4', '5', '4', '4']\n",
      "Your final choice is : {'Roll': 5, 'Basil Box': 4, \"Puck'N Wings\": 4, 'Regal Garden': 5, 'El Pocho Antojitos Bar': 5, 'Krispy Kreme': 4, 'UNIUN': 5, 'Maison Close 1888': 4, 'Stelvio': 4}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #Initiate a frame or master? \n",
    "    tk = Tk()\n",
    "    Label(tk, text='List of restaurants from 3 locations - choose 3 each').pack()\n",
    "    #Label(tk, text='List of restaurants from 3 locations - choose 3 each').grid()\n",
    "    \n",
    "    #Creating the multi-listbox object, pass in a tuple of tuple (lists), this will be the list objects\n",
    "    initialSetuUp = MultiListbox_Initialize(tk, (('Yonge & Finch Intersection', 20), ('Bloor & Bathurst Intersection', 20), \\\n",
    "                            ('Queen & Spadina Intersection', 20)),list(dict1_yongeFinch.keys()),list(dict2_bloorBathurst.keys()),\\\n",
    "                                 list(dict3_queenSpadina.keys()))\n",
    "\n",
    "    #loop through the length of recommend list, # = 5 \n",
    "    for index in range(len(dict1_yongeFinch)):\n",
    "        #First restaurant information to display \n",
    "        #Following the restaurants' name \n",
    "        restList1 = list(dict1_yongeFinch.keys())[index]\n",
    "        restList2 = list(dict2_bloorBathurst.keys())[index]\n",
    "        restList3 = list(dict3_queenSpadina.keys())[index]\n",
    "        \n",
    "        initialSetuUp.insert(END, (' ', ' ', ' '))\n",
    "        \n",
    "        #Inserting the restaurant names\n",
    "        initialSetuUp.insert(END, ('%d: %s' % (index + 1, restList1),'%d: %s' % (index + 1, restList2),\n",
    "                         '%d: %s' % (index + 1, restList3)))\n",
    "        \n",
    "        #Looping through each attribute keys - resinfo\n",
    "        for resinfo in dict1_yongeFinch.get(restList1).keys():\n",
    "            restList1Info = resinfo + ':' + str(dict1_yongeFinch.get(restList1).get(resinfo,''))\n",
    "            restList2Info = resinfo + ':' + str(dict2_bloorBathurst.get(restList2).get(resinfo,''))\n",
    "            restList3Info = resinfo + ':' + str(dict3_queenSpadina.get(restList3).get(resinfo,''))\n",
    "            \n",
    "            initialSetuUp.insert(END, (restList1Info, restList2Info, restList3Info))\n",
    "        \n",
    "        initialSetuUp.insert(END, ('----------------', '----------------', '----------------'))\n",
    "    \n",
    "    initialSetuUp.pack(expand=YES,fill=BOTH)\n",
    "    #initialSetuUp.grid(expand=YES,fill=BOTH)\n",
    "\n",
    "    tk.mainloop()\n",
    "    \n",
    "    #Get user response\n",
    "    UserInitialResponse = initialSetuUp.responseDict\n",
    "    \n",
    "    #store the user response into local file\n",
    "    #csv_fileName = \"UserTestResult{:d}.json\".format(userTestNumber)\n",
    "    #with open('userStudyResults//'+csv_fileName, 'w') as fp:\n",
    "        #json.dump(response, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0] [2235 3602 3858 2327 2684 1862 2312 2201 2706] [5 4 4 5 5 4 5 4 4]\n"
     ]
    }
   ],
   "source": [
    "#Process user inputs\n",
    "assert len(UserInitialResponse) ==9,\"User response has duplicates!\"\n",
    "\n",
    "import statistics \n",
    "#Get row lis\n",
    "rowList = [0] * len(UserInitialResponse)\n",
    "#Get col list\n",
    "colList = []\n",
    "#Get data list\n",
    "dataList = []\n",
    "\n",
    "for resName, rating in UserInitialResponse.items():\n",
    "    #Append the bus_num_id as column values\n",
    "    #RestaurantBusId mapps restuarnt names to business ids\n",
    "    colList.append(RestaurantBusId[resName])\n",
    "    dataList.append(rating)\n",
    "\n",
    "userAvg = statistics.mean(dataList) \n",
    "dataWuserAvgList = np.array(dataList) - np.array([userAvg] * len(UserInitialResponse))+ 0.001\n",
    "rows = np.array(rowList)\n",
    "cols = np.array(colList)\n",
    "data = np.array(dataList)\n",
    "dataWuser = np.array(dataWuserAvgList) \n",
    "print(rows, cols,data)\n",
    "\n",
    "#Get explicit data\n",
    "userSetUpMatrix = csr_matrix((data, (rows, cols)), shape=(1, rtrain.shape[1]))\n",
    "#Get with user rating\n",
    "userSetUpMatrix_WuserAvg = csr_matrix((dataWuser, (rows, cols)), shape=(1, rtrain.shape[1]))\n",
    "\n",
    "#Generate Implicit user rating vector\n",
    "implicitUserSetUpMtx = csr_matrix((data, (rows, cols)), shape=(1, rtrain.shape[1]))\n",
    "implicitUserSetUpMtx[(userSetUpMatrix > 3).nonzero()] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking new user data with original data, ONLY RUN ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6101x3998 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 180041 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This would overwrite rtrain,rtrain_implicit, and rtrain_WuserAvg\n",
    "csr_vappend(rtrain,userSetUpMatrix)\n",
    "csr_vappend(rtrain_implicit, implicitUserSetUpMtx)\n",
    "csr_vappend(rtrain_userAvg, userSetUpMatrix_WuserAvg)\n",
    "\n",
    "#User_index = #This should be the user index rtrain.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarity score for new user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update rtrain similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6100 * 6100\n",
    "#should import \n",
    "initial_rtrain_similarity = np.load('..\\\\data\\\\userStudy\\\\UU_explicit.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the similarity vector for the new user 1*6101\n",
    "newSimilarityVector_explicit = cosine_similarity(X=userSetUpMatrix, Y=rtrain, dense_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new user similarity row\n",
    "# 1 * 6100\n",
    "newSimilarity_row = newSimilarityVector_explicit[0][:-1]\n",
    "newSimilarity_row = np.expand_dims(newSimilarity_row, axis=0)\n",
    "\n",
    "#new user similarity column\n",
    "# 6101 * 1\n",
    "newSimilarity_col = newSimilarityVector_explicit.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack the bottom row\n",
    "new_rtrain_similarity = np.vstack((initial_rtrain_similarity, newSimilarity_row))\n",
    "\n",
    "#stack the last column\n",
    "#new trains similarity: 6101 * 6100, new similarity_col: 6101 * 1\n",
    "new_rtrain_similarity = np.hstack((new_rtrain_similarity, newSimilarity_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "#(new_rtrain_similarity[-1,:] == newSimilarity_row).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update rtrain_implicit similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial rtrain implicit similarity: 6100 * 6100\n",
    "#should import \n",
    "initial_rtrainImplicit_similarity = np.load('..\\\\data\\\\userStudy\\\\UU_implicit.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the similarity vector for the new user 1*6101\n",
    "newSimilarityVector_implicit = cosine_similarity(X=implicitUserSetUpMtx, Y=rtrain_implicit, dense_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new user similarity row\n",
    "# 1 * 6100\n",
    "newSimilarity_implicit_row = newSimilarityVector_implicit[0][:-1]\n",
    "newSimilarity_implicit_row = np.expand_dims(newSimilarity_implicit_row, axis=0)\n",
    "\n",
    "#new user similarity column\n",
    "# 6101 * 1\n",
    "newSimilarity_implicit_col = newSimilarityVector_implicit.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack the bottom row\n",
    "new_rtrainImplicit_similarity = np.vstack((initial_rtrainImplicit_similarity, newSimilarity_implicit_row))\n",
    "\n",
    "#stack the last column\n",
    "#new trains similarity: 6101 * 6100, new similarity_col: 6101 * 1\n",
    "new_rtrainImplicit_similarity = np.hstack((new_rtrainImplicit_similarity, newSimilarity_implicit_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 2, 7],\n",
       "       [0, 0, 3, 7],\n",
       "       [4, 5, 6, 7]], dtype=int32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row = np.array([0, 0, 1, 2, 2, 2])\n",
    "# col = np.array([0, 2, 2, 0, 1, 2])\n",
    "# data = np.array([1, 2, 3, 4, 5, 6])\n",
    "# sp = csr_matrix((data, (row, col)), shape=(3, 3))\n",
    "# sparse.hstack((sp,np.array([7,7,7])[:,None])).A\n",
    "\n",
    "#X  = np.array([1,2,3],[3,3,3],[2,2,2])\n",
    "#X  = np.array([1,2,3])\n",
    "#X = np.expand_dims(X, axis=0)\n",
    "#Y = np.array([1,1,4])\n",
    "#Y = np.expand_dims(Y, axis=0)\n",
    "#Y = np.array([[1, 2, 3], [4, 5, 6], [1,5,7]], np.int32)\n",
    "#similarity = cosine_similarity(X=X, Y=Y, dense_output=True)\n",
    "\n",
    "\n",
    "#dimension of X: 1*3, dimension of Y 3*3, \n",
    "# similarity = cosine_similarity(X=X, Y=Y.transpose(), dense_output=True)\n",
    "# Ynew = np.expand_dims(Y[:,1],axis=0)\n",
    "# cosine_similarity(X=X, Y=Ynew, dense_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-rating KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. With ratings that subtracts user average rating, cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6101/6101 [00:31<00:00, 195.07it/s]\n",
      "100%|| 6101/6101 [00:02<00:00, 2893.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3655, 2235, 1819, 2396, 3489, 3115, 3414, 2726,  691,  753,  883,\n",
       "       2600, 1774, 3166, 3329,  324,  353, 1246,  631, 3909, 1923, 3974,\n",
       "        770, 2460, 1912, 2900, 3042, 3222, 1771, 1914,  918,  966,  593,\n",
       "       3249,  192, 2715, 3982,  292, 2525, 3430, 3882, 3237,  989,  572,\n",
       "        355, 1293, 2175, 2526, 3607, 2647], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "#similarity_1 = train(rtrain)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "#UI_predictionScore_Explicit = predict(rtrain_userAvg, 100, similarity_1, item_similarity_en= False)\n",
    "#UI_predict_Explicit = prediction(UI_predictionScore_Explicit, 50, rtrain_userAvg)\n",
    "#user_item_res1 = evaluate(user_item_predict1, rvalid_userAvg)\n",
    "#UI_predict_Explicit[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implicit User-rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "#similarity_2 = train(rtrain_implicit)\n",
    "UI_predictionScore_Implicit = predict(rtrain_implicit, 100, new_rtrainImplicit_similarity, item_similarity_en= False)\n",
    "UI_predict_Implicit = prediction(UI_predictionScore_Implicit, 50, rtrain_implicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3489,  101, 3059, 2133, 1185,  301, 2579, 1112, 1956,  211, 2156,\n",
       "        248, 1618,  818, 2456, 2974, 3655,  576,  340, 3553, 2053,  687,\n",
       "       1819, 2153, 2760,  709,  792,  816,  829, 2934, 2396,  629, 1488,\n",
       "        753, 3414, 1013,  260,  953,  356, 1847,  741, 2571, 1950, 1299,\n",
       "        606, 1865, 3629, 1710, 3357, 1548], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UI_predict_Implicit[6100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implicit similarity, Explicit user-rating combination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|                                                                  | 943/6101 [00:02<00:14, 354.41it/s]"
     ]
    }
   ],
   "source": [
    "#similarity_3 = train(rtrain_implicit)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "UI_predictionScore_IECombined = predict(rtrain, 100, new_rtrainImplicit_similarity, item_similarity_en= False)\n",
    "UI_predict_IECombined = prediction(UI_predictionScore_IECombined, 50, rtrain)\n",
    "# user_item_res1 = evaluate(UI_predict_IECombined, rvalid)\n",
    "# user_item_res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. NO NEED Something random, to be filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6101/6101 [00:30<00:00, 179.39it/s]\n",
      "100%|| 6101/6101 [00:02<00:00, 2929.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# similarity_4 = train(rtrain)\n",
    "# UI_predictionScore_random1 = predict(rtrain_implicit, 100, similarity_4, item_similarity_en= False)\n",
    "# UI_predict_random1 = prediction(UI_predictionScore_random1, 50, rtrain_implicit)\n",
    "# # user_item_res1 = evaluate(UI_predict_IECombined, rvalid)\n",
    "# # user_item_res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. something random, to be filled 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#userVisitMatrix = getImplicitMatrix(rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity1 = train(rtrain)\n",
    "# similarity2 = train(rtrain_implicit)\n",
    "# similarity3 =train(userVisitMatrix)\n",
    "# similarity4 = train(rtrain_userAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6101/6101 [00:36<00:00, 167.57it/s]\n",
      "100%|| 6101/6101 [00:02<00:00, 2815.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# UI_predictionScore_max = predictUU(rtrain, 100, 'max', similarity1, similarity2, similarity3, similarity4)\n",
    "# UI_predict_max = prediction(UI_predictionScore_max, 50, rtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. IU, Explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3998/3998 [00:32<00:00, 124.25it/s]\n",
      "100%|| 6101/6101 [00:02<00:00, 2189.85it/s]\n"
     ]
    }
   ],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "#similarity_6 = train(rtrain.T)\n",
    "#similarity_2 = train(rtrain_implicit)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "#IU_predictionScore_Explicit = predict(rtrain, 100, similarity_6, item_similarity_en= True)\n",
    "#IU_predict_Explicit = prediction(IU_predictionScore_Explicit, 50, rtrain)\n",
    "#user_item_res1 = evaluate(user_item_predict1, rvalid_userAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3655, 2396,  753, 3489,  691, 2235,  292,  883, 3947, 2018, 1450,\n",
       "       1592, 3222, 1340, 2600,  905,  355, 1520, 1162, 2728, 2663, 3388,\n",
       "       3237, 1147, 1874, 1547,  953, 3775,  728, 3629, 2080,  835, 1725,\n",
       "       2715, 3042, 2526, 3983, 2522, 2856, 1798,   36, 3175, 3245, 3564,\n",
       "       2726, 2887, 1246,  101, 1543, 3546], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IU_predict_Explicit[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Item_based TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "II_similarity_IK = np.load('..\\\\data\\\\userStudy\\\\II_usingIK.npy')\n",
    "item_based_prediction_score4 = predict(rtrain, 10, II_similarity_IK, item_similarity_en= True)\n",
    "#for each restuarant top50 users \n",
    "Item_predict_tfidf = prediction(item_based_prediction_score4, 50, rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 863, 3631, 2728,  758,  135,   92, 3994, 3501, 1546, 2128,  223,\n",
       "       2938, 2666, 1747,  153, 1081,  769, 3663,  828,  227, 2600,   12,\n",
       "       2755, 3719, 3208, 3411, 3661, 1948, 1922, 2834,  699, 1869, 3164,\n",
       "        685, 2971,  145, 2153, 3951,  721,  207, 1000, 2877, 2630, 1340,\n",
       "       1564,  132,  676, 3426, 1960, 1880], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Item_predict_tfidf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation Section: Produce the list of restaurants close to the set location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Remember to manually change this userIndex!!!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# three locations for user recommendation\n",
    "intersection = [bloor_and_yonge,dundas_and_yonge,spadina_and_dundas]\n",
    "# get 3 items for each location\n",
    "businessIndexRange = 3\n",
    "# 5 metrics and store them in a list\n",
    "metric = [UI_predict_Implicit, popular_list, UI_predict_IECombined, IU_predict_Explicit, Item_predict_tfidf]\n",
    "\n",
    "# manually enter this number!!!!!!\n",
    "userIndex = rtrain.shape[0]\n",
    "\n",
    "#Need this number to perform the user test, so don't repeat, don't mess up\n",
    "userTestNumber = random.randint(1,1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6101 223\n"
     ]
    }
   ],
   "source": [
    "print(userIndex, userTestNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3489,  101, 3059, 2133, 1185,  301, 2579, 1112, 1956,  211, 2156,\n",
       "        248, 1618,  818, 2456, 2974, 3655,  576,  340, 3553, 2053,  687,\n",
       "       1819, 2153, 2760,  709,  792,  816,  829, 2934, 2396,  629, 1488,\n",
       "        753, 3414, 1013,  260,  953,  356, 1847,  741, 2571, 1950, 1299,\n",
       "        606, 1865, 3629, 1710, 3357, 1548], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UI_predict_Implicit[6100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the recommendations for each metric using loops, store in the res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "isList\n",
      "3998\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "isList\n",
      "3998\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "isList\n",
      "3998\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# get 3 top recommendations for each locations\n",
    "# store three locations in one list for each metric\n",
    "\n",
    "#loop through the 5 recommendation algorithms, initialize the res list \n",
    "res = [[] for i in range(len(metric))]\n",
    "#Loop through the intersections \n",
    "for i in range(len(intersection)):\n",
    "    #loop through each metric \n",
    "    for j in range(len(metric)):\n",
    "       \n",
    "        temp = geographical_dist(metric[j],intersection[i],userIndex-1,businessIndexRange-1)\n",
    "        res[j] += temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2459, 3232, 2366, 1070, 2774,  554, 1132,  579,  519, 2014,  227,\n",
       "       1575,  558, 2839, 3125, 1104, 3790, 2569, 1799, 1164, 2138, 1816,\n",
       "        340, 1112,  668, 1226, 1532, 1062, 3763,  451, 3706, 3593, 1742,\n",
       "       3707,  241, 1437, 3045, 1311, 2950, 3614, 3300, 2261, 1023, 2020,\n",
       "        990, 1518, 1088, 1086,  656, 2674], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric[j][6100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[629, 2156, 3553, 211, 1618],\n",
       " [1253, 1248, 1795, 1798, 2934, 1488],\n",
       " [629, 1552, 2156, 3553, 1618, 211],\n",
       " [1795, 1658, 1724, 1618],\n",
       " [2459, 1742]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 recommendations for each metric\n",
    "# the sequence is -> [UI_predict_Implicit, popular_list, UI_predict_IECombined, IU_predict_Explicit, Item_predict_tfidf]\n",
    "# get three recommendations for each location, so the len for each row is 3*3\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look back the the origal prediction matrix and rearrange the recommendations list, choose the top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_final = []\n",
    "#loop through the elements in the res list (5 recommend lists)\n",
    "for element in range(len(res)):\n",
    "    dic = {}\n",
    "    #loop through each element in the recomemndation list (order:element)\n",
    "    for i in range(len(res[element])):\n",
    "        #if the recommendation is a list \n",
    "        if isinstance(metric[element], list):\n",
    "            dic[metric[element].index(res[element][i])] = res[element][i]\n",
    "        #if the recommendation is a matrix \n",
    "        else:\n",
    "            dic[metric[element][userIndex-1].tolist().index(res[element][i])] = res[element][i]\n",
    "    temp = []\n",
    "    for j in sorted(dic.keys()):\n",
    "        temp.append(int(dic[j]))\n",
    "    res_final.append(temp[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[211, 2156, 1618],\n",
       " [1795, 2934, 1488],\n",
       " [2156, 1618, 211],\n",
       " [1795, 1724, 1618],\n",
       " [2459, 1742]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the final recommendations for each metric\n",
    "res_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder!!!! manually adjust userIndex after inserting a new row of user preference!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce the list of restaurants close to the set location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #First list\n",
    "# UI_predict_Explicit = geographical_dist(UI_predict_Explicit,intersection,userIndex,businessIndexRange)\n",
    "\n",
    "# #Second list\n",
    "# UI_predict_Implicit = geographical_dist(UI_predict_Implicit,intersection,userIndex,businessIndexRange)\n",
    "\n",
    "# #Third list\n",
    "# UI_predict_IECombined = geographical_dist(UI_predict_IECombined,intersection,userIndex,businessIndexRange)\n",
    "\n",
    "# #Fourth lsit\n",
    "# UI_predict_popular = geographical_dist(popular_list,intersection,userIndex,businessIndexRange)\n",
    "\n",
    "# #Five list\n",
    "# UI_predict_max = geographical_dist(UI_predict_max,intersection,userIndex,businessIndexRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI_predict_Explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI_predict_Implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI_predict_popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI_predict_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df[\"business_num_id\"] == UI_prediction[userIndex][busIndex]].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match restaurant information according to business_num_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Current sequence: UI_predict_Implicit, popular_list, UI_predict_IECombined, IU_predict_Explicit, Item_predict_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'userIndex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-ddfdbace9031>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#Add a for loop for the top recommended items\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdict1_ExplicitRecommend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstructResDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbusinessIndexRange\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mdict2_ImplicitRecommend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstructResDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbusinessIndexRange\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mdict3_EICombineRecommend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstructResDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbusinessIndexRange\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'userIndex' is not defined"
     ]
    }
   ],
   "source": [
    "#Trying to recommend for user 0 for now \n",
    "dict1_ExplicitRecommend = {}\n",
    "dict2_ImplicitRecommend = {}\n",
    "dict3_EICombineRecommend = {}\n",
    "dict4_PopularityRecommend = {}\n",
    "dict5_maxRecommend = {}\n",
    "#User predict [user index] [item index]\n",
    "#Add a for loop for the top recommended items\n",
    "\n",
    "dict1_ExplicitRecommend = constructResDictionary(userIndex, businessIndexRange, res_final[0])\n",
    "dict2_ImplicitRecommend = constructResDictionary(userIndex, businessIndexRange, res_final[1])\n",
    "dict3_EICombineRecommend = constructResDictionary(userIndex, businessIndexRange, res_final[2])\n",
    "dict4_PopularityRecommend = constructResDictionary(userIndex, businessIndexRange, res_final[3])\n",
    "dict5_maxRecommend = constructResDictionary(userIndex, businessIndexRange, res_final[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence for now: UI_predict_Implicit, popular_list, UI_predict_IECombined, IU_predict_Explicit, Item_predict_tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tkinter.IntVar object at 0x00000222CF96B710>, <tkinter.IntVar object at 0x0000022301FDCA58>, <tkinter.IntVar object at 0x0000022302348080>, <tkinter.IntVar object at 0x00000222D0DACDD8>, <tkinter.IntVar object at 0x0000022300A3C128>]\n",
      "list: 1\n",
      "Like: \"lreason for like 1 \"\n",
      "Dislike: \"\"\n",
      "list: 2\n",
      "Like: \"reason like 2 \"\n",
      "Dislike: \"\"\n",
      "list: 3\n",
      "Like: \"okok\"\n",
      "Dislike: \"nto really good \"\n",
      "list: 4\n",
      "Like: \"good \"\n",
      "Dislike: \"\"\n",
      "list: 5\n",
      "Like: \"\"\n",
      "Dislike: \"\"\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #Initiate a frame or master? \n",
    "    tk = Tk()\n",
    "    Label(tk, text='List of recommended restaurants').pack()\n",
    "    \n",
    "    #Creating the multi-listbox object, pass in a tuple of tuple (lists), this will be the list objects\n",
    "    mlb = MultiListbox_entries(tk, (('Implicit User-Rating', 20), ('Popularity List', 20), \\\n",
    "                            ('Explicit Implicit Combined', 20), ('Explicit Item-Rating', 20), ('TF-IDF Score of Item', 20)))\n",
    "\n",
    "    #loop through the length of recommend list, # = 5 \n",
    "    for index in range(len(dict1_ExplicitRecommend)):\n",
    "        #First restaurant information to display \n",
    "        #Following the restaurants' name \n",
    "        restList1 = list(dict1_ExplicitRecommend.keys())[index]\n",
    "        restList2 = list(dict2_ImplicitRecommend.keys())[index]\n",
    "        restList3 = list(dict3_EICombineRecommend.keys())[index]\n",
    "        restList4 = list(dict4_PopularityRecommend.keys())[index]\n",
    "        restList5 = list(dict5_maxRecommend.keys())[index]\n",
    "        \n",
    "        mlb.insert(END, (' ', ' ', ' ', ' ', ' '))\n",
    "        \n",
    "        #Inserting the restaurant names\n",
    "        mlb.insert(END, ('%d: %s' % (index + 1, restList1),'%d: %s' % (index + 1, restList2),\n",
    "                         '%d: %s' % (index + 1, restList3),'%d: %s' % (index + 1, restList4),\n",
    "                         '%d: %s' % (index + 1, restList5)))\n",
    "        \n",
    "        #Looping through each attribute keys - resinfo\n",
    "        for resinfo in dict1_ExplicitRecommend.get(restList1).keys():\n",
    "            restList1Info = resinfo + ':' + str(dict1_ExplicitRecommend.get(restList1).get(resinfo,''))\n",
    "            restList2Info = resinfo + ':' + str(dict2_ImplicitRecommend.get(restList2).get(resinfo,''))\n",
    "            restList3Info = resinfo + ':' + str(dict3_EICombineRecommend.get(restList3).get(resinfo,''))\n",
    "            restList4Info = resinfo + ':' + str(dict4_PopularityRecommend.get(restList4).get(resinfo,''))\n",
    "            restList5Info = resinfo + ':' + str(dict5_maxRecommend.get(restList5).get(resinfo,''))\n",
    "            \n",
    "            mlb.insert(END, (restList1Info, restList2Info, restList3Info, restList4Info, restList5Info))\n",
    "        \n",
    "        mlb.insert(END, ('----------------', '----------------', '----------------', '----------------', '----------------'))\n",
    "    \n",
    "    mlb.pack(expand=YES,fill=BOTH)\n",
    "\n",
    "    tk.mainloop()\n",
    "    \n",
    "    #Get user response\n",
    "    response = mlb.responseDict\n",
    "    \n",
    "    #store the user response into local file\n",
    "    csv_fileName = \"UserTestResult{:d}.json\".format(userTestNumber)\n",
    "    with open('userStudyResults//'+csv_fileName, 'w') as fp:\n",
    "        json.dump(response, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I get the user response information here, so please make sure the sequence for list names are correct when initializing our mlb object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_fileName = \"UserTestResult{:d}.json\".format(userTestNumber)\n",
    "# with open(csv_fileName, 'w') as fp:\n",
    "#     json.dump(response, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load the result file with json \n",
    "# with open(csv_fileName, 'r') as fp:\n",
    "#     test = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2074, 2584, 2628, 2656, 2661, 2666, 2681, 2809, 2939])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the 0th user review data \n",
    "rows, cols = rtrain[rtrain.shape[0]-1].nonzero()\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_num_id</th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>business_stars</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39824</th>\n",
       "      <td>2939</td>\n",
       "      <td>Hot Star Large Fried Chicken</td>\n",
       "      <td>[{'alias': 'chickenshop', 'title': 'Chicken Shop'}]</td>\n",
       "      <td>3.5</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50209</th>\n",
       "      <td>2584</td>\n",
       "      <td>Sansotei Ramen</td>\n",
       "      <td>[{'alias': 'ramen', 'title': 'Ramen'}, {'alias': 'noodles', 'title': 'Noodles'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78288</th>\n",
       "      <td>2809</td>\n",
       "      <td>Kimchi House</td>\n",
       "      <td>[{'alias': 'korean', 'title': 'Korean'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110843</th>\n",
       "      <td>2628</td>\n",
       "      <td>La Palette</td>\n",
       "      <td>[{'alias': 'french', 'title': 'French'}, {'alias': 'desserts', 'title': 'Desserts'}, {'alias': 'bars', 'title': 'Bars'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125039</th>\n",
       "      <td>2074</td>\n",
       "      <td>Kevin's Taiyaki</td>\n",
       "      <td>[{'alias': 'gourmet', 'title': 'Specialty Food'}, {'alias': 'asianfusion', 'title': 'Asian Fusion'}, {'alias': 'japanese', 'title': 'Japanese'}]</td>\n",
       "      <td>4.5</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133837</th>\n",
       "      <td>2661</td>\n",
       "      <td>Sang-Ji Fried Bao</td>\n",
       "      <td>[{'alias': 'noodles', 'title': 'Noodles'}, {'alias': 'dimsum', 'title': 'Dim Sum'}, {'alias': 'soup', 'title': 'Soup'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173529</th>\n",
       "      <td>2666</td>\n",
       "      <td>Juicy Dumpling</td>\n",
       "      <td>[{'alias': 'chinese', 'title': 'Chinese'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173987</th>\n",
       "      <td>2681</td>\n",
       "      <td>El Pocho Antojitos Bar</td>\n",
       "      <td>[{'alias': 'bars', 'title': 'Bars'}, {'alias': 'breakfast_brunch', 'title': 'Breakfast &amp; Brunch'}, {'alias': 'mexican', 'title': 'Mexican'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181290</th>\n",
       "      <td>2656</td>\n",
       "      <td>Hong Kong Bistro Cafe</td>\n",
       "      <td>[{'alias': 'chinese', 'title': 'Chinese'}, {'alias': 'cafes', 'title': 'Cafes'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        business_num_id                          name  \\\n",
       "39824   2939             Hot Star Large Fried Chicken   \n",
       "50209   2584             Sansotei Ramen                 \n",
       "78288   2809             Kimchi House                   \n",
       "110843  2628             La Palette                     \n",
       "125039  2074             Kevin's Taiyaki                \n",
       "133837  2661             Sang-Ji Fried Bao              \n",
       "173529  2666             Juicy Dumpling                 \n",
       "173987  2681             El Pocho Antojitos Bar         \n",
       "181290  2656             Hong Kong Bistro Cafe          \n",
       "\n",
       "                                                                                                                                              categories  \\\n",
       "39824   [{'alias': 'chickenshop', 'title': 'Chicken Shop'}]                                                                                                \n",
       "50209   [{'alias': 'ramen', 'title': 'Ramen'}, {'alias': 'noodles', 'title': 'Noodles'}]                                                                   \n",
       "78288   [{'alias': 'korean', 'title': 'Korean'}]                                                                                                           \n",
       "110843  [{'alias': 'french', 'title': 'French'}, {'alias': 'desserts', 'title': 'Desserts'}, {'alias': 'bars', 'title': 'Bars'}]                           \n",
       "125039  [{'alias': 'gourmet', 'title': 'Specialty Food'}, {'alias': 'asianfusion', 'title': 'Asian Fusion'}, {'alias': 'japanese', 'title': 'Japanese'}]   \n",
       "133837  [{'alias': 'noodles', 'title': 'Noodles'}, {'alias': 'dimsum', 'title': 'Dim Sum'}, {'alias': 'soup', 'title': 'Soup'}]                            \n",
       "173529  [{'alias': 'chinese', 'title': 'Chinese'}]                                                                                                         \n",
       "173987  [{'alias': 'bars', 'title': 'Bars'}, {'alias': 'breakfast_brunch', 'title': 'Breakfast & Brunch'}, {'alias': 'mexican', 'title': 'Mexican'}]       \n",
       "181290  [{'alias': 'chinese', 'title': 'Chinese'}, {'alias': 'cafes', 'title': 'Cafes'}]                                                                   \n",
       "\n",
       "        business_stars  review_count  \n",
       "39824   3.5             52            \n",
       "50209   4.0             94            \n",
       "78288   4.0             36            \n",
       "110843  4.0             215           \n",
       "125039  4.5             56            \n",
       "133837  4.0             93            \n",
       "173529  4.0             73            \n",
       "173987  4.0             47            \n",
       "181290  4.0             87            "
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the corresponding business information using the business num id \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df[df[\"business_num_id\"].isin(cols)].drop_duplicates(subset = 'business_num_id', keep = 'first')[[\"business_num_id\",\"name\",\"categories\",\"business_stars\",\"review_count_y\"]].rename(columns={'review_count_y': 'review_count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tkinter examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "#Multiple checkboxes example\n",
    "\n",
    "# from tkinter import *\n",
    "# class Checkbar(Frame):\n",
    "#    def __init__(self, parent=None, picks=[], side=LEFT, anchor=W):\n",
    "#       Frame.__init__(self, parent)\n",
    "#       self.vars = []\n",
    "    \n",
    "#       for pick in picks:\n",
    "#          var = IntVar()\n",
    "#          chk = Checkbutton(self, text=pick, variable=var)\n",
    "#          chk.pack(side=side, anchor=anchor, expand=YES)\n",
    "#          self.vars.append(var)\n",
    "            \n",
    "#    def state(self):\n",
    "#       return map((lambda var: var.get()), self.vars)\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#    root = Tk()\n",
    "#    lng = Checkbar(root, ['Python', 'Ruby', 'Perl', 'C++'])\n",
    "#    tgl = Checkbar(root, ['English','German'])\n",
    "#    lng.pack(side=TOP,  fill=X)\n",
    "#    tgl.pack(side=LEFT)\n",
    "#    lng.config(relief=GROOVE, bd=2)\n",
    "\n",
    "#    def allstates(): \n",
    "#       print(list(lng.state()), list(tgl.state()))\n",
    "#    Button(root, text='Quit', command=root.quit).pack(side=RIGHT)\n",
    "#    Button(root, text='Peek', command=allstates).pack(side=RIGHT)\n",
    "#    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For checkbox demo \n",
    "# class checkBox(Frame):\n",
    "    \n",
    "#     def __init__(self, master):\n",
    "#             self.var = IntVar()\n",
    "            \n",
    "#             c = Checkbutton(master, text=\"Liked\", variable=self.var, command=self.cb)\n",
    "#             c.pack()\n",
    "\n",
    "#     def cb(self):\n",
    "#         print (\"variable is\", self.var.get())\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     #Initialize the root \n",
    "#     root = Tk()\n",
    "\n",
    "#     #Initialize the entries\n",
    "#     checkbox = checkBox(root)\n",
    "\n",
    "#     root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Demo for text boxes\n",
    "# fields = ['Like', 'Dislike']\n",
    "\n",
    "# def fetch(entries):\n",
    "#     for entry in entries:\n",
    "#         #index 0 is the field \n",
    "#         #index 1 is the entry data \n",
    "#         field = entry[0]\n",
    "#         text  = entry[1].get()\n",
    "#         print('%s: \"%s\"' % (field, text)) \n",
    "\n",
    "# def makeform(root, fields):\n",
    "#     entries = []\n",
    "#     #For each field, like \n",
    "#     for field in fields:\n",
    "#         row = Frame(root)\n",
    "#         lab = Label(row, width=15, text=field, anchor='w')\n",
    "#         ent = Entry(row)\n",
    "#         row.pack(side=TOP, fill=X, padx=5, pady=5)\n",
    "#         lab.pack(side=LEFT)\n",
    "#         ent.pack(side=RIGHT, expand=YES, fill=X)\n",
    "#         entries.append((field, ent))\n",
    "#     return entries\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     #Initialize the root \n",
    "#     root = Tk()\n",
    "    \n",
    "#     #Initialize the entries\n",
    "#     ents = makeform(root, fields)\n",
    "    \n",
    "#     #the buttons \n",
    "#     root.bind('<Button-1>', (lambda event, e=ents: fetch(e)))   \n",
    "#     b1 = Button(root, text='Show', command=(lambda e=ents: fetch(e)))\n",
    "#     b1.pack(side=LEFT, padx=5, pady=5)\n",
    "#     b2 = Button(root, text='Quit', command=root.quit)\n",
    "#     b2.pack(side=LEFT, padx=5, pady=5)\n",
    "#     root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Version\n",
    "# class MultiListbox(Frame):\n",
    "    \n",
    "#     def __init__(self, master, lists):\n",
    "#         Frame.__init__(self, master)\n",
    "#         self.lists = []\n",
    "        \n",
    "#         #Loop through the lists, l is the list label and widthW is the width \n",
    "#         for l,widthW in lists:\n",
    "            \n",
    "#             frame = Frame(self); frame.pack(side=LEFT, expand=YES, fill=BOTH)\n",
    "            \n",
    "#             Label(frame, text=l, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "            \n",
    "#             lb = Listbox(frame, width=widthW, borderwidth=0, selectborderwidth=0,\n",
    "#                  relief=FLAT, exportselection=FALSE)\n",
    "#             lb.pack(expand=YES, fill=BOTH)\n",
    "            \n",
    "#             self.lists.append(lb)\n",
    "            \n",
    "#             lb.bind('<B1-Motion>', lambda e, s=self: s._select(e.y))\n",
    "#             lb.bind('<Button-1>', lambda e, s=self: s._select(e.y))\n",
    "            \n",
    "#             lb.bind('<Leave>', lambda e: 'break')\n",
    "            \n",
    "#             lb.bind('<B2-Motion>', lambda e, s=self: s._b2motion(e.x, e.y))\n",
    "#             lb.bind('<Button-2>', lambda e, s=self: s._button2(e.x, e.y))\n",
    "            \n",
    "#         #pakcing the frame\n",
    "#         frame = Frame(self); frame.pack(side=LEFT, fill=Y)\n",
    "        \n",
    "#         #packing the label\n",
    "#         Label(frame, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "        \n",
    "#         #Setting a scrollbar \n",
    "# #         sb = Scrollbar(frame, orient=VERTICAL, command=self._scroll)\n",
    "        \n",
    "# #         sb.pack(expand=YES, fill=Y)\n",
    "        \n",
    "# #         self.lists[0]['yscrollcommand']=sb.set\n",
    "\n",
    "#     def _select(self, y):\n",
    "#         row = self.lists[0].nearest(y)\n",
    "#         self.selection_clear(0, END)\n",
    "#         self.selection_set(row)\n",
    "#         return 'break'\n",
    "\n",
    "#     def _button2(self, x, y):\n",
    "#         for l in self.lists: l.scan_mark(x, y)\n",
    "#         return 'break'\n",
    "\n",
    "#     def _b2motion(self, x, y):\n",
    "#         for l in self.lists: l.scan_dragto(x, y)\n",
    "#         return 'break'\n",
    "\n",
    "#     def _scroll(self, *args):\n",
    "#         for l in self.lists:\n",
    "#             apply(l.yview, args)\n",
    "\n",
    "#     def curselection(self):\n",
    "#         return self.lists[0].curselection()\n",
    "\n",
    "#     def delete(self, first, last=None):\n",
    "#         for l in self.lists:\n",
    "#             l.delete(first, last)\n",
    "\n",
    "# #     def get(self, first, last=None):\n",
    "# #         result = []\n",
    "# #         for l in self.lists:\n",
    "# #             result.append(l.get(first,last))\n",
    "# #         if last: return apply(map, [None] + result)\n",
    "# #         return result\n",
    "\n",
    "#     def index(self, index):\n",
    "#         self.lists[0].index(index)\n",
    "\n",
    "#     def insert(self, index, *elements):\n",
    "#         #Loop through the elements \n",
    "#         for element in elements:\n",
    "#             i = 0\n",
    "#             for l in self.lists:\n",
    "#                 l.insert(index, element[i])\n",
    "#                 i = i + 1\n",
    "\n",
    "#     def size(self):\n",
    "#         return self.lists[0].size()\n",
    "\n",
    "#     def see(self, index):\n",
    "#         for l in self.lists:\n",
    "#             l.see(index)\n",
    "\n",
    "#     def selection_anchor(self, index):\n",
    "#         for l in self.lists:\n",
    "#             l.selection_anchor(index)\n",
    "\n",
    "#     def selection_clear(self, first, last=None):\n",
    "#         for l in self.lists:\n",
    "#             l.selection_clear(first, last)\n",
    "\n",
    "#     def selection_includes(self, index):\n",
    "#         return self.lists[0].selection_includes(index)\n",
    "\n",
    "#     def selection_set(self, first, last=None):\n",
    "#         for l in self.lists:\n",
    "#             l.selection_set(first, last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Dropdown mdemo\n",
    "# OPTIONS = [\n",
    "# \"Jan\",\n",
    "# \"Feb\",\n",
    "# \"Mar\"\n",
    "# ] #etc\n",
    "\n",
    "# master = Tk()\n",
    "\n",
    "# variable = StringVar(master)\n",
    "# variable.set(OPTIONS[0]) # default value\n",
    "\n",
    "# w = OptionMenu(master, variable, *OPTIONS)\n",
    "# w.pack()\n",
    "\n",
    "# def ok():\n",
    "#     print (\"value is:\" + variable.get())\n",
    "\n",
    "# #retrive the drop down value \n",
    "# button = Button(master, text=\"OK\", command=ok)\n",
    "# button.pack()\n",
    "\n",
    "# mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
