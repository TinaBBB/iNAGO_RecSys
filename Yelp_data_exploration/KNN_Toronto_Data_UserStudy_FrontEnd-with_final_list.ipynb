{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This files combines the basic structures and logic of the implementation of user study, sequence is not correct, the final version will be in another file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in c:\\programdata\\anaconda3\\lib\\site-packages (1.20.0)\n",
      "Requirement already satisfied: geographiclib<2,>=1.49 in c:\\programdata\\anaconda3\\lib\\site-packages (from geopy) (1.50)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.sparse import csr_matrix, load_npz, save_npz\n",
    "from tqdm import tqdm\n",
    "import statistics as stats\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import yaml\n",
    "import scipy.sparse as sparse\n",
    "import yaml\n",
    "from tkinter import *\n",
    "import tkinter\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy import distance\n",
    "from geopy import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewJson = \"..\\\\data\\\\Export_CleanedReview.json\"\n",
    "#reviewJsonWithClosedRes = \"..\\\\data\\\\Export_CleanedReviewWithClosedRes.json\"\n",
    "reviewJsonToronto = \"..\\\\data\\\\Export_TorontoData.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Select top frenquent user and top frequenty restaurants that had at least 1 review >= 4 stars (Kickking out users that gave all  reviews <=3 and restaurants that never got start >= 4 stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yelp_df(path = 'data/', filename = 'Export_CleanedReview.json', sampling=False, top_user_num=6100, top_item_num=4000):\n",
    "    \"\"\"\n",
    "    Get the pandas dataframe\n",
    "    Sampling only the top users/items by density \n",
    "    Implicit representation applies\n",
    "    \"\"\"\n",
    "    with open(filename,'r') as f:\n",
    "        data = f.readlines()\n",
    "        data = list(map(json.loads, data))\n",
    "    \n",
    "    data = data[0]\n",
    "    #Get all the data from the dggeata file\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df.rename(columns={'stars': 'review_stars', 'text': 'review_text', 'cool': 'review_cool',\n",
    "                       'funny': 'review_funny', 'useful': 'review_useful'},\n",
    "              inplace=True)\n",
    "\n",
    "    df['business_num_id'] = df.business_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.business_id.nunique()))\n",
    "    df['business_num_id'] = df['business_num_id'].astype('int')\n",
    "\n",
    "    df['user_num_id'] = df.user_id.astype('category').\\\n",
    "    cat.rename_categories(range(0, df.user_id.nunique()))\n",
    "    df['user_num_id'] = df['user_num_id'].astype('int')\n",
    "\n",
    "    df['timestamp'] = df['date'].apply(date_to_timestamp)\n",
    "\n",
    "    if sampling:\n",
    "        df = filter_yelp_df(df, top_user_num=top_user_num, top_item_num=top_item_num)\n",
    "        # Refresh num id\n",
    "        df['business_num_id'] = df.business_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.business_id.nunique()))\n",
    "        df['business_num_id'] = df['business_num_id'].astype('int')\n",
    "        \n",
    "        df['user_num_id'] = df.user_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.user_id.nunique()))\n",
    "        df['user_num_id'] = df['user_num_id'].astype('int')\n",
    "#     drop_list = ['date','review_id','review_funny','review_cool','review_useful']\n",
    "#     df = df.drop(drop_list, axis=1)\n",
    "\n",
    "    df = df.reset_index(drop = True)\n",
    "\n",
    "    return df \n",
    "\n",
    "def filter_yelp_df(df, top_user_num=6100, top_item_num=4000):\n",
    "    #Getting the reviews where starts are above 3\n",
    "    df_implicit = df[df['review_stars']>3]\n",
    "    frequent_user_id = df_implicit['user_num_id'].value_counts().head(top_user_num).index.values\n",
    "    frequent_item_id = df_implicit['business_num_id'].value_counts().head(top_item_num).index.values\n",
    "    return df.loc[(df['user_num_id'].isin(frequent_user_id)) & (df['business_num_id'].isin(frequent_item_id))]\n",
    "\n",
    "def date_to_timestamp(date):\n",
    "    dt = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    return time.mktime(dt.timetuple())\n",
    "\n",
    "def df_to_sparse(df, row_name='userId', col_name='movieId', value_name='rating',\n",
    "                 shape=None):\n",
    "    rows = df[row_name]\n",
    "    cols = df[col_name]\n",
    "    if value_name is not None:\n",
    "        values = df[value_name]\n",
    "    else:\n",
    "        values = [1]*len(rows)\n",
    "\n",
    "    return csr_matrix((values, (rows, cols)), shape=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rating-UI and timestamp-UI matrix from original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_timestamp_matrix(df, sampling=False, top_user_num=6100, top_item_num=4000):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #make the df implicit with top frenquent users and \n",
    "    #no need to sample anymore if df was sampled before \n",
    "    if sampling:\n",
    "        df = filter_yelp_df(df, top_user_num=top_user_num, top_item_num=top_item_num)\n",
    "\n",
    "    rating_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "                                 col_name='business_num_id',\n",
    "                                 value_name='review_stars',\n",
    "                                 shape=None)\n",
    "    \n",
    "    #Have same dimension and data entries with rating_matrix, except that the review stars are - user avg\n",
    "#     ratingWuserAvg_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "#                                  col_name='business_num_id',\n",
    "#                                  value_name='reviewStars_userAvg',\n",
    "#                                  shape=None)\n",
    "    \n",
    "    timestamp_matrix = df_to_sparse(df, row_name='user_num_id',\n",
    "                                    col_name='business_num_id',\n",
    "                                    value_name='timestamp',\n",
    "                                    shape=None)\n",
    "    \n",
    "    \n",
    "    IC_matrix = get_I_C(df)\n",
    "#     ratingWuserAvg_matrix\n",
    "    return rating_matrix, timestamp_matrix, IC_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_I_C(df):\n",
    "    lst = df.categories.values.tolist()\n",
    "    cat = []\n",
    "    for i in range(len(lst)):\n",
    "        cat.extend(lst[i].split(', '))\n",
    "    unique_cat = set(cat)\n",
    "    #     set categories id\n",
    "    df_cat = pd.DataFrame(list(unique_cat),columns=[\"Categories\"])\n",
    "    df_cat['cat_id'] = df_cat.Categories.astype('category').cat.rename_categories(range(0, df_cat.Categories.nunique()))\n",
    "    dict_cat = df_cat.set_index('Categories')['cat_id'].to_dict()\n",
    "    \n",
    "    df_I_C = pd.DataFrame(columns=['business_num_id', 'cat_id'])\n",
    "    \n",
    "    for i in range((df['business_num_id'].unique().shape)[0]):\n",
    "        df_temp = df[df['business_num_id'] == i].iloc[:1]\n",
    "        temp_lst = df_temp['categories'].to_list()[0].split(\",\")\n",
    "        for j in range(len(temp_lst)):\n",
    "            df_I_C = df_I_C.append({'business_num_id' : i  , 'cat_id' : dict_cat[temp_lst[j].strip()]} , ignore_index=True)\n",
    "    \n",
    "    IC_Matrix = df_to_sparse(df_I_C, row_name='business_num_id',\n",
    "                                 col_name='cat_id',\n",
    "                                 value_name=None,\n",
    "                                 shape=None)    \n",
    "    return IC_Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time ordered split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_ordered_split(rating_matrix, ratingWuserAvg_matrix, timestamp_matrix, ratio=[0.5, 0.2, 0.3],\n",
    "                       implicit=True, remove_empty=False, threshold=3,\n",
    "                       sampling=False, sampling_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Split the data to train,valid,test by time\n",
    "    ratio:  train:valid:test\n",
    "    threshold: for implicit representation\n",
    "    \"\"\"\n",
    "    if implicit:\n",
    "        temp_rating_matrix = sparse.csr_matrix(rating_matrix.shape)\n",
    "        temp_rating_matrix[(rating_matrix > threshold).nonzero()] = 1\n",
    "        rating_matrix = temp_rating_matrix\n",
    "        timestamp_matrix = timestamp_matrix.multiply(rating_matrix)\n",
    "        #ratingWuserAvg_matrix = ratingWuserAvg_matrix.multiply(rating_matrix)\n",
    "\n",
    "    nonzero_index = None\n",
    "\n",
    "    #Default false, not removing empty columns and rows\n",
    "    #Should not have this case, since users should have at least 1 record of 4,5 \n",
    "    #And restuarant should have at least 1 record of 4,5 \n",
    "    if remove_empty:\n",
    "        # Remove empty columns. record original item index\n",
    "        nonzero_index = np.unique(rating_matrix.nonzero()[1])\n",
    "        rating_matrix = rating_matrix[:, nonzero_index]\n",
    "        timestamp_matrix = timestamp_matrix[:, nonzero_index]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[:, nonzero_index]\n",
    "\n",
    "        # Remove empty rows. record original user index\n",
    "        nonzero_rows = np.unique(rating_matrix.nonzero()[0])\n",
    "        rating_matrix = rating_matrix[nonzero_rows]\n",
    "        timestamp_matrix = timestamp_matrix[nonzero_rows]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[nonzero_rows]\n",
    "\n",
    "    user_num, item_num = rating_matrix.shape\n",
    "\n",
    "    rtrain = []\n",
    "    rtrain_userAvg = []\n",
    "    rtime = []\n",
    "    rvalid = []\n",
    "    rvalid_userAvg = []\n",
    "    rtest = []\n",
    "    rtest_userAvg = []\n",
    "    # Get the index list corresponding to item for train,valid,test\n",
    "    item_idx_train = []\n",
    "    item_idx_valid = []\n",
    "    item_idx_test = []\n",
    "    \n",
    "    for i in tqdm(range(user_num)):\n",
    "        #Get the non_zero indexs, restuarants where the user visited/liked if implicit \n",
    "        item_indexes = rating_matrix[i].nonzero()[1]\n",
    "        \n",
    "        #Get the data for the user\n",
    "        data = rating_matrix[i].data\n",
    "        \n",
    "        #Get time stamp value \n",
    "        timestamp = timestamp_matrix[i].data\n",
    "        \n",
    "        #Get review stars with user avg data \n",
    "        if implicit == False:\n",
    "            dataWuserAvg = ratingWuserAvg_matrix[i].data\n",
    "        \n",
    "        #Non zero reviews for this user\n",
    "        num_nonzeros = len(item_indexes)\n",
    "        \n",
    "        #If the user has at least one review\n",
    "        if num_nonzeros >= 1:\n",
    "            #Get number of test and valid data \n",
    "            #train is 30%\n",
    "            num_test = int(num_nonzeros * ratio[2])\n",
    "            #validate is 50%\n",
    "            num_valid = int(num_nonzeros * (ratio[1] + ratio[2]))\n",
    "\n",
    "            valid_offset = num_nonzeros - num_valid\n",
    "            test_offset = num_nonzeros - num_test\n",
    "\n",
    "            #Sort the timestamp for each review for the user\n",
    "            argsort = np.argsort(timestamp)\n",
    "            \n",
    "            #Sort the reviews for the user according to the time stamp \n",
    "            data = data[argsort]\n",
    "            \n",
    "            #Sort the review with user avg accoridng to the time stamp\n",
    "            if implicit == False:\n",
    "                dataWuserAvg = dataWuserAvg[argsort]\n",
    "            \n",
    "            #Non-zero review index sort according to time\n",
    "            item_indexes = item_indexes[argsort]\n",
    "            \n",
    "            #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "            rtrain.append([data[:valid_offset], np.full(valid_offset, i), item_indexes[:valid_offset]])\n",
    "            \n",
    "            if implicit == False:\n",
    "                #Changing valid set to binary\n",
    "                count=valid_offset\n",
    "                for eachData in data[valid_offset:test_offset]:\n",
    "                    #if rating-avgRating > 0 then like\n",
    "                    if eachData >= 4:\n",
    "                        data[count] = 1\n",
    "                    else:\n",
    "                        data[count] = 0\n",
    "                    count += 1\n",
    "                \n",
    "            #50%-70%\n",
    "            rvalid.append([data[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                           item_indexes[valid_offset:test_offset]])\n",
    "            #remaining 30%\n",
    "            rtest.append([data[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "            \n",
    "            if implicit == False:\n",
    "                #Now for the rating matrix that considers user average rating\n",
    "                #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "                rtrain_userAvg.append([dataWuserAvg[:valid_offset], np.full(valid_offset, i), item_indexes[:valid_offset]])\n",
    "                #50%-70%\n",
    "\n",
    "                #Changing valid set to binary\n",
    "                count=valid_offset\n",
    "                for eachData in dataWuserAvg[valid_offset:test_offset]:\n",
    "                    #if rating-avgRating > 0 then like\n",
    "                    if eachData > 0:\n",
    "                        dataWuserAvg[count] = 1\n",
    "                    else:\n",
    "                        dataWuserAvg[count] = 0\n",
    "                    count += 1\n",
    "\n",
    "                rvalid_userAvg.append([dataWuserAvg[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                               item_indexes[valid_offset:test_offset]])\n",
    "\n",
    "                #Change test set to binary even we don't use it\n",
    "                countTest = test_offset\n",
    "                for eachData in dataWuserAvg[test_offset:]:\n",
    "                    #if rating-avgRating > 0 then like\n",
    "                    if eachData > 0:\n",
    "                        dataWuserAvg[count] = 1\n",
    "                    else:\n",
    "                        dataWuserAvg[count] = 0\n",
    "                    count += 1\n",
    "\n",
    "\n",
    "                #remaining 30%\n",
    "                rtest_userAvg.append([dataWuserAvg[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "                \n",
    "            item_idx_train.append(item_indexes[:valid_offset])\n",
    "            \n",
    "#             item_idx_valid.append(item_indexes[valid_offset:test_offset])\n",
    "#             item_idx_test.append(item_indexes[test_offset:])\n",
    "        else:\n",
    "            item_idx_train.append([])\n",
    "#             item_idx_valid.append([])\n",
    "#             item_idx_test.append([])\n",
    "    \n",
    "    rtrain = np.array(rtrain)\n",
    "    rvalid = np.array(rvalid)\n",
    "    rtest = np.array(rtest)\n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = np.array(rtrain_userAvg)\n",
    "        rvalid_userAvg = np.array(rvalid_userAvg)\n",
    "        rtest_userAvg = np.array(rtest_userAvg)\n",
    "\n",
    "    #take non-zeros values, row index, and column (non-zero) index and store into sparse matrix\n",
    "    rtrain = sparse.csr_matrix((np.hstack(rtrain[:, 0]), (np.hstack(rtrain[:, 1]), np.hstack(rtrain[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rvalid = sparse.csr_matrix((np.hstack(rvalid[:, 0]), (np.hstack(rvalid[:, 1]), np.hstack(rvalid[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rtest = sparse.csr_matrix((np.hstack(rtest[:, 0]), (np.hstack(rtest[:, 1]), np.hstack(rtest[:, 2]))),\n",
    "                              shape=rating_matrix.shape, dtype=np.float32)\n",
    "    \n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = sparse.csr_matrix((np.hstack(rtrain_userAvg[:, 0]), (np.hstack(rtrain_userAvg[:, 1]), np.hstack(rtrain_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rvalid_userAvg = sparse.csr_matrix((np.hstack(rvalid_userAvg[:, 0]), (np.hstack(rvalid_userAvg[:, 1]), np.hstack(rvalid_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rtest_userAvg = sparse.csr_matrix((np.hstack(rtest_userAvg[:, 0]), (np.hstack(rtest_userAvg[:, 1]), np.hstack(rtest_userAvg[:, 2]))),\n",
    "                                  shape=rating_matrix.shape, dtype=np.float32)\n",
    "\n",
    "    return rtrain, rvalid, rtest,rtrain_userAvg, rvalid_userAvg, rtest_userAvg, nonzero_index, timestamp_matrix, item_idx_train, item_idx_valid, item_idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_ordered_splitModified(rating_matrix, ratingWuserAvg_matrix, timestamp_matrix, ratio=[0.5, 0.2, 0.3],\n",
    "                       implicit=True, remove_empty=False, threshold=3,\n",
    "                       sampling=False, sampling_ratio=0.1, trainSampling=1):\n",
    "    \"\"\"\n",
    "    Split the data to train,valid,test by time\n",
    "    ratio:  train:valid:test\n",
    "    threshold: for implicit representation\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if implicit:\n",
    "        temp_rating_matrix = sparse.csr_matrix(rating_matrix.shape)\n",
    "        temp_rating_matrix[(rating_matrix > threshold).nonzero()] = 1\n",
    "        rating_matrix = temp_rating_matrix\n",
    "        timestamp_matrix = timestamp_matrix.multiply(rating_matrix)\n",
    "        #ratingWuserAvg_matrix = ratingWuserAvg_matrix.multiply(rating_matrix)\n",
    "\n",
    "    nonzero_index = None\n",
    "\n",
    "    #Default false, not removing empty columns and rows\n",
    "    #Should not have this case, since users should have at least 1 record of 4,5 \n",
    "    #And restuarant should have at least 1 record of 4,5 \n",
    "    if remove_empty:\n",
    "        # Remove empty columns. record original item index\n",
    "        nonzero_index = np.unique(rating_matrix.nonzero()[1])\n",
    "        rating_matrix = rating_matrix[:, nonzero_index]\n",
    "        timestamp_matrix = timestamp_matrix[:, nonzero_index]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[:, nonzero_index]\n",
    "\n",
    "        # Remove empty rows. record original user index\n",
    "        nonzero_rows = np.unique(rating_matrix.nonzero()[0])\n",
    "        rating_matrix = rating_matrix[nonzero_rows]\n",
    "        timestamp_matrix = timestamp_matrix[nonzero_rows]\n",
    "        ratingWuserAvg_matrix = ratingWuserAvg_matrix[nonzero_rows]\n",
    "\n",
    "    user_num, item_num = rating_matrix.shape\n",
    "\n",
    "    rtrain = []\n",
    "    rtrain_userAvg = []\n",
    "    rtime = []\n",
    "    rvalid = []\n",
    "    rvalid_userAvg = []\n",
    "    rtest = []\n",
    "    rtest_userAvg = []\n",
    "    # Get the index list corresponding to item for train,valid,test\n",
    "    item_idx_train = []\n",
    "    item_idx_valid = []\n",
    "    item_idx_test = []\n",
    "    \n",
    "    for i in tqdm(range(user_num)):\n",
    "        #Get the non_zero indexs, restuarants where the user visited/liked if implicit \n",
    "        item_indexes = rating_matrix[i].nonzero()[1]        \n",
    "        #Get the data for the user\n",
    "        data = rating_matrix[i].data      \n",
    "        #Get time stamp value \n",
    "        timestamp = timestamp_matrix[i].data \n",
    "        #Get review stars with user avg data \n",
    "        if implicit == False:\n",
    "            dataWuserAvg = ratingWuserAvg_matrix[i].data\n",
    "\n",
    "            \n",
    "        #Non zero reviews for this user\n",
    "        num_nonzeros = len(item_indexes)\n",
    "        \n",
    "        #If the user has at least one review\n",
    "        if num_nonzeros >= 1:\n",
    "            num_test = int(num_nonzeros * ratio[2])\n",
    "            num_valid = int(num_nonzeros * (ratio[1] + ratio[2]))\n",
    "            valid_offset = num_nonzeros - num_valid\n",
    "            \n",
    "            # Adding this for sampling for training set\n",
    "            valid_offsetSample = int(valid_offset*trainSampling)\n",
    "            test_offset = num_nonzeros - num_test\n",
    "            \n",
    "            #Sort the timestamp for each review for the user\n",
    "            argsort = np.argsort(timestamp)\n",
    "            \n",
    "            #Sort the reviews for the user according to the time stamp \n",
    "            data = data[argsort]\n",
    "            \n",
    "            #Sort the review with user avg accoridng to the time stamp\n",
    "            if implicit == False:\n",
    "                dataWuserAvg = dataWuserAvg[argsort]\n",
    "            \n",
    "            #Non-zero review index sort according to time\n",
    "            item_indexes = item_indexes[argsort]\n",
    "            \n",
    "            #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "            #if take from old to new\n",
    "            #rtrain.append([data[:valid_offsetSample], np.full(valid_offsetSample, i), item_indexes[:valid_offsetSample]])\n",
    "            #if take from new to old\n",
    "            rtrain.append([data[valid_offset-valid_offsetSample:valid_offset], np.full(valid_offsetSample, i), item_indexes[valid_offset-valid_offsetSample:valid_offset]])\n",
    "            rvalid.append([data[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                           item_indexes[valid_offset:test_offset]])\n",
    "            rtest.append([data[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "            \n",
    "            if implicit == False:\n",
    "                #Now for the rating matrix that considers user average rating\n",
    "                #list of ratings, num of valid_offset index, index where there's non-zeros\n",
    "                #from old to new\n",
    "                #rtrain_userAvg.append([dataWuserAvg[:valid_offsetSample], np.full(valid_offsetSample, i), item_indexes[:valid_offsetSample]])\n",
    "                #take nearest\n",
    "                rtrain_userAvg.append([dataWuserAvg[valid_offset-valid_offsetSample:valid_offset], np.full(valid_offsetSample, i), item_indexes[valid_offset-valid_offsetSample:valid_offset]])                \n",
    "                    \n",
    "                rvalid_userAvg.append([dataWuserAvg[valid_offset:test_offset], np.full(test_offset - valid_offset, i),\n",
    "                               item_indexes[valid_offset:test_offset]])\n",
    "                \n",
    "                rtest_userAvg.append([dataWuserAvg[test_offset:], np.full(num_test, i), item_indexes[test_offset:]])\n",
    "                \n",
    "            item_idx_train.append(item_indexes[:valid_offsetSample])\n",
    "            \n",
    "        else:\n",
    "            item_idx_train.append([])\n",
    "    \n",
    "    rtrain = np.array(rtrain)\n",
    "    rvalid = np.array(rvalid)\n",
    "    rtest = np.array(rtest)\n",
    "   \n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = np.array(rtrain_userAvg)\n",
    "        rvalid_userAvg = np.array(rvalid_userAvg)\n",
    "        rtest_userAvg = np.array(rtest_userAvg)\n",
    "\n",
    "    #take non-zeros values, row index, and column (non-zero) index and store into sparse matrix\n",
    "    rtrain = sparse.csr_matrix((np.hstack(rtrain[:, 0]), (np.hstack(rtrain[:, 1]), np.hstack(rtrain[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rvalid = sparse.csr_matrix((np.hstack(rvalid[:, 0]), (np.hstack(rvalid[:, 1]), np.hstack(rvalid[:, 2]))),\n",
    "                               shape=rating_matrix.shape, dtype=np.float32)\n",
    "    rtest = sparse.csr_matrix((np.hstack(rtest[:, 0]), (np.hstack(rtest[:, 1]), np.hstack(rtest[:, 2]))),\n",
    "                              shape=rating_matrix.shape, dtype=np.float32)\n",
    "    \n",
    "    if implicit == False:\n",
    "        rtrain_userAvg = sparse.csr_matrix((np.hstack(rtrain_userAvg[:, 0]), (np.hstack(rtrain_userAvg[:, 1]), np.hstack(rtrain_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rvalid_userAvg = sparse.csr_matrix((np.hstack(rvalid_userAvg[:, 0]), (np.hstack(rvalid_userAvg[:, 1]), np.hstack(rvalid_userAvg[:, 2]))),\n",
    "                                   shape=rating_matrix.shape, dtype=np.float32)\n",
    "        rtest_userAvg = sparse.csr_matrix((np.hstack(rtest_userAvg[:, 0]), (np.hstack(rtest_userAvg[:, 1]), np.hstack(rtest_userAvg[:, 2]))),\n",
    "                                  shape=rating_matrix.shape, dtype=np.float32)\n",
    "\n",
    "    return rtrain, rvalid, rtest,rtrain_userAvg, rvalid_userAvg, rtest_userAvg, nonzero_index, timestamp_matrix, item_idx_train, item_idx_valid, item_idx_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get df for training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Item idex matrix stores the reivews starts\n",
    "#This function returns a list of index for the reviews included in training set \n",
    "def get_corpus_idx_list(df, item_idx_matrix):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    df: total dataframe\n",
    "    item_idx_matrix: train index list got from time_split \n",
    "    Output: row index in original dataframe for training data by time split\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    #For all the users: 5791\n",
    "    for i in tqdm(range(len(item_idx_matrix))):\n",
    "        \n",
    "        #find row index where user_num_id is i\n",
    "        a = df.index[df['user_num_id'] == i].tolist()\n",
    "        \n",
    "        #loop through the busienss id that the user i reviewed for in offvalid set \n",
    "        for item_idx in  item_idx_matrix[i]:\n",
    "            \n",
    "            #get the row index for reviews for business that the user liked in the train set\n",
    "            b = df.index[df['business_num_id'] == item_idx].tolist()\n",
    "            \n",
    "            #Find the index for which this user liked, one user only rate a business once\n",
    "            idx_to_add = list(set(a).intersection(b))\n",
    "            \n",
    "            if idx_to_add not in lst:\n",
    "                lst.extend(idx_to_add)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess using Term Frequency - CounterVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shenti10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shenti10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Stemming and Lemmatisation\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Get corpus and CountVector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "new_words = ['not_the']\n",
    "stop_words = stop_words.union(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#Should 'because' added?\n",
    "def preprocess(df, reset_list = [',','.','?',';','however','but']):\n",
    "    corpus = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        text = df['review_text'][i]\n",
    "        change_flg = 0\n",
    "        #Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        ##Convert to list from string, loop through the review text\n",
    "        text = text.split()\n",
    "        \n",
    "        #any sentence that encounters a not, the folloing words will become not phrase until hit the sentence end\n",
    "        for j in range(len(text)):\n",
    "            #Make the not_ hack\n",
    "            if text[j] == 'not':\n",
    "                change_flg = 1\n",
    "#                 print 'changes is made after ', i\n",
    "                continue\n",
    "            #if was 1 was round and not hit a 'not' in this round\n",
    "            if change_flg == 1 and any(reset in text[j] for reset in reset_list):\n",
    "                text[j] = 'not_' + text[j]\n",
    "                change_flg = 0\n",
    "#                 print 'reset at ', i\n",
    "            if change_flg == 1:\n",
    "                text[j] = 'not_' + text[j]\n",
    "        \n",
    "        #Convert back to string\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "        #Remove punctuations\n",
    "#       text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        \n",
    "        #remove tags\n",
    "        text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "        \n",
    "        # remove special characters and digits\n",
    "        text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "        \n",
    "        ##Convert to list from string\n",
    "        text = text.split()\n",
    "        \n",
    "        ##Stemming\n",
    "        ps=PorterStemmer()\n",
    "        \n",
    "        #Lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        text = [lem.lemmatize(word) for word in text if not word in  \n",
    "                stop_words] \n",
    "        text = \" \".join(text)\n",
    "        corpus.append(text)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def train(matrix_train):\n",
    "    similarity = cosine_similarity(X=matrix_train, Y=None, dense_output=True)\n",
    "    return similarity\n",
    "\n",
    "def get_I_K(df, X, row_name = 'business_num_id', binary = True, shape = (121994,6000)):\n",
    "    \"\"\"\n",
    "    get the item-keyphrase matrix\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    cols = []\n",
    "    vals = []\n",
    "    #For each review history\n",
    "    for i in tqdm(range(X.shape[0])):\n",
    "        #Get the array of frequencies for document/review i \n",
    "        arr = X[i].toarray() \n",
    "        nonzero_element = arr.nonzero()[1]  # Get nonzero element in each line, keyphrase that appears index \n",
    "        length_of_nonzero = len(nonzero_element) #number of important keyphrase that appears\n",
    "        \n",
    "        # df[row_name][i] is the item idex\n",
    "        #Get a list row index that indicates the document/review\n",
    "        rows.extend(np.array([df[row_name][i]]*length_of_nonzero)) ## Item index\n",
    "        #print(rows)\n",
    "        \n",
    "        #Get a list of column index indicating the key phrase that appears in i document/review\n",
    "        cols.extend(nonzero_element) ## Keyword Index\n",
    "        if binary:\n",
    "            #Create a bunch of 1s\n",
    "            vals.extend(np.array([1]*length_of_nonzero))\n",
    "        else:\n",
    "            #If not binary \n",
    "            vals.extend(arr[arr.nonzero()])    \n",
    "    return csr_matrix((vals, (rows, cols)), shape=shape)\n",
    "\n",
    "\n",
    "#Get a UI matrix if it's not item_similarity based or else IU\n",
    "def predict(matrix_train, k, similarity, item_similarity_en = False):\n",
    "    prediction_scores = []\n",
    "    \n",
    "    #inverse to IU matrix\n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    #for each user or item, depends UI or IU \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "        # Get user u's prediction scores for all items\n",
    "        #Get prediction/similarity score for each user 1*num or user or num of items\n",
    "        vector_u = similarity[user_index]\n",
    "\n",
    "        # Get closest K neighbors excluding user u self\n",
    "        #Decending accoding to similarity score, select top k\n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        \n",
    "        # Get neighbors similarity weights and ratings\n",
    "        similar_users_weights = similarity[user_index][similar_users]\n",
    "        \n",
    "        #similar_users_weights_sum = np.sum(similar_users_weights)\n",
    "        #print(similar_users_weights.shape)\n",
    "        #shape: num of res * k\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "              \n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "        #print(prediction_scores_u)\n",
    "        \n",
    "        \n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "        \n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    return res\n",
    "\n",
    "\n",
    "#Preidction score is UI or IU?\n",
    "def prediction(prediction_score, topK, matrix_Train):\n",
    "\n",
    "    prediction = []\n",
    "\n",
    "    #for each user\n",
    "    for user_index in tqdm(range(matrix_Train.shape[0])):\n",
    "        \n",
    "        #take the prediction scores for user 1 * num res\n",
    "        vector_u = prediction_score[user_index]\n",
    "        \n",
    "        #The restuarant the user rated\n",
    "        vector_train = matrix_Train[user_index]\n",
    "        \n",
    "        if len(vector_train.nonzero()[0]) > 0:\n",
    "            vector_predict = sub_routine(vector_u, vector_train, topK=topK)\n",
    "        else:\n",
    "            vector_predict = np.zeros(topK, dtype=np.float32)\n",
    "\n",
    "        prediction.append(vector_predict)\n",
    "\n",
    "    return np.vstack(prediction)\n",
    "\n",
    "#topK: the number of restuarants we are suggesting \n",
    "#if vector_train has number, then the user has visited\n",
    "def sub_routine(vector_u, vector_train, topK=500):\n",
    "\n",
    "    #index where non-zero\n",
    "    train_index = vector_train.nonzero()[1]\n",
    "    \n",
    "    vector_u = vector_u\n",
    "    \n",
    "    #get topk + num rated res prediction score descending, top index \n",
    "    candidate_index = np.argpartition(-vector_u, topK+len(train_index))[:topK+len(train_index)]\n",
    "    \n",
    "    #sort top prediction score index in range topK+len(train_index) into vector_u`\n",
    "    vector_u = candidate_index[vector_u[candidate_index].argsort()[::-1]]\n",
    "    \n",
    "    #deleted the rated res from the topk+train_index prediction score vector for user u \n",
    "    #Delete the user rated res index from the topk+numRated index\n",
    "    vector_u = np.delete(vector_u, np.isin(vector_u, train_index).nonzero()[0])\n",
    "\n",
    "    #so we only include the top K prediction score here\n",
    "    return vector_u[:topK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recallk(vector_true_dense, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "\n",
    "def precisionk(vector_predict, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_predict)\n",
    "\n",
    "\n",
    "def average_precisionk(vector_predict, hits, **unused):\n",
    "    precisions = np.cumsum(hits, dtype=np.float32)/range(1, len(vector_predict)+1)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "def r_precision(vector_true_dense, vector_predict, **unused):\n",
    "    vector_predict_short = vector_predict[:len(vector_true_dense)]\n",
    "    hits = len(np.isin(vector_predict_short, vector_true_dense).nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "\n",
    "def _dcg_support(size):\n",
    "    arr = np.arange(1, size+1)+1\n",
    "    return 1./np.log2(arr)\n",
    "\n",
    "\n",
    "def ndcg(vector_true_dense, vector_predict, hits):\n",
    "    idcg = np.sum(_dcg_support(len(vector_true_dense)))\n",
    "    dcg_base = _dcg_support(len(vector_predict))\n",
    "    dcg_base[np.logical_not(hits)] = 0\n",
    "    dcg = np.sum(dcg_base)\n",
    "    return dcg/idcg\n",
    "\n",
    "\n",
    "def click(hits, **unused):\n",
    "    first_hit = next((i for i, x in enumerate(hits) if x), None)\n",
    "    if first_hit is None:\n",
    "        return 5\n",
    "    else:\n",
    "        return first_hit/10\n",
    "\n",
    "\n",
    "def evaluate(matrix_Predict, matrix_Test, metric_names =['R-Precision', 'NDCG', 'Precision', 'Recall', 'MAP'], atK = [5, 10, 15, 20, 50], analytical=False):\n",
    "    \"\"\"\n",
    "    :param matrix_U: Latent representations of users, for LRecs it is RQ, for ALSs it is U\n",
    "    :param matrix_V: Latent representations of items, for LRecs it is Q, for ALSs it is V\n",
    "    :param matrix_Train: Rating matrix for training, features.\n",
    "    :param matrix_Test: Rating matrix for evaluation, true labels.\n",
    "    :param k: Top K retrieval\n",
    "    :param metric_names: Evaluation metrics\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global_metrics = {\n",
    "        #\"R-Precision\": r_precision,\n",
    "        #\"NDCG\": ndcg,\n",
    "        #\"Clicks\": click\n",
    "    }\n",
    "\n",
    "    local_metrics = {\n",
    "        #\"Precision\": precisionk,\n",
    "        #\"Recall\": recallk,\n",
    "        \"MAP\": average_precisionk\n",
    "    }\n",
    "\n",
    "    output = dict()\n",
    "\n",
    "    num_users = matrix_Predict.shape[0]\n",
    "\n",
    "    for k in atK:\n",
    "\n",
    "        local_metric_names = list(set(metric_names).intersection(local_metrics.keys()))\n",
    "        results = {name: [] for name in local_metric_names}\n",
    "        topK_Predict = matrix_Predict[:, :k]\n",
    "\n",
    "        for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "            vector_predict = topK_Predict[user_index]\n",
    "            if len(vector_predict.nonzero()[0]) > 0:\n",
    "                vector_true = matrix_Test[user_index]\n",
    "                vector_true_dense = vector_true.nonzero()[1]\n",
    "                hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "                if vector_true_dense.size > 0:\n",
    "                    for name in local_metric_names:\n",
    "                        results[name].append(local_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                                 vector_predict=vector_predict,\n",
    "                                                                 hits=hits))\n",
    "        results_summary = dict()\n",
    "        if analytical:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = round(results[name],4)\n",
    "        else:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = (round((np.average(results[name])),4),\n",
    "                                                              round((1.96*np.std(results[name])/np.sqrt(num_users)),4))\n",
    "        output.update(results_summary)\n",
    "\n",
    "    global_metric_names = list(set(metric_names).intersection(global_metrics.keys()))\n",
    "    results = {name: [] for name in global_metric_names}\n",
    "\n",
    "    topK_Predict = matrix_Predict[:]\n",
    "\n",
    "    for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "        vector_predict = topK_Predict[user_index]\n",
    "\n",
    "        if len(vector_predict.nonzero()[0]) > 0:\n",
    "            vector_true = matrix_Test[user_index]\n",
    "            vector_true_dense = vector_true.nonzero()[1]\n",
    "            hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "            # if user_index == 1:\n",
    "            #     import ipdb;\n",
    "            #     ipdb.set_trace()\n",
    "\n",
    "            if vector_true_dense.size > 0:\n",
    "                for name in global_metric_names:\n",
    "                    results[name].append(global_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                              vector_predict=vector_predict,\n",
    "                                                              hits=hits))\n",
    "    results_summary = dict()\n",
    "    if analytical:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = round(results[name],4)\n",
    "    else:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = (round(np.average(results[name]),4), round((1.96*np.std(results[name])/np.sqrt(num_users)),4))\n",
    "    output.update(results_summary)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add 2 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility functions\n",
    "#3906 restuarant, 3000 keyphrase, 5791 user \n",
    "def add_two_matrix(ratio, U_I_matrix,I_K_matrix, shape = (3906, 3000+5791)):\n",
    "    # ratio determine Keywords/User in the matrix\n",
    "    rows = []\n",
    "    cols = []\n",
    "    datas = []\n",
    "    I_U_matrix = U_I_matrix.transpose()\n",
    "    \n",
    "    #for each restuarant\n",
    "    for i in tqdm(range(I_K_matrix.shape[0])):\n",
    "        #key phrase that this item has, column(key phrase) index\n",
    "        nonzero1 = I_K_matrix[i].nonzero()\n",
    "        \n",
    "        #user that rated this item, column(user) index \n",
    "        nonzero2 = I_U_matrix[i].nonzero()\n",
    "        \n",
    "        #Trying to create a sparse matrix that stores \n",
    "        #index of restuarant for (K + U) times\n",
    "        row = [i]*(len(nonzero1[1])+len(nonzero2[1]))\n",
    "        \n",
    "        #column index for key phrase and users that are non-zero\n",
    "        col = nonzero1[1].tolist()+ nonzero2[1].tolist()\n",
    "        \n",
    "        \n",
    "        data = [ratio]*len(nonzero1[1])+[1-ratio]*len(nonzero2[1]) # Binary representation of I-K/U matrix\n",
    "        \n",
    "        rows.extend(row)\n",
    "        cols.extend(col)\n",
    "        datas.extend(data)\n",
    "    return csr_matrix( (datas,(rows,cols)), shape=shape )\n",
    "\n",
    "def transfer_to_implicit(rating_matrix, threshold = 0):\n",
    "    temp_rating_matrix = sparse.csr_matrix(rating_matrix.shape)\n",
    "    temp_rating_matrix[(rating_matrix > threshold).nonzero()] = 1\n",
    "    rating_matrix = temp_rating_matrix\n",
    "    return rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImplicitMatrix(sparseMatrix, threashold=0):\n",
    "    temp_matrix = sparse.csr_matrix(sparseMatrix.shape)\n",
    "    temp_matrix[(sparseMatrix > threashold).nonzero()] = 1\n",
    "    return temp_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictUU(matrix_train, k, chooseWeigthMethod, similarity1=None, similarity2=None, similarity3=None, similarity4=None, similarity5=None, item_similarity_en = False):\n",
    "    prediction_scores = []\n",
    "    #Convert from list to ndarray, add an axis\n",
    "    if isinstance(chooseWeigthMethod, list):\n",
    "        chooseWeigthMethod = np.array(chooseWeigthMethod)[:, np.newaxis]\n",
    "   \n",
    "    \"make sure that when passing in chooseWeightMethod, the weight must be aligned with similarity metrices, even if set to None\"\n",
    "    \"They should add to 1 as well\"\n",
    "    #inverse to IU matrix\n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    #for each user or item, depends UI or IU \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "    #for user_index in tqdm(range(10,20)):\n",
    "        \n",
    "        numberSimilarMatrix = 0\n",
    "        # Get user u's prediction scores for all items \n",
    "        #Get prediction/similarity score for each user 1*num or user or num of items\n",
    "        if similarity1 is not None:\n",
    "            vector_u1 = similarity1[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u1 = [0]*matrix_train.shape[0]\n",
    "            \n",
    "        if similarity2 is not None:\n",
    "            vector_u2 = similarity2[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u2 = [0]*len(vector_u1)\n",
    "            \n",
    "        if similarity3 is not None:\n",
    "            vector_u3 = similarity3[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u3 = [0]*len(vector_u1)\n",
    "            \n",
    "        if similarity4 is not None:\n",
    "            vector_u4 = similarity4[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u4 = [0]*len(vector_u1)\n",
    "        \n",
    "        if similarity5 is not None:\n",
    "            vector_u5 = similarity5[user_index]\n",
    "            numberSimilarMatrix += 1\n",
    "        else:\n",
    "            vector_u5 = [0]*len(vector_u1)\n",
    "        \n",
    "        #Temperary vector that stacks all 4 vectors together\n",
    "        tempVector = np.array([vector_u1,vector_u2,vector_u3,vector_u4, vector_u5])\n",
    "        \n",
    "        if chooseWeigthMethod is None:\n",
    "            #Get the similarity score from the first similarity matrix anyways \n",
    "            vector_u = vector_u1.copy()\n",
    "            \n",
    "        #If we are choosing the max, min, avg or similarity scores\n",
    "        if chooseWeigthMethod is not None:\n",
    "            if chooseWeigthMethod == 'max':\n",
    "                vector_u = tempVector.max(axis=0)\n",
    "            elif chooseWeigthMethod == 'min':\n",
    "                vector_u = tempVector.min(axis=0)\n",
    "            elif chooseWeigthMethod == 'average':\n",
    "                vector_u = tempVector.mean(axis=0)\n",
    "            elif isinstance(chooseWeigthMethod, np.ndarray):\n",
    "                #Validate that number of weights passed in equals number of matrices\n",
    "                #assert(len(chooseWeigthMethod) == numberSimilarMatrix)\n",
    "                #Get the new combined similarity vector \n",
    "                weighted_u = tempVector * chooseWeigthMethod\n",
    "                vector_u =np.sum(weighted_u, axis=0)\n",
    "                #print((vector_u == vector_u4).sum())\n",
    "                \n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        \n",
    "        # Get neighbors similarity weights and ratings\n",
    "        #similar_users_weights = similarity1[user_index][similar_users]\n",
    "        similar_users_weights = vector_u[similar_users]\n",
    "        \n",
    "        #similar_users_weights_sum = np.sum(similar_users_weights)\n",
    "        #print(similar_users_weights.shape)\n",
    "        #shape: num of res * k\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "              \n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "        #print(prediction_scores_u)\n",
    "        \n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "        \n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for UI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the user number that we are trying to recommend for\n",
    "#Enter the # of businesses that we want to recommend for them \n",
    "#Pass in the UI_Prediction matrix for users \n",
    "#userIndex parameter not used \n",
    "def constructResDictionary(userIndex, busIndexRange, UI_prediction):\n",
    "    #Construct the dictionary for the recommended restaurants to display \n",
    "    dictionaryToConstruct = {}\n",
    "    \n",
    "    #Loop through the number of businesses \n",
    "    for busIndex in range(busIndexRange):\n",
    "        #Get the business information for the recommended business \n",
    "        businessSeries = df[df[\"business_num_id\"] == UI_prediction[busIndex]].iloc[0]\n",
    "        #Get the business name \n",
    "        busName = businessSeries['name']\n",
    "        \n",
    "        #get the list of strings to generate the address information \n",
    "        address_generator = (str(w) for w in yaml.safe_load(businessSeries.location)['display_address'])\n",
    "        busLocation = ', '.join(address_generator)\n",
    "        bus_Price = businessSeries.price\n",
    "        busStars = businessSeries.business_stars\n",
    "        busReviewCount = businessSeries.review_count_y \n",
    "        category_generator = (str(s) for s in [cateDict['title'] for cateDict in yaml.safe_load(businessSeries.categories)])\n",
    "        #busCategories = [cateDict['title'] for cateDict in yaml.safe_load(businessSeries.categories)]\n",
    "        busCategories = ', '.join(category_generator)\n",
    "        #Now add the restaurant to the dictionary\n",
    "        dictionaryToConstruct[busName] = {'Address': busLocation,\\\n",
    "                                'Price': bus_Price,\\\n",
    "                                 'Star': busStars, \\\n",
    "                                 'Review Count': busReviewCount, \\\n",
    "                                 'Category': busCategories}\n",
    "    return dictionaryToConstruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get original dataframe out of the review datastet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_yelp_df(path ='', filename=reviewJsonToronto, sampling= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get rating-UI matrix and timestepm-UI matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix, timestamp_matrix , I_C_matrix = get_rating_timestamp_matrix(df)\n",
    "\n",
    "# get ratingWuserAvg_matrix\n",
    "rating_array = rating_matrix.toarray()\n",
    "user_average_array = rating_array.sum(axis = 1)/np.count_nonzero(rating_array,axis = 1)\n",
    "init_UI = np.zeros(rating_array.shape)\n",
    "init_UI[rating_array.nonzero()] = 1\n",
    "for i in range(user_average_array.shape[0]):\n",
    "    init_UI[i] = init_UI[i] * (user_average_array[i]-0.001) \n",
    "user_average_array = init_UI\n",
    "ratingWuserAvg_array = rating_array - user_average_array\n",
    "ratingWuserAvg_matrix=sparse.csr_matrix(ratingWuserAvg_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split to get rtrain-UI matrix and valid and test.. item_index_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  del sys.path[0]\n",
      "100%|| 6100/6100 [00:01<00:00, 3101.55it/s]\n"
     ]
    }
   ],
   "source": [
    "rtrain_implicit, rvalid_implicit, rtest_implicit, rtrain_userAvg_implicit, rvalid_userAvg_implicit, rtest_userAvg_implicit, nonzero_index, rtime, item_idx_matrix_train_implicit,item_idx_matrix_valid_implicit, item_idx_matrix_test_implicit = time_ordered_splitModified(rating_matrix=rating_matrix, ratingWuserAvg_matrix=ratingWuserAvg_matrix, timestamp_matrix=timestamp_matrix,\n",
    "                                                                     ratio=[0.5,0.2,0.3],\n",
    "                                                                     implicit=True,\n",
    "                                                                     remove_empty=False, threshold=3,sampling=False, \n",
    "                                                                     sampling_ratio=0.1, trainSampling=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6100/6100 [00:02<00:00, 2623.05it/s]\n"
     ]
    }
   ],
   "source": [
    "rtrain, rvalid, rtest, rtrain_userAvg, rvalid_userAvg, rtest_userAvg, nonzero_index, rtime, item_idx_matrix_train,item_idx_matrix_valid, item_idx_matrix_test = time_ordered_splitModified(rating_matrix=rating_matrix, ratingWuserAvg_matrix=ratingWuserAvg_matrix, timestamp_matrix=timestamp_matrix,\n",
    "                                                                     ratio=[0.5,0.2,0.3],\n",
    "                                                                     implicit=False,\n",
    "                                                                     remove_empty=False, threshold=3,\n",
    "                                                                     sampling=False, sampling_ratio=0.1, \n",
    "                                                                     trainSampling=0.95)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTENTION: I am combining rtrain+rvalid+rtest over here to use for user study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain = rtrain + rvalid + rtest \n",
    "rtrain_implicit = rtrain_implicit + rvalid_implicit + rtest_implicit\n",
    "rtrain_userAvg = rtrain_userAvg + rvalid_userAvg + rtest_userAvg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I am modifing df_train to the whole df as well, since we are using the whole data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get df shrink to df_train for rtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Get the list of row index for the training set \n",
    "# lst_train = get_corpus_idx_list(df, item_idx_matrix_train)\n",
    "\n",
    "# # Get the training dataframe from the original dataframe\n",
    "# df_train = df.loc[lst_train]\n",
    "\n",
    "# #Resetting the index of the train data\n",
    "# df_train = df_train.reset_index(drop=True)\n",
    "\n",
    "# df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3997"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_train['business_num_id'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If using term frequency only to compute corpus and X(review vs. terms) CounterVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire corpus\n",
    "#corpus = preprocess(df_train)\n",
    "# X row: df_train row, column: key words frequency \n",
    "# When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n",
    "#cv=CountVectorizer(max_df=0.9,stop_words=stop_words, max_features=5000, ngram_range=(1,1))\n",
    "#X=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If using TD-IDF to compute corpus and X (business vs. terms) TfIdfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 187837/187837 [02:25<00:00, 1290.78it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = preprocess(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary to store business: review text\n",
    "dict_text = {}\n",
    "for i in range(len(corpus)):\n",
    "    if df_train['business_num_id'][i] not in dict_text:\n",
    "        dict_text[df_train['business_num_id'][i]] = corpus[i]\n",
    "    else:\n",
    "        temp = dict_text[df_train['business_num_id'][i]]\n",
    "        temp = temp + corpus[i]\n",
    "        dict_text[df_train['business_num_id'][i]] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list for the review text, where the row dimension = total business ids\n",
    "list_text = []\n",
    "for key in range(0,max(list(dict_text.keys()))+1) :\n",
    "    if key not in dict_text.keys():\n",
    "        list_text.extend([\"\"])\n",
    "    else:\n",
    "        list_text.extend([dict_text[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the X vector, where dimension is #business vs #terms\n",
    "vectorizer = TfidfVectorizer(max_df=0.9,stop_words=stop_words, max_features=5000, ngram_range=(1,1))\n",
    "X_cleaned = vectorizer.fit_transform(list_text).toarray()\n",
    "X_cleaned_sparse = csr_matrix(X_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-rating KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. With ratings that subtracts user average rating, cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6100/6100 [00:19<00:00, 317.45it/s]\n",
      "100%|| 6100/6100 [00:02<00:00, 2989.07it/s]\n"
     ]
    }
   ],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "similarity_1 = train(rtrain)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "UI_predictionScore_Explicit = predict(rtrain_userAvg, 100, similarity_1, item_similarity_en= False)\n",
    "UI_predict_Explicit = prediction(UI_predictionScore_Explicit, 50, rtrain_userAvg)\n",
    "#user_item_res1 = evaluate(user_item_predict1, rvalid_userAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3655, 2396, 3115, 1819, 1774, 3329, 3607,  593,  691,  440, 2669,\n",
       "       1293, 2018, 2192, 3609, 3414,  302,  572, 1374,  667, 2294, 3309,\n",
       "        989,  511, 1346,  101, 3166, 2900, 3073, 3542, 1771,  918, 2175,\n",
       "       3909, 3042, 2715, 2856,  155, 3489,  192,  425,  966, 2277, 1582,\n",
       "       2628,  631, 2156, 2771, 2892,  688], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UI_predict_Explicit[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recommendation list if only use train data\n",
    "# array([2396,  101, 2579, 2600, 1819, 3489,  355, 3607, 3542, 1374,  915,\n",
    "#         845, 2018, 1912, 3982,  425, 3245, 1894,  375, 2906, 1083, 2295,\n",
    "#         593, 2842, 1057,  629, 2525, 2538, 3629, 2277,  324, 2816, 3115,\n",
    "#         107,  594, 3951,  302, 1927, 3631, 1935, 1131, 1884,  829, 3166,\n",
    "#        2505, 1393, 3947,  636, 2856,  861], dtype=int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implicit User-rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6100/6100 [00:18<00:00, 337.93it/s]\n",
      "100%|| 6100/6100 [00:02<00:00, 2770.30it/s]\n"
     ]
    }
   ],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "similarity_2 = train(rtrain_implicit)\n",
    "UI_predictionScore_Implicit = predict(rtrain_implicit, 100, similarity_2, item_similarity_en= False)\n",
    "UI_predict_Implicit = prediction(UI_predictionScore_Implicit, 50, rtrain_implicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3655,  753, 2396, 2235, 1582,  101,  355, 2856,  883, 2733, 2663,\n",
       "        691, 2600,  292, 2669, 3222, 1912, 2743, 1248, 2887, 2579, 2526,\n",
       "       3564, 2460, 1914, 2361, 3245, 2156, 1555, 3672, 2728, 1910, 1725,\n",
       "       2080, 3295, 1813, 2146, 2175, 2358, 3629, 3354, 1013,  779,  328,\n",
       "       1800, 1829, 1212, 2547,  508, 3175], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UI_predict_Implicit[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implicit similarity, Explicit user-rating combination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6100/6100 [00:18<00:00, 322.76it/s]\n",
      "100%|| 6100/6100 [00:02<00:00, 2558.61it/s]\n"
     ]
    }
   ],
   "source": [
    "similarity_3 = train(rtrain_implicit)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "UI_predictionScore_IECombined = predict(rtrain, 100, similarity_3, item_similarity_en= False)\n",
    "UI_predict_IECombined = prediction(UI_predictionScore_IECombined, 50, rtrain)\n",
    "# user_item_res1 = evaluate(UI_predict_IECombined, rvalid)\n",
    "# user_item_res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. NO NEED Something random, to be filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6100/6100 [00:20<00:00, 299.12it/s]\n",
      "100%|| 6100/6100 [00:02<00:00, 2714.36it/s]\n"
     ]
    }
   ],
   "source": [
    "similarity_4 = train(rtrain)\n",
    "UI_predictionScore_random1 = predict(rtrain_implicit, 100, similarity_4, item_similarity_en= False)\n",
    "UI_predict_random1 = prediction(UI_predictionScore_random1, 50, rtrain_implicit)\n",
    "# user_item_res1 = evaluate(UI_predict_IECombined, rvalid)\n",
    "# user_item_res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. something random, to be filled 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "userVisitMatrix = getImplicitMatrix(rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity1 = train(rtrain)\n",
    "similarity2 = train(rtrain_implicit)\n",
    "similarity3 =train(userVisitMatrix)\n",
    "similarity4 = train(rtrain_userAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6100/6100 [00:36<00:00, 168.96it/s]\n",
      "100%|| 6100/6100 [00:02<00:00, 2773.05it/s]\n"
     ]
    }
   ],
   "source": [
    "UI_predictionScore_max = predictUU(rtrain, 100, 'max', similarity1, similarity2, similarity3, similarity4)\n",
    "UI_predict_max = prediction(UI_predictionScore_max, 50, rtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. IU, Explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3998/3998 [00:21<00:00, 189.15it/s]\n",
      "100%|| 6100/6100 [00:02<00:00, 2224.60it/s]\n"
     ]
    }
   ],
   "source": [
    "#UU similarity, using cosine similarity\n",
    "similarity_6 = train(rtrain.T)\n",
    "#get a user-item matrix  UI prediction\n",
    "#Predict using UI matrix with ratings in it \n",
    "IU_predictionScore_Explicit = predict(rtrain, 100, similarity_6, item_similarity_en= True)\n",
    "IU_predict_Explicit = prediction(IU_predictionScore_Explicit, 50, rtrain)\n",
    "#user_item_res1 = evaluate(user_item_predict1, rvalid_userAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3655, 2396,  753, 3489,  691, 2235,  292,  883, 3947, 2018, 1450,\n",
       "       1592, 3222, 1340, 2600,  905,  355, 1520, 1162, 2728, 2663, 3388,\n",
       "       3237, 1147, 1874, 1547,  953, 3775,  728, 3629, 2080,  835, 1725,\n",
       "       2715, 3042, 2526, 3983, 2522, 2856, 1798,   36, 3175, 3245, 3564,\n",
       "       2726, 2887, 1246,  101, 1543, 3546], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IU_predict_Explicit[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Item_based TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3998/3998 [00:06<00:00, 559.41it/s]\n",
      "100%|| 6100/6100 [00:02<00:00, 2318.81it/s]\n"
     ]
    }
   ],
   "source": [
    "IK_MATRIX = X_cleaned_sparse\n",
    "I_I_similarity = train(IK_MATRIX)\n",
    "item_based_prediction_score4 = predict(rtrain, 10, I_I_similarity, item_similarity_en= True)\n",
    "#for each restuarant top50 users \n",
    "Item_predict_tfidf = prediction(item_based_prediction_score4, 50, rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 863, 3631, 2728,  758,  135,   92, 3994, 3501, 1546, 2128,  223,\n",
       "       2938, 2666, 1747,  153, 1081,  769, 3663,  828,  227, 2600,   12,\n",
       "       2755, 3719, 3208, 3411, 3661, 1948, 1922, 2834,  699, 1869, 3164,\n",
       "        685, 2971,  145, 2153, 3951,  721,  207, 1000, 2877, 2630, 1340,\n",
       "       1564,  132,  676, 3426, 1960, 1880], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Item_predict_tfidf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Popularity Metrics\n",
    "#### get the popular items in three ways\n",
    "1. avg stars\n",
    "2. number of reviews\n",
    "3. percentage liked\n",
    "\n",
    "The small analysis and the map are in the Analyse_3_ways_of_popularities.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of reviews popularity list, redundent with the output of the next method\n",
    "dff_popular = df.copy()\n",
    "dff_popular = dff_popular.sort_values(by=[\"review_count_y\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "#Get the list of restaurants accoridng to their popularity level\n",
    "popular_list = dff_popular[\"business_num_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_three_popularity_matrix(df_original,rtrain):\n",
    "    # get the list of popular items by ranking the number of reviews\n",
    "    dff_popular = df_original.copy()\n",
    "    dff_popular = dff_popular.sort_values(by=[\"review_count_y\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "    popular_list_num_of_reviews = dff_popular[\"business_num_id\"].tolist()\n",
    "    \n",
    "    # get the list of popular items by ranking average rating score\n",
    "    dff_popular_rating = df_original.copy()\n",
    "    dff_popular_rating = dff_popular.sort_values(by=[\"business_stars\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "    popular_list_avg_stars = dff_popular_rating[\"business_num_id\"].tolist()\n",
    "    \n",
    "    # get the popularity items by using the percentage liked method(number of liked items / total items)\n",
    "    numUsers = rtrain.shape[0]\n",
    "    numItems = rtrain.shape[1]\n",
    "    \n",
    "    predictionMatrix = np.zeros((numUsers , numItems))\n",
    "\n",
    "    # Define function for converting 1-5 rating to 0/1 (like / don't like)\n",
    "    vf = np.vectorize(lambda x: 1 if x >= 4 else 0)\n",
    "    rtrain_array = rtrain.toarray()\n",
    "    # For every item calculate the number of people liked (4-5) divided by the number of people that rated\n",
    "    itemPopularity = np.zeros((numItems))\n",
    "    for item in range(numItems):\n",
    "        numOfUsersRated = len(rtrain_array[:, item].nonzero()[0])\n",
    "        numOfUsersLiked = len(vf(rtrain_array[:, item]).nonzero()[0])\n",
    "#         if numOfUsersRated == 0:\n",
    "        # set a threshold to filter out restaurants with very few reviews\n",
    "        if numOfUsersRated <= 100:\n",
    "            itemPopularity[item] = 0\n",
    "        else:\n",
    "            itemPopularity[item] = numOfUsersLiked/numOfUsersRated\n",
    "    popular_list_liked_ratio = itemPopularity.argsort()\n",
    "    \n",
    "    return np.asarray(popular_list_num_of_reviews),np.asarray(popular_list_avg_stars),popular_list_liked_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of users and number of items\n",
    "numUsers = rtrain.shape[0]\n",
    "numItems = rtrain.shape[1]\n",
    "\n",
    "# get the 1d array of avg. stars, number of reviews and percentage liked ratio for the three popular metric\n",
    "popular_list_num_of_reviews,popular_list_avg_stars,popular_list_liked_ratio = get_three_popularity_matrix(df,rtrain)\n",
    "\n",
    "# transfer to a matrix(list * number of users)\n",
    "matrix_popular_list_num_of_reviews = np.tile(popular_list_num_of_reviews,(numUsers,1))\n",
    "matrix_popular_list_avg_stars = np.tile(popular_list_avg_stars,(numUsers,1))\n",
    "matrix_popular_list_liked_ratio = np.tile(popular_list_liked_ratio,(numUsers,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommend 3 lists for three locations by using \"popular_list_liked_ratio\" method\n",
    "\n",
    "Note that the input of the geographical_dist method can only be list or n-d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either a list or an array\n",
    "def geographical_dist(prediction_matrix,intersection,user_id, bus_indexRange):\n",
    "    lst = []\n",
    "    length = 0\n",
    "    #if the prediction matrix is a list of items for an user \n",
    "    if isinstance(prediction_matrix, list):\n",
    "        length = len(prediction_matrix)\n",
    "    #loop through the prediction matrix for the user, if passed in a prediction matrix \n",
    "    else:\n",
    "        length = prediction_matrix[user_id].shape[0]\n",
    "    for j in range(length):\n",
    "        if isinstance(prediction_matrix, list):\n",
    "            coordinateDict = yaml.safe_load(df_location[df_location[\"business_num_id\"] == prediction_matrix[j]].iloc[0].coordinates)\n",
    "        else:    \n",
    "            coordinateDict = yaml.safe_load(df_location[df_location[\"business_num_id\"] == prediction_matrix[user_id][j]].iloc[0].coordinates)\n",
    "        test_point = Point(coordinateDict['latitude'],coordinateDict['longitude'])\n",
    "        \n",
    "        #Get the distance with the test point\n",
    "        result = distance.distance(intersection,test_point).kilometers\n",
    "        if result<=0.6:\n",
    "            #append the jth item if the condition matches\n",
    "            if isinstance(prediction_matrix, list):\n",
    "                lst.append(prediction_matrix[j])\n",
    "            else:\n",
    "                lst.append(prediction_matrix[user_id][j])\n",
    "        lst = lst[0:bus_indexRange]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This step should be at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a copy of df\n",
    "df_location = df.copy()\n",
    "\n",
    "# three locations for user input\n",
    "yonge_and_finch = Point(\"43.779824, -79.415665\")\n",
    "bloor_and_bathurst = Point(\"43.665194,-79.411208\")\n",
    "queen_and_spadina = Point(\"43.648772,-79.396259\")\n",
    "\n",
    "# three locations for user recommendation\n",
    "bloor_and_yonge = Point(\"43.670409,-79.386814\")\n",
    "dundas_and_yonge = Point(\"43.6561,-79.3802\")\n",
    "spadina_and_dundas = Point(\"43.653004,-79.398082\")\n",
    "\n",
    "# list of percentage liked ratio (transfer the format in order to feed in to the geographical_dist method)\n",
    "list_popular_liked_ratio = popular_list_liked_ratio.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# popularity list for input does not need to input the user_id\n",
    "# put user_id as 0 here but it does not matter\n",
    "# bus_indexRange = 15, gives a list of 15 restaurants per location\n",
    "bus_indexRange = 15\n",
    "yonge_and_finch_list = geographical_dist(list_popular_liked_ratio,yonge_and_finch,0, bus_indexRange)\n",
    "bloor_and_bathurst_list = geographical_dist(list_popular_liked_ratio,bloor_and_bathurst,0, bus_indexRange)\n",
    "queen_and_spadina_list = geographical_dist(list_popular_liked_ratio,queen_and_spadina,0, bus_indexRange)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the dictionary to be used for UI \n",
    "#0 is hard coded, not used for now \n",
    "dict1_yongeFinch = constructResDictionary(0, bus_indexRange, yonge_and_finch_list)\n",
    "dict2_bloorBathurst = constructResDictionary(0, bus_indexRange, bloor_and_bathurst_list)\n",
    "dict3_queenSpadina = constructResDictionary(0, bus_indexRange, queen_and_spadina_list)\n",
    "\n",
    "\n",
    "#These construct the restaurant name : business id dictionary for initial user setting\n",
    "RestaurantBusId = {}\n",
    "\n",
    "# yonge_and_finch_list\n",
    "# bloor_and_bathurst_list\n",
    "# queen_and_spadina_list\n",
    "restYF = list(dict1_yongeFinch.keys())\n",
    "restBB = list(dict2_bloorBathurst.keys())\n",
    "restQS = list(dict3_queenSpadina.keys())\n",
    "\n",
    "#loop through all items\n",
    "for count in range(len(yonge_and_finch_list)):\n",
    "    RestaurantBusId[restYF[count]] = yonge_and_finch_list[count]\n",
    "    RestaurantBusId[restBB[count]] = bloor_and_bathurst_list[count]\n",
    "    RestaurantBusId[restQS[count]] = queen_and_spadina_list[count]\n",
    "\n",
    "RestaurantBusId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation \n",
    "#Get the business information for the recommended business \n",
    "# businessSeries = df[df[\"business_num_id\"] == 1904].iloc[0]\n",
    "# #Get the business name \n",
    "# businessSeries['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the UI for user to choose restaurant from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiListbox_Initialize(Frame):\n",
    "    \n",
    "    def __init__(self, master, lists,list1Res,list2Res,list3Res):\n",
    "        Frame.__init__(self, master)\n",
    "        self.lists = []\n",
    "        #self.entries = []\n",
    "        #self.fields = 'Like', 'Dislike'\n",
    "        self.dropDownTextBox = 'Rank1','Score1', 'Rank2','Score2','Rank3','Score3'\n",
    "        #Var for checkbox\n",
    "        #self.var = IntVar()\n",
    "        self.var = []\n",
    "        self.listNames = []\n",
    "        self.responseDict = {}\n",
    "        #self.options = [\"Jan\", \"Feb\", \"Mar\"] #etc\n",
    "        #No 0s or else you won't choose it as liked \n",
    "        self.scores = [5, 4, 3, 2, 1]\n",
    "        #stores the list of restaurants for list 1-3\n",
    "        self.list1Name = list1Res\n",
    "        self.list2Name = list2Res\n",
    "        self.list3Name = list3Res\n",
    "        #To get response from dropdowns \n",
    "        self.rankVar = []\n",
    "        self.scoreVar = []\n",
    "        \n",
    "        #list count 0,1,2\n",
    "        listCount = 0 \n",
    "        #Loop through the lists, l is the list label and widthW is the width \n",
    "        for l,widthW in lists:\n",
    "            \n",
    "            frame = Frame(self); frame.pack(side=LEFT, expand=YES, fill=BOTH)\n",
    "            \n",
    "            Label(frame, text=l, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "            \n",
    "            #store the list names \n",
    "            self.listNames.append(l)\n",
    "            \n",
    "            lb = Listbox(frame, width=widthW, borderwidth=0, selectborderwidth=0,\n",
    "                 relief=FLAT, exportselection=FALSE)\n",
    "            lb.pack(expand=YES, fill=BOTH)\n",
    "            \n",
    "            self.lists.append(lb)\n",
    "            \n",
    "            #lb.bind('<B1-Motion>', lambda e, s=self: s._select(e.y))\n",
    "            #lb.bind('<Button-1>', lambda e, s=self: s._select(e.y))\n",
    "            \n",
    "            #lb.bind('<Leave>', lambda e: 'break')\n",
    "            \n",
    "            #lb.bind('<B2-Motion>', lambda e, s=self: s._b2motion(e.x, e.y))\n",
    "            #lb.bind('<Button-2>', lambda e, s=self: s._button2(e.x, e.y))\n",
    "             \n",
    "            for fieldText in self.dropDownTextBox:\n",
    "\n",
    "                if('Rank' in fieldText):\n",
    "                    \n",
    "                    #Get the current list number \n",
    "                    if listCount+1 == 1:\n",
    "                        currentOptions = self.list1Name\n",
    "                    elif listCount+1 == 2:\n",
    "                        currentOptions = self.list2Name\n",
    "                    elif listCount+1 == 3:\n",
    "                        currentOptions = self.list3Name\n",
    "                        \n",
    "                    variable = StringVar(master)\n",
    "                    variable.set(currentOptions[0]) # default value\n",
    "\n",
    "                    #row = Frame(self)\n",
    "                    lab = Label(frame, width=15, text=fieldText, anchor='center')\n",
    "                    drowDwn = OptionMenu(frame, variable, *currentOptions)\n",
    "                    \n",
    "                    #Append the variable set from dropdown \n",
    "                    self.rankVar.append(variable)\n",
    "                    \n",
    "\n",
    "                    lab.pack(side=TOP,fill=X)\n",
    " \n",
    "                    drowDwn.pack(side=TOP)\n",
    "\n",
    "\n",
    "                elif('Score' in fieldText ):\n",
    "                    variable = StringVar(master)\n",
    "                    variable.set(self.scores[0]) # default value\n",
    "\n",
    "                    #row = Frame(self)\n",
    "                    lab = Label(frame, width=15, text=fieldText, anchor='center')\n",
    "                    drowDwn = OptionMenu(frame, variable, *self.scores)\n",
    "                    \n",
    "                    #Append te varaible set from dropdown \n",
    "                    self.scoreVar.append(variable)\n",
    "\n",
    "                    lab.pack(side=TOP, fill=X)\n",
    "                    drowDwn.pack(side=TOP)\n",
    "            \n",
    "            listCount += 1\n",
    "            \n",
    "        #Set the submit button  \n",
    "        b1 = Button(master, text='Submit', command=self.fetch)\n",
    "        b1.pack(side=BOTTOM, padx=5, pady=5)\n",
    "\n",
    "        #pakcing the frame\n",
    "        #frame = Frame(self); frame.pack(side=LEFT, fill=Y)\n",
    "        \n",
    "        #packing the label\n",
    "        #Label(frame, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "\n",
    "    def _select(self, y):\n",
    "        row = self.lists[0].nearest(y)\n",
    "        self.selection_clear(0, END)\n",
    "        self.selection_set(row)\n",
    "        return 'break'\n",
    "\n",
    "    def _button2(self, x, y):\n",
    "        for l in self.lists: l.scan_mark(x, y)\n",
    "        return 'break'\n",
    "\n",
    "    def _b2motion(self, x, y):\n",
    "        for l in self.lists: l.scan_dragto(x, y)\n",
    "        return 'break'\n",
    "\n",
    "    def _scroll(self, *args):\n",
    "        for l in self.lists:\n",
    "            apply(l.yview, args)\n",
    "\n",
    "    def curselection(self):\n",
    "        return self.lists[0].curselection()\n",
    "\n",
    "    def delete(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.delete(first, last)\n",
    "\n",
    "#     def get(self, first, last=None):\n",
    "#         result = []\n",
    "#         for l in self.lists:\n",
    "#             result.append(l.get(first,last))\n",
    "#         if last: return apply(map, [None] + result)\n",
    "#         return result\n",
    "\n",
    "    def index(self, index):\n",
    "        self.lists[0].index(index)\n",
    "\n",
    "    def insert(self, index, *elements):\n",
    "        #Loop through the elements \n",
    "        for element in elements:\n",
    "            i = 0\n",
    "            for l in self.lists:\n",
    "                l.insert(index, element[i])\n",
    "                i = i + 1\n",
    "\n",
    "    def size(self):\n",
    "        return self.lists[0].size()\n",
    "\n",
    "    def see(self, index):\n",
    "        for l in self.lists:\n",
    "            l.see(index)\n",
    "\n",
    "    def selection_anchor(self, index):\n",
    "        for l in self.lists:\n",
    "            l.selection_anchor(index)\n",
    "\n",
    "    def selection_clear(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.selection_clear(first, last)\n",
    "\n",
    "    def selection_includes(self, index):\n",
    "        return self.lists[0].selection_includes(index)\n",
    "\n",
    "    def selection_set(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.selection_set(first, last)\n",
    "            \n",
    "    def fetch(self):\n",
    "        localDict = {}\n",
    "        localRankList = list(map((lambda var: var.get()), self.rankVar))\n",
    "        localScoreList = list(map((lambda var: var.get()), self.scoreVar))\n",
    "        \n",
    "        #lop through each selected restaurant and score, construct the dictionary \n",
    "        for count in range(len(localRankList)):\n",
    "            localDict[localRankList[count]] = int(localScoreList[count])\n",
    "            \n",
    "        print('Your selected restaurants:', localRankList)\n",
    "        print('Your scores given to the restaurants', localScoreList)\n",
    "        print('Your final choice is :', localDict)\n",
    "         \n",
    "        self.responseDict = localDict.copy()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your selected restaurants: ['Sang-Ji Fried Bao', 'Pyung Won House', 'Hot Star Large Fried Chicken', 'Mapo Korean BBQ', 'Apiecalypse Now!', 'Jin Dal Lae', 'The Porch Toronto', 'Hong Kong Bistro Cafe', 'b.good']\n",
      "Your scores given to the restaurants ['5', '5', '5', '5', '5', '5', '5', '5', '5']\n",
      "Your final choice is : {'Sang-Ji Fried Bao': 5, 'Pyung Won House': 5, 'Hot Star Large Fried Chicken': 5, 'Mapo Korean BBQ': 5, 'Apiecalypse Now!': 5, 'Jin Dal Lae': 5, 'The Porch Toronto': 5, 'Hong Kong Bistro Cafe': 5, 'b.good': 5}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #Initiate a frame or master? \n",
    "    tk = Tk()\n",
    "    Label(tk, text='List of restaurants from 3 locations - choose 3 each').pack()\n",
    "    \n",
    "    #Creating the multi-listbox object, pass in a tuple of tuple (lists), this will be the list objects\n",
    "    initialSetuUp = MultiListbox_Initialize(tk, (('Yonge & Finch Intersection', 20), ('Bloor & Bathurst Intersection', 20), \\\n",
    "                            ('Queen & Spadina Intersection', 20)),list(dict1_yongeFinch.keys()),list(dict2_bloorBathurst.keys()),\\\n",
    "                                 list(dict3_queenSpadina.keys()))\n",
    "\n",
    "    #loop through the length of recommend list, # = 5 \n",
    "    for index in range(len(dict1_yongeFinch)):\n",
    "        #First restaurant information to display \n",
    "        #Following the restaurants' name \n",
    "        restList1 = list(dict1_yongeFinch.keys())[index]\n",
    "        restList2 = list(dict2_bloorBathurst.keys())[index]\n",
    "        restList3 = list(dict3_queenSpadina.keys())[index]\n",
    "        \n",
    "        initialSetuUp.insert(END, (' ', ' ', ' '))\n",
    "        \n",
    "        #Inserting the restaurant names\n",
    "        initialSetuUp.insert(END, ('%d: %s' % (index + 1, restList1),'%d: %s' % (index + 1, restList2),\n",
    "                         '%d: %s' % (index + 1, restList3)))\n",
    "        \n",
    "        #Looping through each attribute keys - resinfo\n",
    "        for resinfo in dict1_yongeFinch.get(restList1).keys():\n",
    "            restList1Info = resinfo + ':' + str(dict1_yongeFinch.get(restList1).get(resinfo,''))\n",
    "            restList2Info = resinfo + ':' + str(dict2_bloorBathurst.get(restList2).get(resinfo,''))\n",
    "            restList3Info = resinfo + ':' + str(dict3_queenSpadina.get(restList3).get(resinfo,''))\n",
    "            \n",
    "            initialSetuUp.insert(END, (restList1Info, restList2Info, restList3Info))\n",
    "        \n",
    "        initialSetuUp.insert(END, ('----------------', '----------------', '----------------'))\n",
    "    \n",
    "    initialSetuUp.pack(expand=YES,fill=BOTH)\n",
    "\n",
    "    tk.mainloop()\n",
    "    \n",
    "    #Get user response\n",
    "    UserInitialResponse = initialSetuUp.responseDict\n",
    "    \n",
    "    #store the user response into local file\n",
    "    #csv_fileName = \"UserTestResult{:d}.json\".format(userTestNumber)\n",
    "    #with open('userStudyResults//'+csv_fileName, 'w') as fp:\n",
    "        #json.dump(response, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "UserInitialResponse\n",
    "assert len(UserInitialResponse) ==9,\"User response has duplicates!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0] [2661 2697 2939 2632 2951 2950 2612 2656 2654] [5 5 5 5 5 5 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "#Get row lis\n",
    "rowList = [0] * len(UserInitialResponse)\n",
    "#Get col list\n",
    "colList = []\n",
    "#Get data list\n",
    "dataList = []\n",
    "\n",
    "for resName, rating in UserInitialResponse.items():\n",
    "    #Append the bus_num_id as column values\n",
    "    #RestaurantBusId mapps restuarnt names to business ids\n",
    "    colList.append(RestaurantBusId[resName])\n",
    "    dataList.append(rating)\n",
    "\n",
    "userAvg = statistics.mean(dataList) \n",
    "dataWuserAvgList = np.array(dataList) - np.array([userAvg] * len(UserInitialResponse))+ 0.001\n",
    "rows = np.array(rowList)\n",
    "cols = np.array(colList)\n",
    "data = np.array(dataList)\n",
    "dataWuser = np.array(dataWuserAvgList) \n",
    "print(rows, cols,data)\n",
    "\n",
    "#Get explicit data\n",
    "userSetUpMatrix = csr_matrix((data, (rows, cols)), shape=(1, rtrain.shape[1]))\n",
    "#Get with user rating\n",
    "userSetUpMatrix_WuserAvg = csr_matrix((dataWuser, (rows, cols)), shape=(1, rtrain.shape[1]))\n",
    "\n",
    "#Generate Implicit user rating vector\n",
    "implicitUserSetUpMtx = csr_matrix((data, (rows, cols)), shape=(1, rtrain.shape[1]))\n",
    "implicitUserSetUpMtx[(userSetUpMatrix > 3).nonzero()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_vappend(a,b):\n",
    "    \"\"\" Takes in 2 csr_matrices and appends the second one to the bottom of the first one. \n",
    "    Much faster than scipy.sparse.vstack but assumes the type to be csr and overwrites\n",
    "    the first matrix instead of copying it. The data, indices, and indptr still get copied.\"\"\"\n",
    "\n",
    "    a.data = np.hstack((a.data,b.data))\n",
    "    a.indices = np.hstack((a.indices,b.indices))\n",
    "    a.indptr = np.hstack((a.indptr,(b.indptr + a.nnz)[1:]))\n",
    "    a._shape = (a.shape[0]+b.shape[0],b.shape[1])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only run the below cell once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6101x3998 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 179729 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This would overwrite rtrain,rtrain_implicit, and rtrain_WuserAvg\n",
    "csr_vappend(rtrain,userSetUpMatrix)\n",
    "csr_vappend(rtrain_implicit, implicitUserSetUpMtx)\n",
    "csr_vappend(rtrain_userAvg, userSetUpMatrix_WuserAvg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#before combine\n",
    "rtrain\n",
    "<6100x3998 sparse matrix of type '<class 'numpy.float32'>'\n",
    "\twith 179720 stored elements in Compressed Sparse Row format>\n",
    "after combine:\n",
    "rtrain\n",
    "<6101x3998 sparse matrix of type '<class 'numpy.float64'>'\n",
    "\twith 179729 stored elements in Compressed Sparse Row format>\n",
    "    \n",
    "(6099, 19)\t5.0\n",
    "(6099, 22)\t3.0\n",
    "(6099, 323)\t3.0\n",
    "(6099, 778)\t4.0\n",
    "(6099, 1144)\t4.0\n",
    "(6099, 1161)\t4.0\n",
    "(6099, 1525)\t4.0\n",
    "(6099, 1782)\t5.0\n",
    "(6099, 2029)\t4.0\n",
    "(6099, 2572)\t4.0\n",
    "(6099, 2702)\t4.0\n",
    "(6099, 2870)\t4.0\n",
    "(6099, 2940)\t3.0\n",
    "(6099, 2987)\t3.0\n",
    "(6099, 3388)\t3.0\n",
    "(6099, 3816)\t5.0\n",
    "\n",
    "(6100, 2612)\t5.0\n",
    "  (6100, 2632)\t5.0\n",
    "  (6100, 2654)\t5.0\n",
    "  (6100, 2656)\t5.0\n",
    "  (6100, 2661)\t5.0\n",
    "  (6100, 2697)\t5.0\n",
    "  (6100, 2939)\t5.0\n",
    "  (6100, 2950)\t5.0\n",
    "  (6100, 2951)\t5.0\n",
    "  \n",
    "Before \n",
    "rtrain_implicit\n",
    "<6100x3998 sparse matrix of type '<class 'numpy.float32'>'\n",
    "\twith 116024 stored elements in Compressed Sparse Row format>\n",
    "<6101x3998 sparse matrix of type '<class 'numpy.float64'>'\n",
    "\twith 116033 stored elements in Compressed Sparse Row format>\n",
    "    \n",
    "rtrain_userAvg\n",
    "<6100x3998 sparse matrix of type '<class 'numpy.float32'>'\n",
    "\twith 179720 stored elements in Compressed Sparse Row format>\n",
    "<6101x3998 sparse matrix of type '<class 'numpy.float64'>'\n",
    "\twith 179729 stored elements in Compressed Sparse Row format>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I am going to move this part to before computing similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6101"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This should be the user index\n",
    "rtrain.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list 1-3 demo for user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_num_id</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>categories</th>\n",
       "      <th>business_stars</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>2069</td>\n",
       "      <td>Pastel Creperies &amp; Dessert House</td>\n",
       "      <td>$$</td>\n",
       "      <td>[{'alias': 'creperies', 'title': 'Creperies'}, {'alias': 'desserts', 'title': 'Desserts'}, {'alias': 'coffee', 'title': 'Coffee &amp; Tea'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18077</th>\n",
       "      <td>1943</td>\n",
       "      <td>Sweet O'clock</td>\n",
       "      <td>$</td>\n",
       "      <td>[{'alias': 'desserts', 'title': 'Desserts'}, {'alias': 'tea', 'title': 'Tea Rooms'}, {'alias': 'icecream', 'title': 'Ice Cream &amp; Frozen Yogurt'}]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       business_num_id                              name price  \\\n",
       "1851   2069             Pastel Creperies & Dessert House  $$     \n",
       "18077  1943             Sweet O'clock                     $      \n",
       "\n",
       "                                                                                                                                              categories  \\\n",
       "1851   [{'alias': 'creperies', 'title': 'Creperies'}, {'alias': 'desserts', 'title': 'Desserts'}, {'alias': 'coffee', 'title': 'Coffee & Tea'}]            \n",
       "18077  [{'alias': 'desserts', 'title': 'Desserts'}, {'alias': 'tea', 'title': 'Tea Rooms'}, {'alias': 'icecream', 'title': 'Ice Cream & Frozen Yogurt'}]   \n",
       "\n",
       "       business_stars  review_count  \n",
       "1851   4.0             316           \n",
       "18077  3.0             86            "
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_location_yonge_and_finch=df_location[df_location[\"business_num_id\"].isin(yonge_and_finch_list)].drop_duplicates(subset = 'business_num_id', keep = 'first')[[\"business_num_id\",\"name\", \"price\", \"categories\",\"business_stars\",\"review_count_y\"]].rename(columns={'review_count_y': 'review_count'})\n",
    "df_location_yonge_and_finch.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_num_id</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>categories</th>\n",
       "      <th>business_stars</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12180</th>\n",
       "      <td>2125</td>\n",
       "      <td>The Burger's Priest</td>\n",
       "      <td>$$</td>\n",
       "      <td>[{'alias': 'burgers', 'title': 'Burgers'}]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29885</th>\n",
       "      <td>1926</td>\n",
       "      <td>Smoke's Poutinerie</td>\n",
       "      <td>$</td>\n",
       "      <td>[{'alias': 'vegetarian', 'title': 'Vegetarian'}, {'alias': 'tradamerican', 'title': 'American (Traditional)'}]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       business_num_id                 name price  \\\n",
       "12180  2125             The Burger's Priest  $$     \n",
       "29885  1926             Smoke's Poutinerie   $      \n",
       "\n",
       "                                                                                                           categories  \\\n",
       "12180  [{'alias': 'burgers', 'title': 'Burgers'}]                                                                       \n",
       "29885  [{'alias': 'vegetarian', 'title': 'Vegetarian'}, {'alias': 'tradamerican', 'title': 'American (Traditional)'}]   \n",
       "\n",
       "       business_stars  review_count  \n",
       "12180  3.0             31            \n",
       "29885  3.0             65            "
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_location_bloor_and_bathurst_list=df_location[df_location[\"business_num_id\"].isin(bloor_and_bathurst_list)].drop_duplicates(subset = 'business_num_id', keep = 'first')[[\"business_num_id\",\"name\",\"price\",\"categories\",\"business_stars\",\"review_count_y\"]].rename(columns={'review_count_y': 'review_count'})\n",
    "df_location_bloor_and_bathurst_list.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_num_id</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>categories</th>\n",
       "      <th>business_stars</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>2468</td>\n",
       "      <td>Mana'ish Global Flatbread Cafe</td>\n",
       "      <td>$$</td>\n",
       "      <td>[{'alias': 'lebanese', 'title': 'Lebanese'}, {'alias': 'cafes', 'title': 'Cafes'}, {'alias': 'mediterranean', 'title': 'Mediterranean'}]</td>\n",
       "      <td>4.5</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8799</th>\n",
       "      <td>2467</td>\n",
       "      <td>The Keg Steakhouse &amp; Bar</td>\n",
       "      <td>$$$</td>\n",
       "      <td>[{'alias': 'steak', 'title': 'Steakhouses'}, {'alias': 'bars', 'title': 'Bars'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      business_num_id                            name price  \\\n",
       "2147  2468             Mana'ish Global Flatbread Cafe  $$     \n",
       "8799  2467             The Keg Steakhouse & Bar        $$$    \n",
       "\n",
       "                                                                                                                                    categories  \\\n",
       "2147  [{'alias': 'lebanese', 'title': 'Lebanese'}, {'alias': 'cafes', 'title': 'Cafes'}, {'alias': 'mediterranean', 'title': 'Mediterranean'}]   \n",
       "8799  [{'alias': 'steak', 'title': 'Steakhouses'}, {'alias': 'bars', 'title': 'Bars'}]                                                           \n",
       "\n",
       "      business_stars  review_count  \n",
       "2147  4.5             33            \n",
       "8799  4.0             95            "
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_location_queen_and_spadina_list=df_location[df_location[\"business_num_id\"].isin(queen_and_spadina_list)].drop_duplicates(subset = 'business_num_id', keep = 'first')[[\"business_num_id\",\"name\",\"price\",\"categories\",\"business_stars\",\"review_count_y\"]].rename(columns={'review_count_y': 'review_count'})\n",
    "df_location_queen_and_spadina_list.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation Section: Produce the list of restaurants close to the set location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setting, don't need to modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dff = df.copy()\n",
    "# dff_popular = df.copy()\n",
    "# dff_popular = dff_popular.sort_values(by=[\"review_count_y\"], ascending=False).drop_duplicates(subset = 'business_id', keep = 'first')\n",
    "# popular_list = dff_popular[\"business_num_id\"].tolist()\n",
    "# dundas_and_yonge = Point(\"43.6561,-79.3802\")\n",
    "# bay_and_queens = Point(\"43.6518,-79.3802\")\n",
    "# king_and_jarvis = Point(\"43.650577,-79.371887\")\n",
    "# bloor_and_yonge = Point(\"43.670409,-79.386814\")\n",
    "# yonge_and_eglinton = Point(\"43.7064,-79.3986\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember to manually change this userIndex!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three locations for user recommendation\n",
    "intersection = [bloor_and_yonge,dundas_and_yonge,spadina_and_dundas]\n",
    "# get 3 items for each location\n",
    "businessIndexRange = 3\n",
    "# 5 metrics and store them in a list\n",
    "metric = [UI_predict_Implicit, popular_list, UI_predict_IECombined, IU_predict_Explicit, Item_predict_tfidf]\n",
    "\n",
    "# manually enter this number!!!!!!\n",
    "userIndex = 0\n",
    "\n",
    "#Need this number to perform the user test, so don't repeat, don't mess up\n",
    "userTestNumber = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the recommendations for each metric using loops, store in the res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 3 top recommendations for each locations\n",
    "# store three locations in one list for each metric\n",
    "\n",
    "#loop through the 5 recommendation algorithms, initialize the res list \n",
    "res = [[] for i in range(len(metric))]\n",
    "#Loop through the intersections \n",
    "for i in range(len(intersection)):\n",
    "    #loop through each metric \n",
    "    for j in range(len(metric)):\n",
    "        temp = geographical_dist(metric[j],intersection[i],userIndex,businessIndexRange)\n",
    "        res[j] += temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1248, 883, 2743, 2156, 2856, 1912, 3672],\n",
       " [1253, 1248, 1552, 1795, 1798, 36, 2934, 1488, 1547],\n",
       " [1248, 883, 2743, 2156, 2856, 2728, 1912],\n",
       " [883, 1162, 2715, 1592, 2728, 3388],\n",
       " [135, 12, 1880, 699, 2728, 2666, 1869]]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 recommendations for each metric\n",
    "# the sequence is -> [UI_predict_Implicit, popular_list, UI_predict_IECombined, IU_predict_Explicit, Item_predict_tfidf]\n",
    "# get three recommendations for each location, so the len for each row is 3*3\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look back the the origal prediction matrix and rearrange the recommendations list, choose the top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_final = []\n",
    "#loop through the elements in the res list (5 recommend lists)\n",
    "for element in range(len(res)):\n",
    "    dic = {}\n",
    "    #loop through each element in the recomemndation list (order:element)\n",
    "    for i in range(len(res[element])):\n",
    "        #if the recommendation is a list \n",
    "        if isinstance(metric[element], list):\n",
    "            dic[metric[element].index(res[element][i])] = res[element][i]\n",
    "        #if the recommendation is a matrix \n",
    "        else:\n",
    "            dic[metric[element][userIndex].tolist().index(res[element][i])] = res[element][i]\n",
    "    temp = []\n",
    "    for j in sorted(dic.keys()):\n",
    "        temp.append(int(dic[j]))\n",
    "    res_final.append(temp[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2856, 883, 1912],\n",
       " [1795, 2934, 1488],\n",
       " [883, 2856, 1248],\n",
       " [883, 1592, 1162],\n",
       " [2728, 135, 2666]]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the final recommendations for each metric\n",
    "res_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder!!!! manually adjust userIndex after inserting a new row of user preference!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce the list of restaurants close to the set location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #First list\n",
    "# UI_predict_Explicit = geographical_dist(UI_predict_Explicit,intersection,userIndex,businessIndexRange)\n",
    "\n",
    "# #Second list\n",
    "# UI_predict_Implicit = geographical_dist(UI_predict_Implicit,intersection,userIndex,businessIndexRange)\n",
    "\n",
    "# #Third list\n",
    "# UI_predict_IECombined = geographical_dist(UI_predict_IECombined,intersection,userIndex,businessIndexRange)\n",
    "\n",
    "# #Fourth lsit\n",
    "# UI_predict_popular = geographical_dist(popular_list,intersection,userIndex,businessIndexRange)\n",
    "\n",
    "# #Five list\n",
    "# UI_predict_max = geographical_dist(UI_predict_max,intersection,userIndex,businessIndexRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2378, 2584, 103, 3482, 1893]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UI_predict_Explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2378, 3035, 1441, 3626, 746]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UI_predict_Implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3482, 2378, 1780, 2921, 1480]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UI_predict_popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2378, 3482, 3649, 3725, 1441]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UI_predict_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df[\"business_num_id\"] == UI_prediction[userIndex][busIndex]].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match restaurant information according to business_num_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Current sequence: UI_predict_Implicit, popular_list, UI_predict_IECombined, IU_predict_Explicit, Item_predict_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the user number that we are trying to recommend for\n",
    "#Enter the # of businesses that we want to recommend for them \n",
    "#Pass in the UI_Prediction matrix for users \n",
    "#userIndex parameter not used \n",
    "def constructResDictionary(userIndex, busIndexRange, UI_prediction):\n",
    "    #Construct the dictionary for the recommended restaurants to display \n",
    "    dictionaryToConstruct = {}\n",
    "    \n",
    "    #Loop through the number of businesses \n",
    "    for busIndex in range(busIndexRange):\n",
    "        #Get the business information for the recommended business \n",
    "        businessSeries = df[df[\"business_num_id\"] == UI_prediction[busIndex]].iloc[0]\n",
    "        #Get the business name \n",
    "        busName = businessSeries['name']\n",
    "        \n",
    "        #get the list of strings to generate the address information \n",
    "        address_generator = (str(w) for w in yaml.safe_load(businessSeries.location)['display_address'])\n",
    "        busLocation = ', '.join(address_generator)\n",
    "        bus_Price = businessSeries.price\n",
    "        busStars = businessSeries.business_stars\n",
    "        busReviewCount = businessSeries.review_count_y \n",
    "        category_generator = (str(s) for s in [cateDict['title'] for cateDict in yaml.safe_load(businessSeries.categories)])\n",
    "        #busCategories = [cateDict['title'] for cateDict in yaml.safe_load(businessSeries.categories)]\n",
    "        busCategories = ', '.join(category_generator)\n",
    "        #Now add the restaurant to the dictionary\n",
    "        dictionaryToConstruct[busName] = {'Address': busLocation,\\\n",
    "                                'Price': bus_Price,\\\n",
    "                                 'Star': busStars, \\\n",
    "                                 'Review Count': busReviewCount, \\\n",
    "                                 'Category': busCategories}\n",
    "    return dictionaryToConstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to recommend for user 0 for now \n",
    "\n",
    "\n",
    "dict1_ExplicitRecommend = {}\n",
    "dict2_ImplicitRecommend = {}\n",
    "dict3_EICombineRecommend = {}\n",
    "dict4_PopularityRecommend = {}\n",
    "dict5_maxRecommend = {}\n",
    "#User predict [user index] [item index]\n",
    "#Add a for loop for the top recommended items\n",
    "\n",
    "dict1_ExplicitRecommend = constructResDictionary(userIndex, businessIndexRange, res_final[0])\n",
    "dict2_ImplicitRecommend = constructResDictionary(userIndex, businessIndexRange, res_final[1])\n",
    "dict3_EICombineRecommend = constructResDictionary(userIndex, businessIndexRange, res_final[2])\n",
    "dict4_PopularityRecommend = constructResDictionary(userIndex, businessIndexRange, res_final[3])\n",
    "dict5_maxRecommend = constructResDictionary(userIndex, businessIndexRange, res_final[4])\n",
    "\n",
    "#Say we are recommending 5 restaurants for now\n",
    "# for busIndex in range(5):\n",
    "#     businessSeries = df[df[\"business_num_id\"] == UI_predict_Explicit[0][busIndex]].iloc[0]\n",
    "#     busName = businessSeries['name']\n",
    "#     address_generator = (str(w) for w in yaml.safe_load(businessSeries.location)['display_address'])\n",
    "#     busLocation = ', '.join(address_generator)\n",
    "#     bus_Price = businessSeries.price\n",
    "#     busStars = businessSeries.business_stars\n",
    "#     busReviewCount = businessSeries.review_count_y \n",
    "#     category_generator = (str(s) for s in [cateDict['title'] for cateDict in yaml.safe_load(businessSeries.categories)])\n",
    "#     #busCategories = [cateDict['title'] for cateDict in yaml.safe_load(businessSeries.categories)]\n",
    "#     busCategories = ', '.join(category_generator)\n",
    "#     #Now add the restaurant to the dictionary\n",
    "#     dict1stAlgo[busName] = {'Address': busLocation,\\\n",
    "#                             'Price': bus_Price,\\\n",
    "#                              'Star': busStars, \\\n",
    "#                              'Review Count': busReviewCount, \\\n",
    "#                              'Category': busCategories}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilist box with entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiListbox_entries(Frame):\n",
    "    \n",
    "    def __init__(self, master, lists):\n",
    "        Frame.__init__(self, master)\n",
    "        self.lists = []\n",
    "        self.entries = []\n",
    "        self.fields = 'Like', 'Dislike'\n",
    "        #Var for checkbox\n",
    "        #self.var = IntVar()\n",
    "        self.var = []\n",
    "        self.listNames = []\n",
    "        self.responseDict = {}\n",
    "        \n",
    "        #Loop through the lists, l is the list label and widthW is the width \n",
    "        for l,widthW in lists:\n",
    "            \n",
    "            frame = Frame(self); frame.pack(side=LEFT, expand=YES, fill=BOTH)\n",
    "            \n",
    "            Label(frame, text=l, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "            \n",
    "            #store the list names \n",
    "            self.listNames.append(l)\n",
    "            \n",
    "            lb = Listbox(frame, width=widthW, borderwidth=0, selectborderwidth=0,\n",
    "                 relief=FLAT, exportselection=FALSE)\n",
    "            lb.pack(expand=YES, fill=BOTH)\n",
    "            \n",
    "            self.lists.append(lb)\n",
    "            \n",
    "            lb.bind('<B1-Motion>', lambda e, s=self: s._select(e.y))\n",
    "            lb.bind('<Button-1>', lambda e, s=self: s._select(e.y))\n",
    "            \n",
    "            lb.bind('<Leave>', lambda e: 'break')\n",
    "            \n",
    "            lb.bind('<B2-Motion>', lambda e, s=self: s._b2motion(e.x, e.y))\n",
    "            lb.bind('<Button-2>', lambda e, s=self: s._button2(e.x, e.y))\n",
    "             \n",
    "             #loop through the fields to \n",
    "            for field in fields:\n",
    "                #row = Frame(self)\n",
    "                lab = Label(frame, width=15, text=field, anchor='w')\n",
    "                ent = Entry(frame)\n",
    "                #row.pack(side=BOTTOM, fill=Y, padx=5, pady=5) #=BOTTOM\n",
    "                lab.pack()\n",
    "                #ent.pack(side=RIGHT, expand=YES, fill=Y)\n",
    "                ent.pack(side=TOP, fill=X)\n",
    "                self.entries.append((field, ent))\n",
    "            \n",
    "            localVar = IntVar()\n",
    "            \n",
    "            self.var.append(localVar)\n",
    "            \n",
    "            #Checkbox \n",
    "            #c = Checkbutton(frame, text=\"Liked\", variable=localVar, command=self.cb(count))\n",
    "            c = Checkbutton(frame, text=\"Liked\", variable=localVar)\n",
    "            c.pack()\n",
    "            \n",
    "        print(self.var)\n",
    "        #Set the submit button \n",
    "        #master.bind('<Button-1>', (lambda event, e=self.entries: fetch(e)))   \n",
    "        #b1 = Button(master, text='Submit', command=(lambda e=self.entries: self.fetch(e)))\n",
    "        b1 = Button(master, text='Submit', command=self.fetchDict)\n",
    "        b1.pack(side=BOTTOM, padx=5, pady=5)\n",
    "        #b2 = Button(root, text='Quit', command=root.quit)\n",
    "        #b2.pack(side=LEFT, padx=5, pady=5)\n",
    "            \n",
    "        #pakcing the frame\n",
    "        frame = Frame(self); frame.pack(side=LEFT, fill=Y)\n",
    "        \n",
    "        #packing the label\n",
    "        Label(frame, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "        #Setting a scrollbar \n",
    "#         sb = Scrollbar(frame, orient=VERTICAL, command=self._scroll)\n",
    "#         sb.pack(expand=YES, fill=Y)\n",
    "#         self.lists[0]['yscrollcommand']=sb.set\n",
    "\n",
    "    def _select(self, y):\n",
    "        row = self.lists[0].nearest(y)\n",
    "        self.selection_clear(0, END)\n",
    "        self.selection_set(row)\n",
    "        return 'break'\n",
    "    \n",
    "    #For checkbox \n",
    "    def cb(self, index):\n",
    "        print (\"variable is\", self.var[index].get())\n",
    "\n",
    "    def _button2(self, x, y):\n",
    "        for l in self.lists: l.scan_mark(x, y)\n",
    "        return 'break'\n",
    "\n",
    "    def _b2motion(self, x, y):\n",
    "        for l in self.lists: l.scan_dragto(x, y)\n",
    "        return 'break'\n",
    "\n",
    "    def _scroll(self, *args):\n",
    "        for l in self.lists:\n",
    "            apply(l.yview, args)\n",
    "\n",
    "    def curselection(self):\n",
    "        return self.lists[0].curselection()\n",
    "\n",
    "    def delete(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.delete(first, last)\n",
    "\n",
    "#     def get(self, first, last=None):\n",
    "#         result = []\n",
    "#         for l in self.lists:\n",
    "#             result.append(l.get(first,last))\n",
    "#         if last: return apply(map, [None] + result)\n",
    "#         return result\n",
    "\n",
    "    def index(self, index):\n",
    "        self.lists[0].index(index)\n",
    "\n",
    "    def insert(self, index, *elements):\n",
    "        #Loop through the elements \n",
    "        for element in elements:\n",
    "            i = 0\n",
    "            for l in self.lists:\n",
    "                l.insert(index, element[i])\n",
    "                i = i + 1\n",
    "\n",
    "    def size(self):\n",
    "        return self.lists[0].size()\n",
    "\n",
    "    def see(self, index):\n",
    "        for l in self.lists:\n",
    "            l.see(index)\n",
    "\n",
    "    def selection_anchor(self, index):\n",
    "        for l in self.lists:\n",
    "            l.selection_anchor(index)\n",
    "\n",
    "    def selection_clear(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.selection_clear(first, last)\n",
    "\n",
    "    def selection_includes(self, index):\n",
    "        return self.lists[0].selection_includes(index)\n",
    "\n",
    "    def selection_set(self, first, last=None):\n",
    "        for l in self.lists:\n",
    "            l.selection_set(first, last)\n",
    "            \n",
    "    def fetch(self,entries):\n",
    "        count = 1\n",
    "        \n",
    "        for entry in entries:\n",
    "            if count <= 2:\n",
    "                listCount = 1\n",
    "            elif count <= 4:\n",
    "                listCount = 2\n",
    "            elif count <= 6:\n",
    "                listCount = 3\n",
    "            elif count <= 8:\n",
    "                listCount = 4\n",
    "            elif count <= 10:\n",
    "                listCount = 5             \n",
    "            #print on odd number counts\n",
    "            if count %2 != 0:\n",
    "                print('list:', listCount)\n",
    "            #index 0 is the field \n",
    "            #index 1 is the entry data \n",
    "            field = entry[0]\n",
    "            text  = entry[1].get()\n",
    "            print('%s: \"%s\"' % (field, text))\n",
    "            \n",
    "            count =count + 1\n",
    "            \n",
    "        print(list(map((lambda var: var.get()), self.var)))\n",
    "   \n",
    "    #This function returns a dictionary of the results\n",
    "    def fetchDict(self):\n",
    "        responseDict = {}\n",
    "        count = 1\n",
    "        \n",
    "        likeResult = list(map((lambda var: var.get()), self.var))\n",
    "        \n",
    "        for entry in self.entries:\n",
    "            tempDict = {}\n",
    "            \n",
    "            field = entry[0]\n",
    "            text  = entry[1].get()\n",
    "            # like:\"text content\"\n",
    "            tempDict[field] = text\n",
    "            \n",
    "            if count <= 2:\n",
    "                listCount = 1\n",
    "            elif count <= 4:\n",
    "                listCount = 2\n",
    "            elif count <= 6:\n",
    "                listCount = 3\n",
    "            elif count <= 8:\n",
    "                listCount = 4\n",
    "            elif count <= 10:\n",
    "                listCount = 5      \n",
    "                \n",
    "            #update the dictionary with corresponding listname :  \n",
    "            try:\n",
    "                responseDict[self.listNames[listCount-1]].update(tempDict) \n",
    "            except:\n",
    "                responseDict[self.listNames[listCount-1]] = tempDict \n",
    "            \n",
    "            if count %2 != 0:\n",
    "                #print on odd number counts\n",
    "                print('list:', listCount)\n",
    "                #Get the hit indicator and update the dictionary \n",
    "                tempLikeDict = {'hit': likeResult[listCount-1]}\n",
    "                responseDict[self.listNames[listCount-1]].update(tempLikeDict)\n",
    "            \n",
    "            #Print the current the field and entry text\n",
    "            print('%s: \"%s\"' % (field, text))\n",
    "           \n",
    "            count =count + 1\n",
    "        \n",
    "        #print the hit list\n",
    "        print(list(map((lambda var: var.get()), self.var)))\n",
    "\n",
    "        self.responseDict = responseDict.copy()\n",
    "        \n",
    "        \n",
    "    #Make the entry form \n",
    "    def makeform(root, fields):\n",
    "        #For each field, like \n",
    "        for field in fields:\n",
    "            row = Frame(master)\n",
    "            lab = Label(row, width=15, text=field, anchor='w')\n",
    "            ent = Entry(row)\n",
    "            row.pack(side=TOP, fill=X, padx=5, pady=5)\n",
    "            lab.pack(side=LEFT)\n",
    "            ent.pack(side=RIGHT, expand=YES, fill=X)\n",
    "            self.entries.append((field, ent))\n",
    "            \n",
    "        self.entries =  entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence for now: UI_predict_Implicit, popular_list, UI_predict_IECombined, IU_predict_Explicit, Item_predict_tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tkinter.IntVar object at 0x00000147670A2438>, <tkinter.IntVar object at 0x0000014767A67860>, <tkinter.IntVar object at 0x000001477FCC2D68>, <tkinter.IntVar object at 0x00000147693CB8D0>, <tkinter.IntVar object at 0x0000014768B26B38>]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #Initiate a frame or master? \n",
    "    tk = Tk()\n",
    "    Label(tk, text='List of recommended restaurants').pack()\n",
    "    \n",
    "    #Creating the multi-listbox object, pass in a tuple of tuple (lists), this will be the list objects\n",
    "    mlb = MultiListbox_entries(tk, (('Implicit User-Rating', 20), ('Popularity List', 20), \\\n",
    "                            ('Explicit Implicit Combined', 20), ('Explicit Item-Rating', 20), ('TF-IDF Score of Item', 20)))\n",
    "\n",
    "    #loop through the length of recommend list, # = 5 \n",
    "    for index in range(len(dict1_ExplicitRecommend)):\n",
    "        #First restaurant information to display \n",
    "        #Following the restaurants' name \n",
    "        restList1 = list(dict1_ExplicitRecommend.keys())[index]\n",
    "        restList2 = list(dict2_ImplicitRecommend.keys())[index]\n",
    "        restList3 = list(dict3_EICombineRecommend.keys())[index]\n",
    "        restList4 = list(dict4_PopularityRecommend.keys())[index]\n",
    "        restList5 = list(dict5_maxRecommend.keys())[index]\n",
    "        \n",
    "        mlb.insert(END, (' ', ' ', ' ', ' ', ' '))\n",
    "        \n",
    "        #Inserting the restaurant names\n",
    "        mlb.insert(END, ('%d: %s' % (index + 1, restList1),'%d: %s' % (index + 1, restList2),\n",
    "                         '%d: %s' % (index + 1, restList3),'%d: %s' % (index + 1, restList4),\n",
    "                         '%d: %s' % (index + 1, restList5)))\n",
    "        \n",
    "        #Looping through each attribute keys - resinfo\n",
    "        for resinfo in dict1_ExplicitRecommend.get(restList1).keys():\n",
    "            restList1Info = resinfo + ':' + str(dict1_ExplicitRecommend.get(restList1).get(resinfo,''))\n",
    "            restList2Info = resinfo + ':' + str(dict2_ImplicitRecommend.get(restList2).get(resinfo,''))\n",
    "            restList3Info = resinfo + ':' + str(dict3_EICombineRecommend.get(restList3).get(resinfo,''))\n",
    "            restList4Info = resinfo + ':' + str(dict4_PopularityRecommend.get(restList4).get(resinfo,''))\n",
    "            restList5Info = resinfo + ':' + str(dict5_maxRecommend.get(restList5).get(resinfo,''))\n",
    "            \n",
    "            mlb.insert(END, (restList1Info, restList2Info, restList3Info, restList4Info, restList5Info))\n",
    "        \n",
    "        mlb.insert(END, ('----------------', '----------------', '----------------', '----------------', '----------------'))\n",
    "    \n",
    "    mlb.pack(expand=YES,fill=BOTH)\n",
    "\n",
    "    tk.mainloop()\n",
    "    \n",
    "    #Get user response\n",
    "    response = mlb.responseDict\n",
    "    \n",
    "    #store the user response into local file\n",
    "    csv_fileName = \"UserTestResult{:d}.json\".format(userTestNumber)\n",
    "    with open('userStudyResults//'+csv_fileName, 'w') as fp:\n",
    "        json.dump(response, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I get the user response information here, so please make sure the sequence for list names are correct when initializing our mlb object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_fileName = \"UserTestResult{:d}.json\".format(userTestNumber)\n",
    "# with open(csv_fileName, 'w') as fp:\n",
    "#     json.dump(response, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load the result file with json \n",
    "# with open(csv_fileName, 'r') as fp:\n",
    "#     test = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  38,  155,  290,  310,  333,  384,  435,  440,  768,  882,  916,\n",
       "       1089, 1102, 1212, 1218, 1488, 1529, 1710, 1772, 1953, 2012, 2192,\n",
       "       2343, 2518, 2622, 2760, 2873, 2934, 2945, 3031, 3059, 3083, 3199,\n",
       "       3309, 3376, 3412, 3524, 3540, 3791, 3866])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the 0th user review data \n",
    "rows, cols = rtrain[0].nonzero()\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_num_id</th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>business_stars</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8263</th>\n",
       "      <td>2945</td>\n",
       "      <td>Mayrik</td>\n",
       "      <td>[{'alias': 'mediterranean', 'title': 'Mediterranean'}, {'alias': 'mideastern', 'title': 'Middle Eastern'}]</td>\n",
       "      <td>4.5</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9368</th>\n",
       "      <td>1488</td>\n",
       "      <td>Banh Mi Boys</td>\n",
       "      <td>[{'alias': 'sandwiches', 'title': 'Sandwiches'}]</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11498</th>\n",
       "      <td>2012</td>\n",
       "      <td>Pray Tell</td>\n",
       "      <td>[{'alias': 'cocktailbars', 'title': 'Cocktail Bars'}, {'alias': 'tapasmallplates', 'title': 'Tapas/Small Plates'}, {'alias': 'breakfast_brunch', 'title': 'Breakfast &amp; Brunch'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11674</th>\n",
       "      <td>333</td>\n",
       "      <td>El Rey Mezcal Bar</td>\n",
       "      <td>[{'alias': 'cocktailbars', 'title': 'Cocktail Bars'}, {'alias': 'gastropubs', 'title': 'Gastropubs'}, {'alias': 'mexican', 'title': 'Mexican'}]</td>\n",
       "      <td>3.5</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12569</th>\n",
       "      <td>2873</td>\n",
       "      <td>Odin</td>\n",
       "      <td>[{'alias': 'cafes', 'title': 'Cafes'}, {'alias': 'bars', 'title': 'Bars'}]</td>\n",
       "      <td>3.5</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15311</th>\n",
       "      <td>1529</td>\n",
       "      <td>Si Lom Thai Bistro</td>\n",
       "      <td>[{'alias': 'thai', 'title': 'Thai'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16684</th>\n",
       "      <td>768</td>\n",
       "      <td>Mother's Dumplings</td>\n",
       "      <td>[{'alias': 'chinese', 'title': 'Chinese'}]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18288</th>\n",
       "      <td>2934</td>\n",
       "      <td>Seven Lives Tacos Y Mariscos</td>\n",
       "      <td>[{'alias': 'mexican', 'title': 'Mexican'}]</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23246</th>\n",
       "      <td>3412</td>\n",
       "      <td>Bar Isabel</td>\n",
       "      <td>[{'alias': 'spanish', 'title': 'Spanish'}, {'alias': 'tapas', 'title': 'Tapas Bars'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32766</th>\n",
       "      <td>3524</td>\n",
       "      <td>Constantine</td>\n",
       "      <td>[{'alias': 'mediterranean', 'title': 'Mediterranean'}, {'alias': 'mideastern', 'title': 'Middle Eastern'}, {'alias': 'italian', 'title': 'Italian'}]</td>\n",
       "      <td>3.5</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38630</th>\n",
       "      <td>440</td>\n",
       "      <td>Tabl Middle Eastern Cuisine</td>\n",
       "      <td>[{'alias': 'mideastern', 'title': 'Middle Eastern'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42198</th>\n",
       "      <td>1772</td>\n",
       "      <td>Riverdale Perk Cafe</td>\n",
       "      <td>[{'alias': 'cafes', 'title': 'Cafes'}, {'alias': 'sandwiches', 'title': 'Sandwiches'}, {'alias': 'breakfast_brunch', 'title': 'Breakfast &amp; Brunch'}]</td>\n",
       "      <td>3.5</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43499</th>\n",
       "      <td>2518</td>\n",
       "      <td>The Hearth</td>\n",
       "      <td>[{'alias': 'tradamerican', 'title': 'American (Traditional)'}, {'alias': 'newcanadian', 'title': 'Canadian (New)'}, {'alias': 'breakfast_brunch', 'title': 'Breakfast &amp; Brunch'}]</td>\n",
       "      <td>2.5</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45274</th>\n",
       "      <td>3199</td>\n",
       "      <td>Francesca Italian Bakery &amp; Delicatessen</td>\n",
       "      <td>[{'alias': 'coffee', 'title': 'Coffee &amp; Tea'}, {'alias': 'bakeries', 'title': 'Bakeries'}, {'alias': 'italian', 'title': 'Italian'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47285</th>\n",
       "      <td>2343</td>\n",
       "      <td>Le Slect Bistro</td>\n",
       "      <td>[{'alias': 'french', 'title': 'French'}, {'alias': 'coffee', 'title': 'Coffee &amp; Tea'}, {'alias': 'breakfast_brunch', 'title': 'Breakfast &amp; Brunch'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61088</th>\n",
       "      <td>2622</td>\n",
       "      <td>Simone's Caribbean Restaurant</td>\n",
       "      <td>[{'alias': 'caribbean', 'title': 'Caribbean'}]</td>\n",
       "      <td>4.5</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61304</th>\n",
       "      <td>3083</td>\n",
       "      <td>Local Leaside</td>\n",
       "      <td>[{'alias': 'tradamerican', 'title': 'American (Traditional)'}]</td>\n",
       "      <td>3.5</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67064</th>\n",
       "      <td>3791</td>\n",
       "      <td>Sweet Trolley Bakery</td>\n",
       "      <td>[{'alias': 'bakeries', 'title': 'Bakeries'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75008</th>\n",
       "      <td>384</td>\n",
       "      <td>Buna's Kitchen</td>\n",
       "      <td>[{'alias': 'sandwiches', 'title': 'Sandwiches'}, {'alias': 'salad', 'title': 'Salad'}]</td>\n",
       "      <td>4.5</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88452</th>\n",
       "      <td>1212</td>\n",
       "      <td>Otto's Berlin Dner</td>\n",
       "      <td>[{'alias': 'german', 'title': 'German'}, {'alias': 'bars', 'title': 'Bars'}, {'alias': 'sandwiches', 'title': 'Sandwiches'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95232</th>\n",
       "      <td>3309</td>\n",
       "      <td>Campagnolo</td>\n",
       "      <td>[{'alias': 'tapasmallplates', 'title': 'Tapas/Small Plates'}, {'alias': 'italian', 'title': 'Italian'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95839</th>\n",
       "      <td>3031</td>\n",
       "      <td>Grk Ygrt</td>\n",
       "      <td>[{'alias': 'icecream', 'title': 'Ice Cream &amp; Frozen Yogurt'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98919</th>\n",
       "      <td>310</td>\n",
       "      <td>Bar Raval</td>\n",
       "      <td>[{'alias': 'spanish', 'title': 'Spanish'}, {'alias': 'tapasmallplates', 'title': 'Tapas/Small Plates'}, {'alias': 'cocktailbars', 'title': 'Cocktail Bars'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101198</th>\n",
       "      <td>3059</td>\n",
       "      <td>Mildred's Temple Kitchen</td>\n",
       "      <td>[{'alias': 'breakfast_brunch', 'title': 'Breakfast &amp; Brunch'}, {'alias': 'newcanadian', 'title': 'Canadian (New)'}, {'alias': 'vegetarian', 'title': 'Vegetarian'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102493</th>\n",
       "      <td>2760</td>\n",
       "      <td>Kekou Gelato</td>\n",
       "      <td>[{'alias': 'icecream', 'title': 'Ice Cream &amp; Frozen Yogurt'}, {'alias': 'desserts', 'title': 'Desserts'}]</td>\n",
       "      <td>4.5</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110652</th>\n",
       "      <td>916</td>\n",
       "      <td>Delight</td>\n",
       "      <td>[{'alias': 'desserts', 'title': 'Desserts'}, {'alias': 'coffee', 'title': 'Coffee &amp; Tea'}, {'alias': 'chocolate', 'title': 'Chocolatiers &amp; Shops'}, {'alias': 'icecream', 'title': 'Ice Cream &amp; Frozen Yogurt'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111946</th>\n",
       "      <td>3866</td>\n",
       "      <td>College Falafel</td>\n",
       "      <td>[{'alias': 'mideastern', 'title': 'Middle Eastern'}, {'alias': 'mediterranean', 'title': 'Mediterranean'}, {'alias': 'sandwiches', 'title': 'Sandwiches'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117599</th>\n",
       "      <td>3540</td>\n",
       "      <td>Souk Tabule</td>\n",
       "      <td>[{'alias': 'mideastern', 'title': 'Middle Eastern'}, {'alias': 'salad', 'title': 'Salad'}, {'alias': 'soup', 'title': 'Soup'}]</td>\n",
       "      <td>4.5</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118364</th>\n",
       "      <td>1953</td>\n",
       "      <td>Hm Cafe</td>\n",
       "      <td>[{'alias': 'cafes', 'title': 'Cafes'}, {'alias': 'breakfast_brunch', 'title': 'Breakfast &amp; Brunch'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118848</th>\n",
       "      <td>1218</td>\n",
       "      <td>The Poet Cafe</td>\n",
       "      <td>[{'alias': 'cafes', 'title': 'Cafes'}, {'alias': 'tapasmallplates', 'title': 'Tapas/Small Plates'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119430</th>\n",
       "      <td>882</td>\n",
       "      <td>Pero Restaurant &amp; Lounge</td>\n",
       "      <td>[{'alias': 'african', 'title': 'African'}, {'alias': 'vegan', 'title': 'Vegan'}, {'alias': 'vegetarian', 'title': 'Vegetarian'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119454</th>\n",
       "      <td>3376</td>\n",
       "      <td>Blaze Fast-Fire'd Pizza</td>\n",
       "      <td>[{'alias': 'pizza', 'title': 'Pizza'}, {'alias': 'salad', 'title': 'Salad'}, {'alias': 'hotdogs', 'title': 'Fast Food'}]</td>\n",
       "      <td>4.5</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137507</th>\n",
       "      <td>38</td>\n",
       "      <td>The Works Gourmet Burger Bistro</td>\n",
       "      <td>[{'alias': 'burgers', 'title': 'Burgers'}]</td>\n",
       "      <td>3.5</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139583</th>\n",
       "      <td>2192</td>\n",
       "      <td>Lao Thai Restaurant</td>\n",
       "      <td>[{'alias': 'thai', 'title': 'Thai'}, {'alias': 'laotian', 'title': 'Laotian'}]</td>\n",
       "      <td>4.5</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144305</th>\n",
       "      <td>435</td>\n",
       "      <td>Asian Legend</td>\n",
       "      <td>[{'alias': 'dimsum', 'title': 'Dim Sum'}]</td>\n",
       "      <td>3.5</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153504</th>\n",
       "      <td>1710</td>\n",
       "      <td>Yummy Yummy Dumplings</td>\n",
       "      <td>[{'alias': 'chinese', 'title': 'Chinese'}, {'alias': 'noodles', 'title': 'Noodles'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155220</th>\n",
       "      <td>1102</td>\n",
       "      <td>Graceful Vegetarian Restaurant</td>\n",
       "      <td>[{'alias': 'vegetarian', 'title': 'Vegetarian'}, {'alias': 'cantonese', 'title': 'Cantonese'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161118</th>\n",
       "      <td>1089</td>\n",
       "      <td>La Cubana</td>\n",
       "      <td>[{'alias': 'cuban', 'title': 'Cuban'}, {'alias': 'desserts', 'title': 'Desserts'}, {'alias': 'sandwiches', 'title': 'Sandwiches'}]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172541</th>\n",
       "      <td>290</td>\n",
       "      <td>Tracy Dessert</td>\n",
       "      <td>[{'alias': 'desserts', 'title': 'Desserts'}]</td>\n",
       "      <td>3.5</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176165</th>\n",
       "      <td>155</td>\n",
       "      <td>El Charro</td>\n",
       "      <td>[{'alias': 'mexican', 'title': 'Mexican'}]</td>\n",
       "      <td>4.5</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        business_num_id                                     name  \\\n",
       "8263    2945             Mayrik                                    \n",
       "9368    1488             Banh Mi Boys                              \n",
       "11498   2012             Pray Tell                                 \n",
       "11674   333              El Rey Mezcal Bar                         \n",
       "12569   2873             Odin                                      \n",
       "15311   1529             Si Lom Thai Bistro                        \n",
       "16684   768              Mother's Dumplings                        \n",
       "18288   2934             Seven Lives Tacos Y Mariscos              \n",
       "23246   3412             Bar Isabel                                \n",
       "32766   3524             Constantine                               \n",
       "38630   440              Tabl Middle Eastern Cuisine             \n",
       "42198   1772             Riverdale Perk Cafe                       \n",
       "43499   2518             The Hearth                                \n",
       "45274   3199             Francesca Italian Bakery & Delicatessen   \n",
       "47285   2343             Le Slect Bistro                          \n",
       "61088   2622             Simone's Caribbean Restaurant             \n",
       "61304   3083             Local Leaside                             \n",
       "67064   3791             Sweet Trolley Bakery                      \n",
       "75008   384              Buna's Kitchen                            \n",
       "88452   1212             Otto's Berlin Dner                       \n",
       "95232   3309             Campagnolo                                \n",
       "95839   3031             Grk Ygrt                                  \n",
       "98919   310              Bar Raval                                 \n",
       "101198  3059             Mildred's Temple Kitchen                  \n",
       "102493  2760             Kekou Gelato                              \n",
       "110652  916              Delight                                   \n",
       "111946  3866             College Falafel                           \n",
       "117599  3540             Souk Tabule                               \n",
       "118364  1953             Hm Cafe                                  \n",
       "118848  1218             The Poet Cafe                             \n",
       "119430  882              Pero Restaurant & Lounge                  \n",
       "119454  3376             Blaze Fast-Fire'd Pizza                   \n",
       "137507  38               The Works Gourmet Burger Bistro           \n",
       "139583  2192             Lao Thai Restaurant                       \n",
       "144305  435              Asian Legend                              \n",
       "153504  1710             Yummy Yummy Dumplings                     \n",
       "155220  1102             Graceful Vegetarian Restaurant            \n",
       "161118  1089             La Cubana                                 \n",
       "172541  290              Tracy Dessert                             \n",
       "176165  155              El Charro                                 \n",
       "\n",
       "                                                                                                                                                                                                              categories  \\\n",
       "8263    [{'alias': 'mediterranean', 'title': 'Mediterranean'}, {'alias': 'mideastern', 'title': 'Middle Eastern'}]                                                                                                         \n",
       "9368    [{'alias': 'sandwiches', 'title': 'Sandwiches'}]                                                                                                                                                                   \n",
       "11498   [{'alias': 'cocktailbars', 'title': 'Cocktail Bars'}, {'alias': 'tapasmallplates', 'title': 'Tapas/Small Plates'}, {'alias': 'breakfast_brunch', 'title': 'Breakfast & Brunch'}]                                   \n",
       "11674   [{'alias': 'cocktailbars', 'title': 'Cocktail Bars'}, {'alias': 'gastropubs', 'title': 'Gastropubs'}, {'alias': 'mexican', 'title': 'Mexican'}]                                                                    \n",
       "12569   [{'alias': 'cafes', 'title': 'Cafes'}, {'alias': 'bars', 'title': 'Bars'}]                                                                                                                                         \n",
       "15311   [{'alias': 'thai', 'title': 'Thai'}]                                                                                                                                                                               \n",
       "16684   [{'alias': 'chinese', 'title': 'Chinese'}]                                                                                                                                                                         \n",
       "18288   [{'alias': 'mexican', 'title': 'Mexican'}]                                                                                                                                                                         \n",
       "23246   [{'alias': 'spanish', 'title': 'Spanish'}, {'alias': 'tapas', 'title': 'Tapas Bars'}]                                                                                                                              \n",
       "32766   [{'alias': 'mediterranean', 'title': 'Mediterranean'}, {'alias': 'mideastern', 'title': 'Middle Eastern'}, {'alias': 'italian', 'title': 'Italian'}]                                                               \n",
       "38630   [{'alias': 'mideastern', 'title': 'Middle Eastern'}]                                                                                                                                                               \n",
       "42198   [{'alias': 'cafes', 'title': 'Cafes'}, {'alias': 'sandwiches', 'title': 'Sandwiches'}, {'alias': 'breakfast_brunch', 'title': 'Breakfast & Brunch'}]                                                               \n",
       "43499   [{'alias': 'tradamerican', 'title': 'American (Traditional)'}, {'alias': 'newcanadian', 'title': 'Canadian (New)'}, {'alias': 'breakfast_brunch', 'title': 'Breakfast & Brunch'}]                                  \n",
       "45274   [{'alias': 'coffee', 'title': 'Coffee & Tea'}, {'alias': 'bakeries', 'title': 'Bakeries'}, {'alias': 'italian', 'title': 'Italian'}]                                                                               \n",
       "47285   [{'alias': 'french', 'title': 'French'}, {'alias': 'coffee', 'title': 'Coffee & Tea'}, {'alias': 'breakfast_brunch', 'title': 'Breakfast & Brunch'}]                                                               \n",
       "61088   [{'alias': 'caribbean', 'title': 'Caribbean'}]                                                                                                                                                                     \n",
       "61304   [{'alias': 'tradamerican', 'title': 'American (Traditional)'}]                                                                                                                                                     \n",
       "67064   [{'alias': 'bakeries', 'title': 'Bakeries'}]                                                                                                                                                                       \n",
       "75008   [{'alias': 'sandwiches', 'title': 'Sandwiches'}, {'alias': 'salad', 'title': 'Salad'}]                                                                                                                             \n",
       "88452   [{'alias': 'german', 'title': 'German'}, {'alias': 'bars', 'title': 'Bars'}, {'alias': 'sandwiches', 'title': 'Sandwiches'}]                                                                                       \n",
       "95232   [{'alias': 'tapasmallplates', 'title': 'Tapas/Small Plates'}, {'alias': 'italian', 'title': 'Italian'}]                                                                                                            \n",
       "95839   [{'alias': 'icecream', 'title': 'Ice Cream & Frozen Yogurt'}]                                                                                                                                                      \n",
       "98919   [{'alias': 'spanish', 'title': 'Spanish'}, {'alias': 'tapasmallplates', 'title': 'Tapas/Small Plates'}, {'alias': 'cocktailbars', 'title': 'Cocktail Bars'}]                                                       \n",
       "101198  [{'alias': 'breakfast_brunch', 'title': 'Breakfast & Brunch'}, {'alias': 'newcanadian', 'title': 'Canadian (New)'}, {'alias': 'vegetarian', 'title': 'Vegetarian'}]                                                \n",
       "102493  [{'alias': 'icecream', 'title': 'Ice Cream & Frozen Yogurt'}, {'alias': 'desserts', 'title': 'Desserts'}]                                                                                                          \n",
       "110652  [{'alias': 'desserts', 'title': 'Desserts'}, {'alias': 'coffee', 'title': 'Coffee & Tea'}, {'alias': 'chocolate', 'title': 'Chocolatiers & Shops'}, {'alias': 'icecream', 'title': 'Ice Cream & Frozen Yogurt'}]   \n",
       "111946  [{'alias': 'mideastern', 'title': 'Middle Eastern'}, {'alias': 'mediterranean', 'title': 'Mediterranean'}, {'alias': 'sandwiches', 'title': 'Sandwiches'}]                                                         \n",
       "117599  [{'alias': 'mideastern', 'title': 'Middle Eastern'}, {'alias': 'salad', 'title': 'Salad'}, {'alias': 'soup', 'title': 'Soup'}]                                                                                     \n",
       "118364  [{'alias': 'cafes', 'title': 'Cafes'}, {'alias': 'breakfast_brunch', 'title': 'Breakfast & Brunch'}]                                                                                                               \n",
       "118848  [{'alias': 'cafes', 'title': 'Cafes'}, {'alias': 'tapasmallplates', 'title': 'Tapas/Small Plates'}]                                                                                                                \n",
       "119430  [{'alias': 'african', 'title': 'African'}, {'alias': 'vegan', 'title': 'Vegan'}, {'alias': 'vegetarian', 'title': 'Vegetarian'}]                                                                                   \n",
       "119454  [{'alias': 'pizza', 'title': 'Pizza'}, {'alias': 'salad', 'title': 'Salad'}, {'alias': 'hotdogs', 'title': 'Fast Food'}]                                                                                           \n",
       "137507  [{'alias': 'burgers', 'title': 'Burgers'}]                                                                                                                                                                         \n",
       "139583  [{'alias': 'thai', 'title': 'Thai'}, {'alias': 'laotian', 'title': 'Laotian'}]                                                                                                                                     \n",
       "144305  [{'alias': 'dimsum', 'title': 'Dim Sum'}]                                                                                                                                                                          \n",
       "153504  [{'alias': 'chinese', 'title': 'Chinese'}, {'alias': 'noodles', 'title': 'Noodles'}]                                                                                                                               \n",
       "155220  [{'alias': 'vegetarian', 'title': 'Vegetarian'}, {'alias': 'cantonese', 'title': 'Cantonese'}]                                                                                                                     \n",
       "161118  [{'alias': 'cuban', 'title': 'Cuban'}, {'alias': 'desserts', 'title': 'Desserts'}, {'alias': 'sandwiches', 'title': 'Sandwiches'}]                                                                                 \n",
       "172541  [{'alias': 'desserts', 'title': 'Desserts'}]                                                                                                                                                                       \n",
       "176165  [{'alias': 'mexican', 'title': 'Mexican'}]                                                                                                                                                                         \n",
       "\n",
       "        business_stars  review_count  \n",
       "8263    4.5             31            \n",
       "9368    4.5             1049          \n",
       "11498   4.0             54            \n",
       "11674   3.5             49            \n",
       "12569   3.5             63            \n",
       "15311   4.0             108           \n",
       "16684   3.0             478           \n",
       "18288   4.5             1159          \n",
       "23246   4.0             379           \n",
       "32766   3.5             61            \n",
       "38630   4.0             166           \n",
       "42198   3.5             33            \n",
       "43499   2.5             225           \n",
       "45274   4.0             125           \n",
       "47285   4.0             381           \n",
       "61088   4.5             112           \n",
       "61304   3.5             107           \n",
       "67064   4.0             30            \n",
       "75008   4.5             43            \n",
       "88452   4.0             224           \n",
       "95232   4.0             212           \n",
       "95839   4.0             28            \n",
       "98919   4.0             228           \n",
       "101198  4.0             709           \n",
       "102493  4.5             345           \n",
       "110652  4.0             52            \n",
       "111946  4.0             86            \n",
       "117599  4.5             85            \n",
       "118364  4.0             19            \n",
       "118848  4.0             31            \n",
       "119430  4.0             57            \n",
       "119454  4.5             435           \n",
       "137507  3.5             165           \n",
       "139583  4.5             57            \n",
       "144305  3.5             219           \n",
       "153504  4.0             212           \n",
       "155220  4.0             103           \n",
       "161118  4.0             177           \n",
       "172541  3.5             122           \n",
       "176165  4.5             48            "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the corresponding business information using the business num id \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df[df[\"business_num_id\"].isin(cols)].drop_duplicates(subset = 'business_num_id', keep = 'first')[[\"business_num_id\",\"name\",\"categories\",\"business_stars\",\"review_count_y\"]].rename(columns={'review_count_y': 'review_count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tkinter examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "#Multiple checkboxes example\n",
    "\n",
    "# from tkinter import *\n",
    "# class Checkbar(Frame):\n",
    "#    def __init__(self, parent=None, picks=[], side=LEFT, anchor=W):\n",
    "#       Frame.__init__(self, parent)\n",
    "#       self.vars = []\n",
    "    \n",
    "#       for pick in picks:\n",
    "#          var = IntVar()\n",
    "#          chk = Checkbutton(self, text=pick, variable=var)\n",
    "#          chk.pack(side=side, anchor=anchor, expand=YES)\n",
    "#          self.vars.append(var)\n",
    "            \n",
    "#    def state(self):\n",
    "#       return map((lambda var: var.get()), self.vars)\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#    root = Tk()\n",
    "#    lng = Checkbar(root, ['Python', 'Ruby', 'Perl', 'C++'])\n",
    "#    tgl = Checkbar(root, ['English','German'])\n",
    "#    lng.pack(side=TOP,  fill=X)\n",
    "#    tgl.pack(side=LEFT)\n",
    "#    lng.config(relief=GROOVE, bd=2)\n",
    "\n",
    "#    def allstates(): \n",
    "#       print(list(lng.state()), list(tgl.state()))\n",
    "#    Button(root, text='Quit', command=root.quit).pack(side=RIGHT)\n",
    "#    Button(root, text='Peek', command=allstates).pack(side=RIGHT)\n",
    "#    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For checkbox demo \n",
    "# class checkBox(Frame):\n",
    "    \n",
    "#     def __init__(self, master):\n",
    "#             self.var = IntVar()\n",
    "            \n",
    "#             c = Checkbutton(master, text=\"Liked\", variable=self.var, command=self.cb)\n",
    "#             c.pack()\n",
    "\n",
    "#     def cb(self):\n",
    "#         print (\"variable is\", self.var.get())\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     #Initialize the root \n",
    "#     root = Tk()\n",
    "\n",
    "#     #Initialize the entries\n",
    "#     checkbox = checkBox(root)\n",
    "\n",
    "#     root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Demo for text boxes\n",
    "# fields = ['Like', 'Dislike']\n",
    "\n",
    "# def fetch(entries):\n",
    "#     for entry in entries:\n",
    "#         #index 0 is the field \n",
    "#         #index 1 is the entry data \n",
    "#         field = entry[0]\n",
    "#         text  = entry[1].get()\n",
    "#         print('%s: \"%s\"' % (field, text)) \n",
    "\n",
    "# def makeform(root, fields):\n",
    "#     entries = []\n",
    "#     #For each field, like \n",
    "#     for field in fields:\n",
    "#         row = Frame(root)\n",
    "#         lab = Label(row, width=15, text=field, anchor='w')\n",
    "#         ent = Entry(row)\n",
    "#         row.pack(side=TOP, fill=X, padx=5, pady=5)\n",
    "#         lab.pack(side=LEFT)\n",
    "#         ent.pack(side=RIGHT, expand=YES, fill=X)\n",
    "#         entries.append((field, ent))\n",
    "#     return entries\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     #Initialize the root \n",
    "#     root = Tk()\n",
    "    \n",
    "#     #Initialize the entries\n",
    "#     ents = makeform(root, fields)\n",
    "    \n",
    "#     #the buttons \n",
    "#     root.bind('<Button-1>', (lambda event, e=ents: fetch(e)))   \n",
    "#     b1 = Button(root, text='Show', command=(lambda e=ents: fetch(e)))\n",
    "#     b1.pack(side=LEFT, padx=5, pady=5)\n",
    "#     b2 = Button(root, text='Quit', command=root.quit)\n",
    "#     b2.pack(side=LEFT, padx=5, pady=5)\n",
    "#     root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Version\n",
    "# class MultiListbox(Frame):\n",
    "    \n",
    "#     def __init__(self, master, lists):\n",
    "#         Frame.__init__(self, master)\n",
    "#         self.lists = []\n",
    "        \n",
    "#         #Loop through the lists, l is the list label and widthW is the width \n",
    "#         for l,widthW in lists:\n",
    "            \n",
    "#             frame = Frame(self); frame.pack(side=LEFT, expand=YES, fill=BOTH)\n",
    "            \n",
    "#             Label(frame, text=l, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "            \n",
    "#             lb = Listbox(frame, width=widthW, borderwidth=0, selectborderwidth=0,\n",
    "#                  relief=FLAT, exportselection=FALSE)\n",
    "#             lb.pack(expand=YES, fill=BOTH)\n",
    "            \n",
    "#             self.lists.append(lb)\n",
    "            \n",
    "#             lb.bind('<B1-Motion>', lambda e, s=self: s._select(e.y))\n",
    "#             lb.bind('<Button-1>', lambda e, s=self: s._select(e.y))\n",
    "            \n",
    "#             lb.bind('<Leave>', lambda e: 'break')\n",
    "            \n",
    "#             lb.bind('<B2-Motion>', lambda e, s=self: s._b2motion(e.x, e.y))\n",
    "#             lb.bind('<Button-2>', lambda e, s=self: s._button2(e.x, e.y))\n",
    "            \n",
    "#         #pakcing the frame\n",
    "#         frame = Frame(self); frame.pack(side=LEFT, fill=Y)\n",
    "        \n",
    "#         #packing the label\n",
    "#         Label(frame, borderwidth=1, relief=RAISED).pack(fill=X)\n",
    "        \n",
    "#         #Setting a scrollbar \n",
    "# #         sb = Scrollbar(frame, orient=VERTICAL, command=self._scroll)\n",
    "        \n",
    "# #         sb.pack(expand=YES, fill=Y)\n",
    "        \n",
    "# #         self.lists[0]['yscrollcommand']=sb.set\n",
    "\n",
    "#     def _select(self, y):\n",
    "#         row = self.lists[0].nearest(y)\n",
    "#         self.selection_clear(0, END)\n",
    "#         self.selection_set(row)\n",
    "#         return 'break'\n",
    "\n",
    "#     def _button2(self, x, y):\n",
    "#         for l in self.lists: l.scan_mark(x, y)\n",
    "#         return 'break'\n",
    "\n",
    "#     def _b2motion(self, x, y):\n",
    "#         for l in self.lists: l.scan_dragto(x, y)\n",
    "#         return 'break'\n",
    "\n",
    "#     def _scroll(self, *args):\n",
    "#         for l in self.lists:\n",
    "#             apply(l.yview, args)\n",
    "\n",
    "#     def curselection(self):\n",
    "#         return self.lists[0].curselection()\n",
    "\n",
    "#     def delete(self, first, last=None):\n",
    "#         for l in self.lists:\n",
    "#             l.delete(first, last)\n",
    "\n",
    "# #     def get(self, first, last=None):\n",
    "# #         result = []\n",
    "# #         for l in self.lists:\n",
    "# #             result.append(l.get(first,last))\n",
    "# #         if last: return apply(map, [None] + result)\n",
    "# #         return result\n",
    "\n",
    "#     def index(self, index):\n",
    "#         self.lists[0].index(index)\n",
    "\n",
    "#     def insert(self, index, *elements):\n",
    "#         #Loop through the elements \n",
    "#         for element in elements:\n",
    "#             i = 0\n",
    "#             for l in self.lists:\n",
    "#                 l.insert(index, element[i])\n",
    "#                 i = i + 1\n",
    "\n",
    "#     def size(self):\n",
    "#         return self.lists[0].size()\n",
    "\n",
    "#     def see(self, index):\n",
    "#         for l in self.lists:\n",
    "#             l.see(index)\n",
    "\n",
    "#     def selection_anchor(self, index):\n",
    "#         for l in self.lists:\n",
    "#             l.selection_anchor(index)\n",
    "\n",
    "#     def selection_clear(self, first, last=None):\n",
    "#         for l in self.lists:\n",
    "#             l.selection_clear(first, last)\n",
    "\n",
    "#     def selection_includes(self, index):\n",
    "#         return self.lists[0].selection_includes(index)\n",
    "\n",
    "#     def selection_set(self, first, last=None):\n",
    "#         for l in self.lists:\n",
    "#             l.selection_set(first, last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Dropdown mdemo\n",
    "# OPTIONS = [\n",
    "# \"Jan\",\n",
    "# \"Feb\",\n",
    "# \"Mar\"\n",
    "# ] #etc\n",
    "\n",
    "# master = Tk()\n",
    "\n",
    "# variable = StringVar(master)\n",
    "# variable.set(OPTIONS[0]) # default value\n",
    "\n",
    "# w = OptionMenu(master, variable, *OPTIONS)\n",
    "# w.pack()\n",
    "\n",
    "# def ok():\n",
    "#     print (\"value is:\" + variable.get())\n",
    "\n",
    "# #retrive the drop down value \n",
    "# button = Button(master, text=\"OK\", command=ok)\n",
    "# button.pack()\n",
    "\n",
    "# mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
