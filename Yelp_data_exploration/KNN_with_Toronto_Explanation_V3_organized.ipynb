{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\songya25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\songya25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\songya25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\songya25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\songya25\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#Data Packages\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "#Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Counter\n",
    "from collections import Counter\n",
    "import collections\n",
    "\n",
    "#Operation\n",
    "import operator\n",
    "\n",
    "#Natural Language Processing Packages\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "## Download Resources\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.data import find\n",
    "\n",
    "## Machine Learning\n",
    "import sklearn\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-word for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update Lexicon --> Add Polarity Score for Restaurant Specific Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = {\n",
    "\"excellent\":3.99,\n",
    "\"amazing\":3.981,\n",
    "\"awesome\":3.98,\n",
    "\"best\":3.98,\n",
    "\"wonderful\":3.979,\n",
    "\"authentic\":3.97,\n",
    "\"incredible\":3.97,\n",
    "\"incredibly\":3.97,\n",
    "\"popular\":3.96,\n",
    "\"well-known\":3.96,\n",
    "\"delicious\":3.95,\n",
    "\"tasty\":3.94,\n",
    "\"five-star\":3.91,\n",
    "\"fanciest\":3.89,\n",
    "\"finest\":3.89,\n",
    "\"fast\":3.88,\n",
    "\"phenomenal\":3.88,\n",
    "\"exceptional\":3.83,\n",
    "\"exceptionally\":3.83,\n",
    "\"extrodinary\":3.82,\n",
    "\"magnificent\":3.82,\n",
    "\"famous\":3.81,\n",
    "\"fantastic\":3.8,\n",
    "\"splendid\":3.7,\n",
    "\"yummy\":3.7,\n",
    "\"flavorful\":3.699,\n",
    "\"flavourful\":3.699,\n",
    "\"delightful\":3.69,\n",
    "\"decent\":3.6,\n",
    "\"unique\":3.6,\n",
    "\"fancy\":3.5,\n",
    "\"quick\":3.1,\n",
    "\"classic\":3,\n",
    "\"fresh\":3\n",
    "}\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "#Update new words with polarity score\n",
    "sid.lexicon.update(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contraction Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use vader to evaluated sentiment of reviews, vader here!\n",
    "def evalSentences(sentences, to_df=False, columns=[]):\n",
    "    #Instantiate an instance to access SentimentIntensityAnalyzer class\n",
    "    pdlist = []\n",
    "    if to_df:\n",
    "        for sentence in tqdm(sentences):\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            pdlist.append([sentence]+[ss['compound']])\n",
    "        reviewDf = pd.DataFrame(pdlist)\n",
    "        reviewDf.columns = columns\n",
    "        return reviewDf\n",
    "    \n",
    "    else:\n",
    "        for sentence in tqdm(sentences):\n",
    "            print(sentence)\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            for k in sorted(ss):\n",
    "                print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yelp_df(path = 'data/', filename = 'Export_CleanedReview.json', sampling=False, top_user_num=6100, top_item_num=4000):\n",
    "    \"\"\"\n",
    "    Get the pandas dataframe\n",
    "    Sampling only the top users/items by density \n",
    "    Implicit representation applies\n",
    "    \"\"\"\n",
    "    with open(filename,'r') as f:\n",
    "        data = f.readlines()\n",
    "        data = list(map(json.loads, data))\n",
    "    \n",
    "    data = data[0]\n",
    "    #Get all the data from the data\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    df.rename(columns={'stars': 'review_stars', 'text': 'review_text', 'cool': 'review_cool',\n",
    "                       'funny': 'review_funny', 'useful': 'review_useful'}, inplace=True)\n",
    "\n",
    "    df['business_num_id'] = df.business_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.business_id.nunique()))\n",
    "    df['business_num_id'] = df['business_num_id'].astype('int')\n",
    "\n",
    "    df['user_num_id'] = df.user_id.astype('category').\\\n",
    "    cat.rename_categories(range(0, df.user_id.nunique()))\n",
    "    df['user_num_id'] = df['user_num_id'].astype('int')\n",
    "\n",
    "    df['timestamp'] = df['date'].apply(date_to_timestamp)\n",
    "\n",
    "    if sampling:\n",
    "        df = filter_yelp_df(df, top_user_num=top_user_num, top_item_num=top_item_num)\n",
    "        # Refresh num id\n",
    "        df['business_num_id'] = df.business_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.business_id.nunique()))\n",
    "        df['business_num_id'] = df['business_num_id'].astype('int')\n",
    "        \n",
    "        df['user_num_id'] = df.user_id.astype('category').\\\n",
    "        cat.rename_categories(range(0, df.user_id.nunique()))\n",
    "        df['user_num_id'] = df['user_num_id'].astype('int')\n",
    "        # drop_list = ['date','review_id','review_funny','review_cool','review_useful']\n",
    "        # df = df.drop(drop_list, axis=1)\n",
    "    df = df.reset_index(drop = True)\n",
    "    return df \n",
    "\n",
    "def filter_yelp_df(df, top_user_num=6100, top_item_num=4000):\n",
    "    #Getting the reviews where starts are above 3\n",
    "    df_implicit = df[df['review_stars']>3]\n",
    "    frequent_user_id = df_implicit['user_num_id'].value_counts().head(top_user_num).index.values\n",
    "    frequent_item_id = df_implicit['business_num_id'].value_counts().head(top_item_num).index.values\n",
    "    return df.loc[(df['user_num_id'].isin(frequent_user_id)) & (df['business_num_id'].isin(frequent_item_id))]\n",
    "\n",
    "def date_to_timestamp(date):\n",
    "    dt = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    return time.mktime(dt.timetuple())\n",
    "\n",
    "def df_to_sparse(df, row_name='userId', col_name='movieId', value_name='rating',\n",
    "                 shape=None):\n",
    "    rows = df[row_name]\n",
    "    cols = df[col_name]\n",
    "    if value_name is not None:\n",
    "        values = df[value_name]\n",
    "    else:\n",
    "        values = [1]*len(rows)\n",
    "    return csr_matrix((values, (rows, cols)), shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_business_review(business_num_id,df_Subset_Cols):\n",
    "    # Set the business ID below\n",
    "    #3464 Hotpot, 2159 Hand-pulled noodle, 3490 Pai Northern Thai\n",
    "    review_text = df.loc[df['business_num_id'] == business_num_id, 'review_text'].sum()\n",
    "    #review_text\n",
    "    for word in tqdm(review_text.split()):\n",
    "        if word.lower() in contractions:\n",
    "            review_text = review_text.replace(word, contractions[word.lower()])\n",
    "    #print(review_text)\n",
    "    df_spe_item = df_Subset_Cols.loc[df_Subset_Cols['business_num_id'] == business_num_id]\n",
    "    return review_text, df_spe_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun Phrase Extraction Support Functions\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "# generator, generate leaves one by one\n",
    "# Filter only on NP (The other two labels may not be useful)\n",
    "def leaves(tree):\n",
    "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP' or t.label()=='JJ' or t.label()=='RB'):\n",
    "        yield subtree.leaves()\n",
    "\n",
    "# stemming, lematizing, lower case...\n",
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    #word = stemmer.stem(word)\n",
    "    if word != 'was':\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "    return word\n",
    "        \n",
    "# stop-words and length control\n",
    "def acceptable_word(word):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "    special_non_stopwords = ['is','was','are','were'] # Kick out a few words from stopwords list\n",
    "    final_stop_words = list([word for word in stopwords.words('english') if word not in special_non_stopwords])\n",
    "    accepted = (bool(2 <= len(word) <= 40)\n",
    "    and word.lower() not in final_stop_words)\n",
    "    return accepted\n",
    "        \n",
    "# generator, create item once a time\n",
    "def get_terms(tree):\n",
    "    for leaf in leaves(tree):\n",
    "        # Normalize word in \n",
    "        term = [normalise(w) for w,t in leaf if acceptable_word(w) ]\n",
    "        # Phrase only\n",
    "        if len(term)>1:\n",
    "            yield term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten phrase lists to get tokens for analysis\n",
    "def flatten(npTokenList):\n",
    "    finalList =[]\n",
    "    for phrase in npTokenList:\n",
    "        token = ''\n",
    "        for word in phrase:\n",
    "            token += word + ' '\n",
    "        finalList.append(token.rstrip())\n",
    "    return finalList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2\n",
    "grammar1 = r\"\"\"\n",
    "    NBAR:\n",
    "        {<RB|RBS>?<JJ|JJR|JJS>+<NN|NNS>+}\n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}\n",
    "\"\"\"\n",
    "\n",
    "grammar2 = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN|NNS>+<VBZ|VBD><RB>?<JJ>+}\n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyphrase_list(review_text):\n",
    "    tagger = PerceptronTagger()\n",
    "    pos_tag = tagger.tag\n",
    "    taggedToks = pos_tag([word.lower() for word in tqdm(re.findall(r'[\\w]+-[\\w]+[-[\\w]+]?|[\\w]+|[.,!?;]', review_text))])\n",
    "\n",
    "    # Create phrase tree\n",
    "    chunker1 = nltk.RegexpParser(grammar1)\n",
    "    tree1= chunker1.parse(taggedToks)\n",
    "    chunker2 = nltk.RegexpParser(grammar2)\n",
    "    tree2= chunker2.parse(taggedToks)\n",
    "\n",
    "    # Traverse tree and get noun phrases\n",
    "    npTokenList1 = [word for word in get_terms(tree1)]\n",
    "    # npTokenList\n",
    "    npTokenList2 = [word for word in get_terms(tree2)]\n",
    "    # Combine np token list to one\n",
    "    npTokenList_duplicate = npTokenList1 + npTokenList2\n",
    "    npTokenList = []\n",
    "    for i in npTokenList_duplicate:\n",
    "        if i not in npTokenList:\n",
    "            npTokenList.append(i)\n",
    "\n",
    "    Extracted_list1 = flatten(npTokenList1)\n",
    "    Extracted_list2 = flatten(npTokenList2)\n",
    "    #Extracted_list = flatten([word\n",
    "    #                           for word\n",
    "    #                           in get_terms(chunker.parse(pos_tag([word.lower()\n",
    "    #                                                               for word in re.findall(r'\\w+', review_text)])))])\n",
    "    Extracted_list = Extracted_list1 + list(set(Extracted_list2) - set(Extracted_list1))\n",
    "    return Extracted_list,npTokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_list(npToken_List,stop_words):\n",
    "    # create a list of NN from the Keyphrase list\n",
    "    NN_List=[]\n",
    "    \n",
    "    for word in npToken_List:\n",
    "        phrase = pos_tag(word)\n",
    "        for term in phrase:\n",
    "            if term[1]=='NN':\n",
    "                NN_List.append(term[0])\n",
    "    NN_List=list(dict.fromkeys(NN_List))\n",
    "    \n",
    "    # filter out the stop words\n",
    "    NN_List= [x for x in NN_List if x not in stop_words]\n",
    "    return NN_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_from_freq(NN_List,review_text,df_spe_item):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    stop = set(stopwords.words('english'))\n",
    "    # Get the frequency of the noun from the whole review, store in the dict_Counter\n",
    "    dict_Counter = {}\n",
    "    c = collections.Counter([normalise(word) for word in re.findall(r'[\\w]+-[\\w]+[-[\\w]+]?|[\\w]+|[.,!?;]', review_text) if word.lower() not in stop and len(word) > 2])\n",
    "    for term in NN_List:\n",
    "        if term in c:\n",
    "            dict_Counter[term] = dict(c)[term]\n",
    "\n",
    "    \n",
    "    # Get the binary frequency of the noun for each review. If the word in this review, then it is 1, otherwise is 0\n",
    "    # store in the review_freq_counter\n",
    "    # initialize review_freq_counter\n",
    "    review_freq_counter = {}   \n",
    "    for k in dict_Counter.keys():\n",
    "        review_freq_counter[k] = 0\n",
    "        \n",
    "    for i in range(len(df_spe_item.review_text.values)):\n",
    "        for word in dict_Counter.keys():\n",
    "            temp = lemmatizer.lemmatize(df_spe_item.review_text.values[i]).lower()\n",
    "            if word in re.findall(r'[\\w]+-[\\w]+[-[\\w]+]?|[\\w]+|[.,!?;]',temp):\n",
    "                review_freq_counter[word] = review_freq_counter[word]+1  \n",
    "                \n",
    "    # normalized score by --> dict_Counter(total frequency) * review_freq_counter(binary review frequency)\n",
    "    normalized_score = {}\n",
    "    for k in dict_Counter.keys():\n",
    "        normalized_score[k] = 0\n",
    "\n",
    "    #construct the score by multiplying\n",
    "    for word in dict_Counter.keys():\n",
    "        normalized_score[word] = dict_Counter[word] * review_freq_counter[word]\n",
    "\n",
    "    #sort it and store it in a list\n",
    "    final_score_counter=[]\n",
    "    for key, value in tqdm(sorted(normalized_score.items(), key=lambda item: item[1], reverse = True)):\n",
    "        temp = [key,value]\n",
    "        final_score_counter.append(temp)\n",
    "    return final_score_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyphrase_contain_topk(top_word_list, k,Extracted_list):\n",
    "    topk_freq_words = [x[0] for x in top_word_list[0:k]]\n",
    "    keyphrase_list = []\n",
    "    for key_word in Extracted_list:\n",
    "        for freq_word in topk_freq_words:\n",
    "            if freq_word in key_word:\n",
    "                keyphrase_list.append(key_word)\n",
    "    return list(dict.fromkeys(keyphrase_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Read Data\n",
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewJsonToronto = \"..\\\\data\\\\Export_TorontoData.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Day', 'Month', 'Unnamed: 0.1', 'Unnamed: 0_x', 'Unnamed: 0_y',\n",
       "       'Updated', 'Year', 'alias', 'business_id', 'business_stars',\n",
       "       'categories', 'coordinates', 'date', 'display_phone', 'distance',\n",
       "       'friend_count', 'ghost', 'image_url', 'img_dsc', 'img_url', 'index',\n",
       "       'is_closed', 'location', 'name', 'nr', 'phone', 'photo_count', 'price',\n",
       "       'review_count_x', 'review_count_y', 'review_date', 'review_id',\n",
       "       'review_language', 'review_stars', 'review_text', 'transactions', 'ufc',\n",
       "       'url', 'user_id', 'user_loc', 'vote_count', 'business_num_id',\n",
       "       'user_num_id', 'timestamp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_yelp_df(path ='', filename=reviewJsonToronto, sampling= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-setting for the variables\n",
    "set :\n",
    "- which business to use\n",
    "- stop_words\n",
    "- how many nouns\n",
    "- tagger and pos_tag declaration\n",
    "- Add Vader for restaurant-specific words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_num_id = 2159\n",
    "stop_words_defined = [\"place\",\"restaurant\",\"food\", \"amount\",\"experience\",\"side\",\"size\",\"location\",\"quality\" ,\"taste\"\\\n",
    ",\"thing\",\"order\",\"bowl\",\"night\",\"day\",\"flavour\",\"spot\",\"portion\",\"dish\",\"meal\"]\n",
    "tagger = PerceptronTagger()\n",
    "pos_tag = tagger.tag\n",
    "# how many nouns are we looking at\n",
    "top_k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 30651/30651 [00:00<00:00, 383993.87it/s]\n"
     ]
    }
   ],
   "source": [
    "review_text, df_business= get_business_review(2159,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 35558/35558 [00:00<00:00, 3565664.80it/s]\n"
     ]
    }
   ],
   "source": [
    "Extracted_list,npTokenList = get_keyphrase_list(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "Noun_List = get_noun_list(npTokenList,stop_words_defined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the score for the noun list. Normalize by --> dict_Counter(total frequency) * review_freq_counter(binary review frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 482/482 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['noodle', 122850],\n",
       " ['soup', 27280],\n",
       " ['beef', 21120],\n",
       " ['broth', 11408],\n",
       " ['spicy', 7630],\n",
       " ['time', 7458],\n",
       " ['service', 6142],\n",
       " ['menu', 5346],\n",
       " ['sauce', 4505],\n",
       " ['hand', 4380],\n",
       " ['delicious', 3762],\n",
       " ['table', 3362],\n",
       " ['pork', 3300],\n",
       " ['bit', 3283],\n",
       " ['line', 3149],\n",
       " ['try', 2820],\n",
       " ['oil', 2240],\n",
       " ['wait', 2236],\n",
       " ['lanzhou', 1850],\n",
       " ['chewy', 1833],\n",
       " ['meat', 1666],\n",
       " ['lunch', 1591],\n",
       " ['friend', 1296],\n",
       " ['recommend', 1295],\n",
       " ['chinese', 1287],\n",
       " ['thick', 1254],\n",
       " ['thin', 1248],\n",
       " ['salty', 1221],\n",
       " ['texture', 1216],\n",
       " ['mega', 1170],\n",
       " ['thickness', 1152],\n",
       " ['chili', 1110],\n",
       " ['quick', 1080],\n",
       " ['lot', 1066],\n",
       " ['price', 1040],\n",
       " ['option', 912],\n",
       " ['flavourful', 899],\n",
       " ['extra', 884],\n",
       " ['super', 864],\n",
       " ['dinner', 837],\n",
       " ['downtown', 806],\n",
       " ['simple', 784],\n",
       " ['seating', 783],\n",
       " ['hot', 780],\n",
       " ['staff', 750],\n",
       " ['perfect', 700],\n",
       " ['fast', 676],\n",
       " ['type', 651],\n",
       " ['spice', 644],\n",
       " ['medium', 616],\n",
       " ['area', 600],\n",
       " ['tasty', 575],\n",
       " ['chef', 432],\n",
       " ['space', 418],\n",
       " ['thought', 414],\n",
       " ['eating', 400],\n",
       " ['appetizer', 392],\n",
       " ['chilli', 384],\n",
       " ['egg', 375],\n",
       " ['water', 374],\n",
       " ['jellyfish', 374],\n",
       " ['cilantro', 357],\n",
       " ['choice', 352],\n",
       " ['cucumber', 348],\n",
       " ['style', 336],\n",
       " ['tender', 306],\n",
       " ['fan', 304],\n",
       " ['feel', 304],\n",
       " ['way', 304],\n",
       " ['base', 304],\n",
       " ['dough', 304],\n",
       " ['seat', 299],\n",
       " ['door', 288],\n",
       " ['toronto', 272],\n",
       " ['add', 266],\n",
       " ['level', 255],\n",
       " ['pho', 252],\n",
       " ['round', 210],\n",
       " ['kind', 209],\n",
       " ['server', 200],\n",
       " ['everything', 196],\n",
       " ['pull', 195],\n",
       " ['regular', 195],\n",
       " ['visit', 192],\n",
       " ['person', 182],\n",
       " ['decent', 168],\n",
       " ['kitchen', 168],\n",
       " ['lineup', 168],\n",
       " ['light', 156],\n",
       " ['thicker', 156],\n",
       " ['tea', 154],\n",
       " ['hungry', 144],\n",
       " ['friday', 144],\n",
       " ['pulling', 144],\n",
       " ['bland', 132],\n",
       " ['tip', 126],\n",
       " ['loud', 121],\n",
       " ['kick', 121],\n",
       " ['mild', 120],\n",
       " ['kimchi', 112],\n",
       " ['handmade', 110],\n",
       " ['winter', 110],\n",
       " ['veggie', 105],\n",
       " ['filling', 100],\n",
       " ['hype', 100],\n",
       " ['guy', 100],\n",
       " ['bite', 99],\n",
       " ['review', 96],\n",
       " ['flavor', 91],\n",
       " ['bar', 90],\n",
       " ['radish', 88],\n",
       " ['version', 81],\n",
       " ['note', 81],\n",
       " ['everyone', 81],\n",
       " ['tendon', 80],\n",
       " ['customer', 80],\n",
       " ['work', 80],\n",
       " ['craving', 77],\n",
       " ['store', 77],\n",
       " ['waiter', 77],\n",
       " ['half', 72],\n",
       " ['standard', 72],\n",
       " ['entrance', 72],\n",
       " ['dente', 64],\n",
       " ['clean', 64],\n",
       " ['part', 64],\n",
       " ['rush', 64],\n",
       " ['chewiness', 64],\n",
       " ['disappointing', 64],\n",
       " ['house', 63],\n",
       " ['cut', 63],\n",
       " ['waitress', 63],\n",
       " ['onion', 60],\n",
       " ['shank', 60],\n",
       " ['hour', 56],\n",
       " ['msg', 56],\n",
       " ['fun', 56],\n",
       " ['shop', 54],\n",
       " ['mean', 52],\n",
       " ['something', 49],\n",
       " ['fair', 49],\n",
       " ['inside', 49],\n",
       " ['tomato', 45],\n",
       " ['look', 45],\n",
       " ['sweet', 42],\n",
       " ['salad', 42],\n",
       " ['establishment', 42],\n",
       " ['serving', 40],\n",
       " ['group', 36],\n",
       " ['favorite', 36],\n",
       " ['tasting', 36],\n",
       " ['selection', 36],\n",
       " ['max', 36],\n",
       " ['set', 36],\n",
       " ['peak', 36],\n",
       " ['heat', 36],\n",
       " ['top', 36],\n",
       " ['opening', 35],\n",
       " ['kuan', 35],\n",
       " ['street', 35],\n",
       " ['daikon', 35],\n",
       " ['form', 35],\n",
       " ['cook', 35],\n",
       " ['sound', 35],\n",
       " ['container', 35],\n",
       " ['variety', 32],\n",
       " ['name', 30],\n",
       " ['reason', 30],\n",
       " ['bottom', 30],\n",
       " ['mine', 30],\n",
       " ['choy', 30],\n",
       " ['ground', 30],\n",
       " ['comfort', 25],\n",
       " ['feeling', 25],\n",
       " ['thursday', 25],\n",
       " ['deal', 25],\n",
       " ['sign', 25],\n",
       " ['offering', 25],\n",
       " ['width', 25],\n",
       " ['shape', 25],\n",
       " ['weekday', 25],\n",
       " ['wider', 25],\n",
       " ['etc', 25],\n",
       " ['man', 25],\n",
       " ['serve', 25],\n",
       " ['zhou', 25],\n",
       " ['item', 24],\n",
       " ['slice', 21],\n",
       " ['chair', 21],\n",
       " ['number', 20],\n",
       " ['carrot', 18],\n",
       " ['zha', 18],\n",
       " ['jiang', 18],\n",
       " ['counter', 16],\n",
       " ['list', 16],\n",
       " ['room', 16],\n",
       " ['wednesday', 16],\n",
       " ['mediocre', 16],\n",
       " ['fix', 16],\n",
       " ['handle', 16],\n",
       " ['lan', 16],\n",
       " ['saturday', 16],\n",
       " ['sesame', 16],\n",
       " ['evening', 16],\n",
       " ['pricing', 16],\n",
       " ['slamming', 16],\n",
       " ['vinegar', 16],\n",
       " ['consistency', 16],\n",
       " ['signature', 16],\n",
       " ['range', 15],\n",
       " ['magic', 15],\n",
       " ['parsley', 12],\n",
       " ['summer', 12],\n",
       " ['min', 12],\n",
       " ['joint', 12],\n",
       " ['faster', 12],\n",
       " ['greasy', 12],\n",
       " ['ratio', 12],\n",
       " ['husband', 12],\n",
       " ['protein', 9],\n",
       " ['beefy', 9],\n",
       " ['udon', 9],\n",
       " ['peppercorn', 9],\n",
       " ['balance', 9],\n",
       " ['twist', 9],\n",
       " ['line-up', 9],\n",
       " ['preference', 9],\n",
       " ['thickest', 9],\n",
       " ['chewier', 9],\n",
       " ['solo', 9],\n",
       " ['package', 9],\n",
       " ['bomb', 9],\n",
       " ['crunch', 9],\n",
       " ['hand-pulling', 9],\n",
       " ['stuff', 9],\n",
       " ['lunchtime', 9],\n",
       " ['touch', 9],\n",
       " ['regardless', 9],\n",
       " ['cause', 9],\n",
       " ['hand-made', 9],\n",
       " ['idea', 9],\n",
       " ['value', 9],\n",
       " ['stuffed', 9],\n",
       " ['station', 9],\n",
       " ['dining', 9],\n",
       " ['noisy', 9],\n",
       " ['dine', 9],\n",
       " ['compare', 9],\n",
       " ['multiple', 9],\n",
       " ['brisket', 9],\n",
       " ['wife', 9],\n",
       " ['bang', 8],\n",
       " ['con', 8],\n",
       " ['saucy', 8],\n",
       " ['stomach', 8],\n",
       " ['bokchoy', 8],\n",
       " ['refill', 8],\n",
       " ['complex', 6],\n",
       " ['video', 6],\n",
       " ['parking', 6],\n",
       " ['eater', 6],\n",
       " ['seaweed', 6],\n",
       " ['key', 4],\n",
       " ['veg', 4],\n",
       " ['smokey', 4],\n",
       " ['series', 4],\n",
       " ['convenient', 4],\n",
       " ['bonus', 4],\n",
       " ['savoury', 4],\n",
       " ['yum', 4],\n",
       " ['debit', 4],\n",
       " ['transaction', 4],\n",
       " ['background', 4],\n",
       " ['speedy', 4],\n",
       " ['change', 4],\n",
       " ['cleanser', 4],\n",
       " ['chew', 4],\n",
       " ['random', 4],\n",
       " ['maker', 4],\n",
       " ['mess', 4],\n",
       " ['air', 4],\n",
       " ['banging', 4],\n",
       " ['vibe', 4],\n",
       " ['smile', 4],\n",
       " ['springy', 4],\n",
       " ['patron', 4],\n",
       " ['harder', 4],\n",
       " ['vinegary', 4],\n",
       " ['pricey', 4],\n",
       " ['wooden', 4],\n",
       " ['english', 4],\n",
       " ['tenderness', 4],\n",
       " ['benefit', 4],\n",
       " ['garlic', 4],\n",
       " ['smell', 4],\n",
       " ['prepare', 4],\n",
       " ['process', 4],\n",
       " ['traffic', 4],\n",
       " ['system', 4],\n",
       " ['plastic', 4],\n",
       " ['absolute', 4],\n",
       " ['shanghai', 4],\n",
       " ['presentation', 4],\n",
       " ['soy', 4],\n",
       " ['milk', 4],\n",
       " ['beverage', 4],\n",
       " ['tho', 4],\n",
       " ['factor', 4],\n",
       " ['bounce', 4],\n",
       " ['north', 4],\n",
       " ['turnaround', 4],\n",
       " ['mince', 4],\n",
       " ['turnover', 4],\n",
       " ['resto', 4],\n",
       " ['lady', 4],\n",
       " ['conversation', 3],\n",
       " ['elbow', 3],\n",
       " ['stop', 3],\n",
       " ['hair', 3],\n",
       " ['lover', 2],\n",
       " ['step', 2],\n",
       " ['interior', 2],\n",
       " ['noise', 2],\n",
       " ['pepper', 2],\n",
       " ['god', 2],\n",
       " ['aroma', 2],\n",
       " ['variant', 2],\n",
       " ['diner', 2],\n",
       " ['april', 2],\n",
       " ['translation', 2],\n",
       " ['colour', 2],\n",
       " ['soak', 2],\n",
       " ['appetite', 2],\n",
       " ['plane', 2],\n",
       " ['net', 2],\n",
       " ['tastebud', 2],\n",
       " ['problem', 2],\n",
       " ['explanation', 2],\n",
       " ['worker', 2],\n",
       " ['phase', 1],\n",
       " ['fake', 1],\n",
       " ['news', 1],\n",
       " ['monday', 1],\n",
       " ['batch', 1],\n",
       " ['granite', 1],\n",
       " ['bibimbap', 1],\n",
       " ['cheese', 1],\n",
       " ['cooking', 1],\n",
       " ['chart', 1],\n",
       " ['york', 1],\n",
       " ['access', 1],\n",
       " ['combo', 1],\n",
       " ['countless', 1],\n",
       " ['adventure', 1],\n",
       " ['combination', 1],\n",
       " ['basis', 1],\n",
       " ['curtain', 1],\n",
       " ['palate', 1],\n",
       " ['partway', 1],\n",
       " ['technology', 1],\n",
       " ['heft', 1],\n",
       " ['period', 1],\n",
       " ['event', 1],\n",
       " ['audience', 1],\n",
       " ['yup', 1],\n",
       " ['brand', 1],\n",
       " ['zing', 1],\n",
       " ['wonton', 1],\n",
       " ['alternate', 1],\n",
       " ['rice', 1],\n",
       " ['bandwagon', 1],\n",
       " ['performance', 1],\n",
       " ['camper', 1],\n",
       " ['afternoon', 1],\n",
       " ['sorta', 1],\n",
       " ['perusal', 1],\n",
       " ['gauge', 1],\n",
       " ['soya', 1],\n",
       " ['master', 1],\n",
       " ['luck', 1],\n",
       " ['mass', 1],\n",
       " ['mount', 1],\n",
       " ['train', 1],\n",
       " ['spoon', 1],\n",
       " ['investigation', 1],\n",
       " ['activity', 1],\n",
       " ['alsonuff', 1],\n",
       " ['go-to-place', 1],\n",
       " ['beijing', 1],\n",
       " ['apparel', 1],\n",
       " ['preparation', 1],\n",
       " ['tenant', 1],\n",
       " ['nicer', 1],\n",
       " ['fail', 1],\n",
       " ['pet', 1],\n",
       " ['noodlesthere', 1],\n",
       " ['afterward', 1],\n",
       " ['two-person', 1],\n",
       " ['washroom', 1],\n",
       " ['score', 1],\n",
       " ['halal', 1],\n",
       " ['mien', 1],\n",
       " ['coup', 1],\n",
       " ['pasta', 1],\n",
       " ['palette', 1],\n",
       " ['feature', 1],\n",
       " ['puller', 1],\n",
       " ['comforting', 1],\n",
       " ['snack', 1],\n",
       " ['tolerance', 1],\n",
       " ['creation', 1],\n",
       " ['girth', 1],\n",
       " ['presence', 1],\n",
       " ['soybean', 1],\n",
       " ['pan', 1],\n",
       " ['bag', 1],\n",
       " ['app', 1],\n",
       " ['rotation', 1],\n",
       " ['underdone', 1],\n",
       " ['steal', 1],\n",
       " ['memory', 1],\n",
       " ['health', 1],\n",
       " ['stimulation', 1],\n",
       " ['starsfill', 1],\n",
       " ['fillingwe', 1],\n",
       " ['noodz', 1],\n",
       " ['imo', 1],\n",
       " ['daunting', 1],\n",
       " ['pit', 1],\n",
       " ['hearty', 1],\n",
       " ['lam', 1],\n",
       " ['america', 1],\n",
       " ['vat', 1],\n",
       " ['smack', 1],\n",
       " ['arrangement', 1],\n",
       " ['turn-over', 1],\n",
       " ['tomatoey', 1],\n",
       " ['ventilation', 1],\n",
       " ['stir', 1],\n",
       " ['seatsthe', 1],\n",
       " ['site', 1],\n",
       " ['mouthful', 1],\n",
       " ['descent', 1],\n",
       " ['decor', 1],\n",
       " ['dietary', 1],\n",
       " ['partiescame', 0],\n",
       " ['buddy', 0],\n",
       " ['player', 0],\n",
       " ['slam', 0],\n",
       " ['variation', 0],\n",
       " ['herb', 0],\n",
       " ['ingredient', 0],\n",
       " ['minute', 0],\n",
       " ['topping', 0],\n",
       " ['reviewer', 0],\n",
       " ['entree', 0],\n",
       " ['le', 0],\n",
       " ['checkup', 0],\n",
       " ['degree', 0],\n",
       " ['soupi', 0],\n",
       " ['month', 0],\n",
       " ['hippo', 0],\n",
       " ['saida', 0],\n",
       " ['dumpling', 0],\n",
       " ['thud', 0],\n",
       " ['foodie', 0],\n",
       " ['chunk', 0],\n",
       " ['hahagb', 0],\n",
       " ['eye', 0],\n",
       " ['vein', 0],\n",
       " ['song', 0],\n",
       " ['kid', 0],\n",
       " ['frill', 0],\n",
       " ['expectation', 0],\n",
       " ['piece', 0],\n",
       " ['liquid', 0],\n",
       " ['ba', 0],\n",
       " ['meeting', 0],\n",
       " ['aunty', 0],\n",
       " ['cannot', 0],\n",
       " ['restriction', 0],\n",
       " ['serf', 0]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_word_list = get_score_from_freq(Noun_List,review_text,df_business)\n",
    "top_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['favourite noodle house',\n",
       " 'hand-pulled noodle',\n",
       " 'special lanzhou noodle',\n",
       " 'decent noodle',\n",
       " 'fellow noodle lover',\n",
       " 'noodle taste',\n",
       " 'narrow thick noodle',\n",
       " 'noodle soup',\n",
       " 'delicious beef slice',\n",
       " 'lanzhou style noodle',\n",
       " 'mega beef bowl',\n",
       " 'thin noodle',\n",
       " 'cold tomato egg noodle',\n",
       " 'hot noodle soup',\n",
       " 'noodle craving',\n",
       " 'dry mixed flat noodle',\n",
       " 'widest noodle',\n",
       " 'much better noodle',\n",
       " 'thin-thick noodle',\n",
       " 'great beefy flavour',\n",
       " 'classic lanzhou beef noodle',\n",
       " 'authentic lanzhou hand-pulled noodle',\n",
       " 'noodle option',\n",
       " 'special lanzhou beef noodle',\n",
       " 'flat noodle',\n",
       " 'dry noodle',\n",
       " 'different sized noodle',\n",
       " 'kuan noodle',\n",
       " 'over-cooked beef',\n",
       " 'mega size lanzhou beef noodle',\n",
       " 'mega sized beef noodle',\n",
       " 'large beef noodle',\n",
       " 'extra beef',\n",
       " 'noodle technician slam',\n",
       " 'soup noodle',\n",
       " 'ordinary soup noodle',\n",
       " 'chinese noodle',\n",
       " 'saucy noodle',\n",
       " 'noodle bowl',\n",
       " 'noodle size option',\n",
       " 'usual kuan noodle',\n",
       " 'handpulled noodle place',\n",
       " 'soup base',\n",
       " 'thick flat noodle',\n",
       " 'sliced beef',\n",
       " 'busy noodle spot',\n",
       " 'noodle dish',\n",
       " 'noodle price',\n",
       " 'mega beef',\n",
       " 'med wide noodle',\n",
       " 'fresh noodle',\n",
       " 'authentic lanzhou beef noodle',\n",
       " 'xi noodle',\n",
       " 'regular noodle',\n",
       " 'vegetable soup',\n",
       " 'thick noodle',\n",
       " 'fresh handmade noodle',\n",
       " 'thinly sliced beef',\n",
       " 'second largest size noodle',\n",
       " 'extreme wide noodle',\n",
       " 'specific noodle size type',\n",
       " 'really like beef',\n",
       " 'classic soup combo',\n",
       " 'tender beef',\n",
       " 'noodle shop',\n",
       " 'thinnest noodle',\n",
       " 'dry noodle dish',\n",
       " 'flavourful noodle',\n",
       " 'chinese hand-made noodle',\n",
       " 'noodle size',\n",
       " 'basic beef',\n",
       " 'delicious noodle',\n",
       " 'noodle store',\n",
       " 'customizable noodle thickness',\n",
       " 'delicious hand-pulled lanzhou style noodle',\n",
       " 'best noodle joint',\n",
       " 'mixed noodle',\n",
       " 'freshly pull noodle',\n",
       " 'small soup',\n",
       " 'great noodle joint',\n",
       " 'dry noodle soup',\n",
       " 'udon noodle',\n",
       " 'chewy noodle',\n",
       " 'noodle place',\n",
       " 'fish spicy soup',\n",
       " 'popular spicy beef broth',\n",
       " 'special beef tendon noodle',\n",
       " 'fresh pulled noodle',\n",
       " 'vegetarian soup',\n",
       " 'hand-pulled noodle place downtown',\n",
       " 'really delicious noodle',\n",
       " 'cold noodle',\n",
       " 'wide noodle',\n",
       " 'better soup',\n",
       " 'small beef soup',\n",
       " 'constant noodle',\n",
       " 'dried noodle',\n",
       " 'noodle maker',\n",
       " 'yup solid chinese hand-pulled noodle spot downtown',\n",
       " 'noodle thickness',\n",
       " 'gorgeous hand-pulled noodle',\n",
       " 'many hand-pulled noodle',\n",
       " 'narrow-thick noodle',\n",
       " 'vegetarian noodle',\n",
       " 'noodle width',\n",
       " 'fatty beef',\n",
       " 'chewier noodle',\n",
       " 'mainly noodle soup option',\n",
       " 'dry noodle option',\n",
       " 'noodle fix',\n",
       " 'chinese beef noodle',\n",
       " 'clear soup base',\n",
       " 'noodle god',\n",
       " 'large beef bowl',\n",
       " 'extra wide noodle',\n",
       " 'great soup base',\n",
       " 'mega size noodle',\n",
       " 'well-made house-made chinese noodle',\n",
       " 'special beef',\n",
       " 'regular beef noodle soup',\n",
       " 'fresh soup noodle',\n",
       " 'special lanzhou noodle beef',\n",
       " 'noodle restaurant',\n",
       " 'pulled noodle restaurant',\n",
       " 'noodle restaurant downtown',\n",
       " 'excellent hot noodle soup',\n",
       " 'hot beef soup noodle',\n",
       " 'chinese noodle soupi',\n",
       " 'noodle style',\n",
       " 'great noodle',\n",
       " 'hard noodle',\n",
       " 'lanzhou noodle house',\n",
       " 'magic noodle',\n",
       " 'noodle choice',\n",
       " 'customized noodle thickness',\n",
       " 'noodle soup size range',\n",
       " 'many noodle ramen place',\n",
       " 'noodle chef',\n",
       " 'instant noodle',\n",
       " 'instant noodle package',\n",
       " 'heavy noodle',\n",
       " 'wide flat noodle',\n",
       " 'relatively new hand-pulled noodle place',\n",
       " 'hand-pulling noodle',\n",
       " 'mild special lanzhou beef noodle',\n",
       " 'non-soupy noodle',\n",
       " 'beef soup',\n",
       " 'gb hand-pulled noodle',\n",
       " 'chinese-style hand-pulled noodle',\n",
       " 'hong kong-style wonton noodle',\n",
       " 'noodles-in-soup anyways',\n",
       " 'hand-pulled noodle place',\n",
       " 'boutique-sized noodle place',\n",
       " 'special beef noodle',\n",
       " 'thin noodle option',\n",
       " 'noodle shape round',\n",
       " 'noodle soup bandwagon',\n",
       " 'many chinese noodle restaurant',\n",
       " 'noodle variety',\n",
       " 'mean noodle',\n",
       " 'chinese noodle soup',\n",
       " 'small noodle bowl',\n",
       " 'small special lanzhou noodle',\n",
       " 'beef slice',\n",
       " 'special lanzhou beef',\n",
       " 'vegetable noodle',\n",
       " 'noodle master',\n",
       " 'noodle type',\n",
       " 'noodle texture',\n",
       " 'standard noodle',\n",
       " 'hot quick noodle fix',\n",
       " 'noodle mass',\n",
       " 'noodle train',\n",
       " 'chinese noodle restaurant',\n",
       " 'super thin noodle',\n",
       " 'asian noodle dish',\n",
       " 'cm noodle',\n",
       " 'favourite lanzhou noodle spot',\n",
       " 'best beef noodle',\n",
       " 'smallest flat noodle',\n",
       " 'hot hand-pulled noodle',\n",
       " 'noodle size number',\n",
       " 'soup broth',\n",
       " 'noodle preparation',\n",
       " 'hot oil soup',\n",
       " 'mian pork sauce noodle',\n",
       " 'noodle option type thickness',\n",
       " 'lanzhou hand-pulled noodlesthere',\n",
       " 'good noodle',\n",
       " 'noodle making',\n",
       " 'max thick flat noodle',\n",
       " 'thickest round noodle',\n",
       " 'really great northern chinese style noodle',\n",
       " 'lanzhou noodle',\n",
       " 'small lanzhou noodle',\n",
       " 'large pork sauce noodle',\n",
       " 'traditional chinese hand-pulled noodle',\n",
       " 'landzhou hand-pulled noodle',\n",
       " 'cheerful noodle place',\n",
       " 'flat noodle variety',\n",
       " 'reason soup',\n",
       " 'noodle house',\n",
       " 'handpulled noodle',\n",
       " 'braised beef',\n",
       " 'different noodle bowl',\n",
       " 'noodle next time',\n",
       " 'noodle soak',\n",
       " 'noodle medium thickness',\n",
       " 'favourite noodle place',\n",
       " 'common noodle',\n",
       " 'favorite lanzhou noodle',\n",
       " 'small beef noodle',\n",
       " 'yet flavourfulnoodle perfect bite texture',\n",
       " 'great noodle soup',\n",
       " 'extra spicy beef',\n",
       " 'second thinnest noodle',\n",
       " 'noodle soup bowl',\n",
       " 'largest noodle',\n",
       " 'smaller noodle',\n",
       " 'mega noodle soup bf',\n",
       " 'sodium-rich noodle',\n",
       " 'noodle puller',\n",
       " 'jelly beef dish',\n",
       " 'small noodle soup',\n",
       " 'second largest round noodle size',\n",
       " 'several noodle type',\n",
       " 'noodle creation process',\n",
       " 'many noodle shop',\n",
       " 'noodle girth',\n",
       " 'noodle joint',\n",
       " 'new noodle restaurant',\n",
       " 'fresh hand-made noodle',\n",
       " 'biggest flat noodle',\n",
       " 'soup place',\n",
       " 'really good noodle',\n",
       " 'limited pork sauce noodle',\n",
       " 'lanzhou beef noodle',\n",
       " 'lanzhou beef soup noodle',\n",
       " 'freshest noodle',\n",
       " 'noodle soup base',\n",
       " 'best noodle-for-dollar value',\n",
       " 'mega sized lanzhou noodle',\n",
       " 'many authentic chinese noodle shop',\n",
       " 'kuan noodle size',\n",
       " 'braised beef tendon',\n",
       " 'rich soup',\n",
       " 'best noodle',\n",
       " 'really good hand-pulled noodle',\n",
       " 'dry pork noodle',\n",
       " 'extra large beef noodle',\n",
       " 'special beef lanzhou noodle',\n",
       " 'noodle range',\n",
       " 'hot soup',\n",
       " 'really noodle',\n",
       " 'noodle beverage washdowns haha',\n",
       " 'similar noodle',\n",
       " 'dry pork sauce noodle',\n",
       " 'wide noodle type',\n",
       " 'long noodle',\n",
       " 'beef noodle place',\n",
       " 'noodle man',\n",
       " 'noodle dough',\n",
       " 'small beef bowl',\n",
       " 'soup ba',\n",
       " 'thin round noodle',\n",
       " 'really thick noodle',\n",
       " 'dry style noodle',\n",
       " 'large size spicy lanzhou medium thick noodle',\n",
       " 'flat noodle size',\n",
       " 'small beef',\n",
       " 'broad noodle',\n",
       " 'many noodle place',\n",
       " 'cleaner chinese lam zhou noodle',\n",
       " 'many noodle size',\n",
       " 'hand pulled noodle',\n",
       " 'soup is tasty',\n",
       " 'noodle were perfectly al',\n",
       " 'noodle were chewy',\n",
       " 'soup was fresh',\n",
       " 'noodle were stick',\n",
       " 'soup flavour was good',\n",
       " 'noodle were pretty decent',\n",
       " 'soup noodle was delicious',\n",
       " 'noodle was thick',\n",
       " 'beef shank was bad',\n",
       " 'mince pork noodle was delicious',\n",
       " 'beef tendon was ready',\n",
       " 'noodle is good',\n",
       " 'hand pull noodle is great',\n",
       " 'noodle were absolutely delicious',\n",
       " 'noodle were pretty standard',\n",
       " 'soup was nice',\n",
       " 'noodle texture was pretty good',\n",
       " 'soup was ok',\n",
       " 'soup is really good',\n",
       " 'noodle were delicious',\n",
       " 'noodle was nice',\n",
       " 'beef quality was really good',\n",
       " 'noodle were absolutely fresh',\n",
       " 'noodle was also good',\n",
       " 'noodle serf chinese-style hand-pulled',\n",
       " 'pork noodle were tasty',\n",
       " 'noodle cooled fast',\n",
       " 'soup is rich',\n",
       " 'soup is flavourful',\n",
       " 'beef is generous',\n",
       " 'soup was tasty',\n",
       " 'beef was ok',\n",
       " 'beef soup much',\n",
       " 'noodle were perfectly soft',\n",
       " 'noodle were nice',\n",
       " 'noodle were great consistent',\n",
       " 'wife flat noodle',\n",
       " 'soup was bland',\n",
       " 'soup was spicy',\n",
       " 'beef brisket was typical',\n",
       " 'soup is great',\n",
       " 'soup was good',\n",
       " 'noodle was chewy',\n",
       " 'zha jiang noodle was really tasty',\n",
       " 'soup is extremely delicious',\n",
       " 'noodle were dense',\n",
       " 'signature noodle large extra',\n",
       " 'chili beef was great',\n",
       " 'beef noodle were really enjoyable',\n",
       " 'beef appetizer was soooooo good',\n",
       " 'noodle were perfect',\n",
       " 'noodle was perfect',\n",
       " 'soup base is slightly salty',\n",
       " 'soup noodle decent',\n",
       " 'noodle were normal',\n",
       " 'noodle was huge',\n",
       " 'noodle was popular',\n",
       " 'noodle is amazing',\n",
       " 'noodle was good',\n",
       " 'soup was realyl flavourful',\n",
       " 'noodle arrived al',\n",
       " 'beef was delectable',\n",
       " 'noodle were al',\n",
       " 'noodle were ok',\n",
       " 'descent noodle cooked el',\n",
       " 'soup was flavorful',\n",
       " 'spicy beef was good',\n",
       " 'tender beef is delicious',\n",
       " 'noodle were quite nice',\n",
       " 'noodle were fresh',\n",
       " 'noodle tasted good',\n",
       " 'soup base was tasty',\n",
       " 'soup were really good',\n",
       " 'noodle great',\n",
       " 'soup was super',\n",
       " 'noodle was generous',\n",
       " 'pork stir fried noodle']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrase_list = get_keyphrase_contain_topk(top_word_list,top_k ,Extracted_list)\n",
    "keyphrase_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['favourite noodle house',\n",
       " 'hand-pulled noodle',\n",
       " 'special lanzhou noodle',\n",
       " 'decent noodle',\n",
       " 'fellow noodle lover',\n",
       " 'noodle taste',\n",
       " 'narrow thick noodle',\n",
       " 'noodle soup',\n",
       " 'delicious beef slice',\n",
       " 'lanzhou style noodle',\n",
       " 'mega beef bowl',\n",
       " 'thin noodle',\n",
       " 'cold tomato egg noodle',\n",
       " 'hot noodle soup',\n",
       " 'noodle craving',\n",
       " 'dry mixed flat noodle',\n",
       " 'widest noodle',\n",
       " 'much better noodle',\n",
       " 'thin-thick noodle',\n",
       " 'great beefy flavour',\n",
       " 'classic lanzhou beef noodle',\n",
       " 'authentic lanzhou hand-pulled noodle',\n",
       " 'noodle option',\n",
       " 'special lanzhou beef noodle',\n",
       " 'flat noodle',\n",
       " 'dry noodle',\n",
       " 'different sized noodle',\n",
       " 'kuan noodle',\n",
       " 'over-cooked beef',\n",
       " 'mega size lanzhou beef noodle',\n",
       " 'mega sized beef noodle',\n",
       " 'large beef noodle',\n",
       " 'extra beef',\n",
       " 'noodle technician slam',\n",
       " 'soup noodle',\n",
       " 'ordinary soup noodle',\n",
       " 'chinese noodle',\n",
       " 'saucy noodle',\n",
       " 'noodle bowl',\n",
       " 'noodle size option',\n",
       " 'usual kuan noodle',\n",
       " 'handpulled noodle place',\n",
       " 'soup base',\n",
       " 'thick flat noodle',\n",
       " 'sliced beef',\n",
       " 'busy noodle spot',\n",
       " 'noodle dish',\n",
       " 'noodle price',\n",
       " 'mega beef',\n",
       " 'med wide noodle',\n",
       " 'fresh noodle',\n",
       " 'authentic lanzhou beef noodle',\n",
       " 'xi noodle',\n",
       " 'regular noodle',\n",
       " 'vegetable soup',\n",
       " 'thick noodle',\n",
       " 'fresh handmade noodle',\n",
       " 'thinly sliced beef',\n",
       " 'second largest size noodle',\n",
       " 'extreme wide noodle',\n",
       " 'specific noodle size type',\n",
       " 'really like beef',\n",
       " 'classic soup combo',\n",
       " 'tender beef',\n",
       " 'noodle shop',\n",
       " 'thinnest noodle',\n",
       " 'dry noodle dish',\n",
       " 'flavourful noodle',\n",
       " 'chinese hand-made noodle',\n",
       " 'noodle size',\n",
       " 'basic beef',\n",
       " 'delicious noodle',\n",
       " 'noodle store',\n",
       " 'customizable noodle thickness',\n",
       " 'delicious hand-pulled lanzhou style noodle',\n",
       " 'best noodle joint',\n",
       " 'mixed noodle',\n",
       " 'freshly pull noodle',\n",
       " 'small soup',\n",
       " 'great noodle joint',\n",
       " 'dry noodle soup',\n",
       " 'udon noodle',\n",
       " 'chewy noodle',\n",
       " 'noodle place',\n",
       " 'fish spicy soup',\n",
       " 'popular spicy beef broth',\n",
       " 'special beef tendon noodle',\n",
       " 'fresh pulled noodle',\n",
       " 'vegetarian soup',\n",
       " 'hand-pulled noodle place downtown',\n",
       " 'really delicious noodle',\n",
       " 'cold noodle',\n",
       " 'wide noodle',\n",
       " 'better soup',\n",
       " 'small beef soup',\n",
       " 'constant noodle',\n",
       " 'dried noodle',\n",
       " 'noodle maker',\n",
       " 'yup solid chinese hand-pulled noodle spot downtown',\n",
       " 'noodle thickness',\n",
       " 'gorgeous hand-pulled noodle',\n",
       " 'many hand-pulled noodle',\n",
       " 'narrow-thick noodle',\n",
       " 'vegetarian noodle',\n",
       " 'noodle width',\n",
       " 'fatty beef',\n",
       " 'chewier noodle',\n",
       " 'mainly noodle soup option',\n",
       " 'dry noodle option',\n",
       " 'noodle fix',\n",
       " 'chinese beef noodle',\n",
       " 'clear soup base',\n",
       " 'noodle god',\n",
       " 'large beef bowl',\n",
       " 'extra wide noodle',\n",
       " 'great soup base',\n",
       " 'mega size noodle',\n",
       " 'well-made house-made chinese noodle',\n",
       " 'special beef',\n",
       " 'regular beef noodle soup',\n",
       " 'fresh soup noodle',\n",
       " 'special lanzhou noodle beef',\n",
       " 'noodle restaurant',\n",
       " 'pulled noodle restaurant',\n",
       " 'noodle restaurant downtown',\n",
       " 'excellent hot noodle soup',\n",
       " 'hot beef soup noodle',\n",
       " 'chinese noodle soupi',\n",
       " 'noodle style',\n",
       " 'great noodle',\n",
       " 'hard noodle',\n",
       " 'lanzhou noodle house',\n",
       " 'magic noodle',\n",
       " 'noodle choice',\n",
       " 'customized noodle thickness',\n",
       " 'noodle soup size range',\n",
       " 'many noodle ramen place',\n",
       " 'noodle chef',\n",
       " 'instant noodle',\n",
       " 'instant noodle package',\n",
       " 'heavy noodle',\n",
       " 'wide flat noodle',\n",
       " 'relatively new hand-pulled noodle place',\n",
       " 'hand-pulling noodle',\n",
       " 'mild special lanzhou beef noodle',\n",
       " 'non-soupy noodle',\n",
       " 'beef soup',\n",
       " 'gb hand-pulled noodle',\n",
       " 'chinese-style hand-pulled noodle',\n",
       " 'hong kong-style wonton noodle',\n",
       " 'noodles-in-soup anyways',\n",
       " 'hand-pulled noodle place',\n",
       " 'boutique-sized noodle place',\n",
       " 'special beef noodle',\n",
       " 'thin noodle option',\n",
       " 'noodle shape round',\n",
       " 'noodle soup bandwagon',\n",
       " 'many chinese noodle restaurant',\n",
       " 'noodle variety',\n",
       " 'mean noodle',\n",
       " 'chinese noodle soup',\n",
       " 'small noodle bowl',\n",
       " 'small special lanzhou noodle',\n",
       " 'beef slice',\n",
       " 'special lanzhou beef',\n",
       " 'vegetable noodle',\n",
       " 'noodle master',\n",
       " 'noodle type',\n",
       " 'noodle texture',\n",
       " 'standard noodle',\n",
       " 'hot quick noodle fix',\n",
       " 'noodle mass',\n",
       " 'noodle train',\n",
       " 'chinese noodle restaurant',\n",
       " 'super thin noodle',\n",
       " 'asian noodle dish',\n",
       " 'cm noodle',\n",
       " 'favourite lanzhou noodle spot',\n",
       " 'best beef noodle',\n",
       " 'smallest flat noodle',\n",
       " 'hot hand-pulled noodle',\n",
       " 'noodle size number',\n",
       " 'soup broth',\n",
       " 'noodle preparation',\n",
       " 'hot oil soup',\n",
       " 'mian pork sauce noodle',\n",
       " 'noodle option type thickness',\n",
       " 'lanzhou hand-pulled noodlesthere',\n",
       " 'good noodle',\n",
       " 'noodle making',\n",
       " 'max thick flat noodle',\n",
       " 'thickest round noodle',\n",
       " 'really great northern chinese style noodle',\n",
       " 'lanzhou noodle',\n",
       " 'small lanzhou noodle',\n",
       " 'large pork sauce noodle',\n",
       " 'traditional chinese hand-pulled noodle',\n",
       " 'landzhou hand-pulled noodle',\n",
       " 'cheerful noodle place',\n",
       " 'flat noodle variety',\n",
       " 'reason soup',\n",
       " 'noodle house',\n",
       " 'handpulled noodle',\n",
       " 'braised beef',\n",
       " 'different noodle bowl',\n",
       " 'noodle next time',\n",
       " 'noodle soak',\n",
       " 'noodle medium thickness',\n",
       " 'favourite noodle place',\n",
       " 'common noodle',\n",
       " 'favorite lanzhou noodle',\n",
       " 'small beef noodle',\n",
       " 'yet flavourfulnoodle perfect bite texture',\n",
       " 'great noodle soup',\n",
       " 'extra spicy beef',\n",
       " 'second thinnest noodle',\n",
       " 'noodle soup bowl',\n",
       " 'largest noodle',\n",
       " 'smaller noodle',\n",
       " 'mega noodle soup bf',\n",
       " 'sodium-rich noodle',\n",
       " 'noodle puller',\n",
       " 'jelly beef dish',\n",
       " 'small noodle soup',\n",
       " 'second largest round noodle size',\n",
       " 'several noodle type',\n",
       " 'noodle creation process',\n",
       " 'many noodle shop',\n",
       " 'noodle girth',\n",
       " 'noodle joint',\n",
       " 'new noodle restaurant',\n",
       " 'fresh hand-made noodle',\n",
       " 'biggest flat noodle',\n",
       " 'soup place',\n",
       " 'really good noodle',\n",
       " 'limited pork sauce noodle',\n",
       " 'lanzhou beef noodle',\n",
       " 'lanzhou beef soup noodle',\n",
       " 'freshest noodle',\n",
       " 'noodle soup base',\n",
       " 'best noodle-for-dollar value',\n",
       " 'mega sized lanzhou noodle',\n",
       " 'many authentic chinese noodle shop',\n",
       " 'kuan noodle size',\n",
       " 'braised beef tendon',\n",
       " 'rich soup',\n",
       " 'best noodle',\n",
       " 'really good hand-pulled noodle',\n",
       " 'dry pork noodle',\n",
       " 'extra large beef noodle',\n",
       " 'special beef lanzhou noodle',\n",
       " 'noodle range',\n",
       " 'hot soup',\n",
       " 'really noodle',\n",
       " 'noodle beverage washdowns haha',\n",
       " 'similar noodle',\n",
       " 'dry pork sauce noodle',\n",
       " 'wide noodle type',\n",
       " 'long noodle',\n",
       " 'beef noodle place',\n",
       " 'noodle man',\n",
       " 'noodle dough',\n",
       " 'small beef bowl',\n",
       " 'soup ba',\n",
       " 'thin round noodle',\n",
       " 'really thick noodle',\n",
       " 'dry style noodle',\n",
       " 'large size spicy lanzhou medium thick noodle',\n",
       " 'flat noodle size',\n",
       " 'small beef',\n",
       " 'broad noodle',\n",
       " 'many noodle place',\n",
       " 'cleaner chinese lam zhou noodle',\n",
       " 'many noodle size',\n",
       " 'hand pulled noodle',\n",
       " 'soup is tasty',\n",
       " 'noodle were perfectly al',\n",
       " 'noodle were chewy',\n",
       " 'soup was fresh',\n",
       " 'noodle were stick',\n",
       " 'soup flavour was good',\n",
       " 'noodle were pretty decent',\n",
       " 'soup noodle was delicious',\n",
       " 'noodle was thick',\n",
       " 'beef shank was bad',\n",
       " 'mince pork noodle was delicious',\n",
       " 'beef tendon was ready',\n",
       " 'noodle is good',\n",
       " 'hand pull noodle is great',\n",
       " 'noodle were absolutely delicious',\n",
       " 'noodle were pretty standard',\n",
       " 'soup was nice',\n",
       " 'noodle texture was pretty good',\n",
       " 'soup was ok',\n",
       " 'soup is really good',\n",
       " 'noodle were delicious',\n",
       " 'noodle was nice',\n",
       " 'beef quality was really good',\n",
       " 'noodle were absolutely fresh',\n",
       " 'noodle was also good',\n",
       " 'noodle serf chinese-style hand-pulled',\n",
       " 'pork noodle were tasty',\n",
       " 'noodle cooled fast',\n",
       " 'soup is rich',\n",
       " 'soup is flavourful',\n",
       " 'beef is generous',\n",
       " 'soup was tasty',\n",
       " 'beef was ok',\n",
       " 'beef soup much',\n",
       " 'noodle were perfectly soft',\n",
       " 'noodle were nice',\n",
       " 'noodle were great consistent',\n",
       " 'wife flat noodle',\n",
       " 'soup was bland',\n",
       " 'soup was spicy',\n",
       " 'beef brisket was typical',\n",
       " 'soup is great',\n",
       " 'soup was good',\n",
       " 'noodle was chewy',\n",
       " 'zha jiang noodle was really tasty',\n",
       " 'soup is extremely delicious',\n",
       " 'noodle were dense',\n",
       " 'signature noodle large extra',\n",
       " 'chili beef was great',\n",
       " 'beef noodle were really enjoyable',\n",
       " 'beef appetizer was soooooo good',\n",
       " 'noodle were perfect',\n",
       " 'noodle was perfect',\n",
       " 'soup base is slightly salty',\n",
       " 'soup noodle decent',\n",
       " 'noodle were normal',\n",
       " 'noodle was huge',\n",
       " 'noodle was popular',\n",
       " 'noodle is amazing',\n",
       " 'noodle was good',\n",
       " 'soup was realyl flavourful',\n",
       " 'noodle arrived al',\n",
       " 'beef was delectable',\n",
       " 'noodle were al',\n",
       " 'noodle were ok',\n",
       " 'descent noodle cooked el',\n",
       " 'soup was flavorful',\n",
       " 'spicy beef was good',\n",
       " 'tender beef is delicious',\n",
       " 'noodle were quite nice',\n",
       " 'noodle were fresh',\n",
       " 'noodle tasted good',\n",
       " 'soup base was tasty',\n",
       " 'soup were really good',\n",
       " 'noodle great',\n",
       " 'soup was super',\n",
       " 'noodle was generous',\n",
       " 'pork stir fried noodle']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrase_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update Vader score for the keyphrase list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 353/353 [00:00<00:00, 23619.14it/s]\n"
     ]
    }
   ],
   "source": [
    "df_keyphrase = evalSentences(keyphrase_list, to_df=True,columns = ['keyphrase_list', 'Vader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyphrase_list</th>\n",
       "      <th>Vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>noodle were pretty decent</td>\n",
       "      <td>0.8316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>best noodle-for-dollar value</td>\n",
       "      <td>0.8116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>hand pull noodle is great</td>\n",
       "      <td>0.8074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>soup is extremely delicious</td>\n",
       "      <td>0.7386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>noodle were absolutely delicious</td>\n",
       "      <td>0.7386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>really delicious noodle</td>\n",
       "      <td>0.7386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>zha jiang noodle was really tasty</td>\n",
       "      <td>0.7378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>noodle texture was pretty good</td>\n",
       "      <td>0.7269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>excellent hot noodle soup</td>\n",
       "      <td>0.7176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>noodle is amazing</td>\n",
       "      <td>0.7168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>best noodle joint</td>\n",
       "      <td>0.7167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>best noodle</td>\n",
       "      <td>0.7167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>best beef noodle</td>\n",
       "      <td>0.7167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>many authentic chinese noodle shop</td>\n",
       "      <td>0.7158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>authentic lanzhou beef noodle</td>\n",
       "      <td>0.7158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>authentic lanzhou hand-pulled noodle</td>\n",
       "      <td>0.7158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>popular spicy beef broth</td>\n",
       "      <td>0.7149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>noodle was popular</td>\n",
       "      <td>0.7149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>delicious hand-pulled lanzhou style noodle</td>\n",
       "      <td>0.7140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>delicious beef slice</td>\n",
       "      <td>0.7140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>delicious noodle</td>\n",
       "      <td>0.7140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>tender beef is delicious</td>\n",
       "      <td>0.7140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>noodle were delicious</td>\n",
       "      <td>0.7140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>soup noodle was delicious</td>\n",
       "      <td>0.7140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>mince pork noodle was delicious</td>\n",
       "      <td>0.7140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>pork noodle were tasty</td>\n",
       "      <td>0.7131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>soup base was tasty</td>\n",
       "      <td>0.7131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>soup was tasty</td>\n",
       "      <td>0.7131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>soup is tasty</td>\n",
       "      <td>0.7131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>noodle cooled fast</td>\n",
       "      <td>0.7077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mega size noodle</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>extra wide noodle</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>large beef bowl</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>chinese beef noodle</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>noodle fix</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>dry noodle option</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>mainly noodle soup option</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>chewier noodle</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>noodle style</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>lanzhou noodle house</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>heavy noodle</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>instant noodle package</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>chinese-style hand-pulled noodle</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>gb hand-pulled noodle</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>beef soup</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>non-soupy noodle</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>hand-pulling noodle</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>relatively new hand-pulled noodle place</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>wide flat noodle</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>pork stir fried noodle</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>instant noodle</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>noodle chef</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>many noodle ramen place</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>noodle soup size range</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>customized noodle thickness</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>noodle choice</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>hard noodle</td>\n",
       "      <td>-0.1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>limited pork sauce noodle</td>\n",
       "      <td>-0.2263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>noodle technician slam</td>\n",
       "      <td>-0.3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>beef shank was bad</td>\n",
       "      <td>-0.5423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 keyphrase_list   Vader\n",
       "281                   noodle were pretty decent  0.8316\n",
       "240                best noodle-for-dollar value  0.8116\n",
       "288                   hand pull noodle is great  0.8074\n",
       "320                 soup is extremely delicious  0.7386\n",
       "289            noodle were absolutely delicious  0.7386\n",
       "90                      really delicious noodle  0.7386\n",
       "319           zha jiang noodle was really tasty  0.7378\n",
       "292              noodle texture was pretty good  0.7269\n",
       "125                   excellent hot noodle soup  0.7176\n",
       "333                           noodle is amazing  0.7168\n",
       "75                            best noodle joint  0.7167\n",
       "246                                 best noodle  0.7167\n",
       "178                            best beef noodle  0.7167\n",
       "242          many authentic chinese noodle shop  0.7158\n",
       "51                authentic lanzhou beef noodle  0.7158\n",
       "21         authentic lanzhou hand-pulled noodle  0.7158\n",
       "85                     popular spicy beef broth  0.7149\n",
       "332                          noodle was popular  0.7149\n",
       "74   delicious hand-pulled lanzhou style noodle  0.7140\n",
       "8                          delicious beef slice  0.7140\n",
       "71                             delicious noodle  0.7140\n",
       "343                    tender beef is delicious  0.7140\n",
       "295                       noodle were delicious  0.7140\n",
       "282                   soup noodle was delicious  0.7140\n",
       "285             mince pork noodle was delicious  0.7140\n",
       "301                      pork noodle were tasty  0.7131\n",
       "347                         soup base was tasty  0.7131\n",
       "306                              soup was tasty  0.7131\n",
       "275                               soup is tasty  0.7131\n",
       "302                          noodle cooled fast  0.7077\n",
       "..                                          ...     ...\n",
       "116                            mega size noodle  0.0000\n",
       "114                           extra wide noodle  0.0000\n",
       "113                             large beef bowl  0.0000\n",
       "110                         chinese beef noodle  0.0000\n",
       "109                                  noodle fix  0.0000\n",
       "108                           dry noodle option  0.0000\n",
       "107                   mainly noodle soup option  0.0000\n",
       "106                              chewier noodle  0.0000\n",
       "128                                noodle style  0.0000\n",
       "131                        lanzhou noodle house  0.0000\n",
       "140                                heavy noodle  0.0000\n",
       "139                      instant noodle package  0.0000\n",
       "148            chinese-style hand-pulled noodle  0.0000\n",
       "147                       gb hand-pulled noodle  0.0000\n",
       "146                                   beef soup  0.0000\n",
       "145                            non-soupy noodle  0.0000\n",
       "143                         hand-pulling noodle  0.0000\n",
       "142     relatively new hand-pulled noodle place  0.0000\n",
       "141                            wide flat noodle  0.0000\n",
       "352                      pork stir fried noodle  0.0000\n",
       "138                              instant noodle  0.0000\n",
       "137                                 noodle chef  0.0000\n",
       "136                     many noodle ramen place  0.0000\n",
       "135                      noodle soup size range  0.0000\n",
       "134                 customized noodle thickness  0.0000\n",
       "133                               noodle choice  0.0000\n",
       "130                                 hard noodle -0.1027\n",
       "235                   limited pork sauce noodle -0.2263\n",
       "33                       noodle technician slam -0.3818\n",
       "284                          beef shank was bad -0.5423\n",
       "\n",
       "[353 rows x 2 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_keyphrase.sort_values(\"Vader\", ascending = False, inplace = True)\n",
    "df_keyphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['noodle', 'soup', 'beef']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_freq_words = [x[0] for x in top_word_list[0:3]]\n",
    "topk_freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyphrase_list</th>\n",
       "      <th>Vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>beef was delectable</td>\n",
       "      <td>0.5994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          keyphrase_list   Vader\n",
       "337  beef was delectable  0.5994"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_keyphrase[df_keyphrase.index == 337]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'noodle': 'noodle were pretty decent',\n",
       " 'soup': 'soup is extremely delicious',\n",
       " 'beef': 'best beef noodle'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict = {}\n",
    "for word in topk_freq_words:\n",
    "    for i in range(df_keyphrase.keyphrase_list.values.shape[0]):\n",
    "        if word in df_keyphrase[\"keyphrase_list\"][i]:\n",
    "            if word not in res_dict:\n",
    "                res_dict[word] = df_keyphrase[\"keyphrase_list\"][i]\n",
    "            if word in res_dict:\n",
    "                if df_keyphrase[\"Vader\"][i] > df_keyphrase[df_keyphrase[\"keyphrase_list\"] == res_dict[word]].Vader.values[0]:\n",
    "                    res_dict[word] = df_keyphrase[\"keyphrase_list\"][i]\n",
    "res_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
